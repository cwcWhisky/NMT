{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_driver.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPk9WrV0Bm3DbEmNNiC+iuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/NMT_driver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qngiJLEPEzWv"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXApLY8E9xw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGiBT-uxXleg"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacrebleu # for evaluation\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git # for detokenizing model outputs prior to evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVINnIj9E_N0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w_rcYx8JVY5"
      },
      "source": [
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgAPTgJxhNGz"
      },
      "source": [
        "%cd /content/gdrive/My Drive/\n",
        "\n",
        "#from NMT import mod\n",
        "from NMT.src.preprocessing.apply_stanza_processors import apply_stanza_processors\n",
        "from NMT.src.preprocessing.truecase import truecase_corpuses\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train, load_checkpoint\n",
        "from NMT.src.predict import predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgyoFSHKJ5Z-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBipTReKPYM"
      },
      "source": [
        "below, steps 1 thru 4 only ever need to be run once (they save their outputs to text and pickle files).\n",
        "\n",
        "\n",
        "step 1 - **apply stanza processors to tokenize and pos-tag the corpuses**\n",
        "\n",
        "\n",
        "> &lt;corpus\\_name&gt; saved to &lt;corpus\\_path&gt;/stanza\\_&lt;corpus\\_name&gt;.pkl, e.g., train.en saved to /content/gdrive/My Drive/iwslt16\\_en\\_de/stanza\\_train.en.pkl.\n",
        "\n",
        "> can retrieve via retrieve\\_stanza\\_outputs().\n",
        "\n",
        "\n",
        "step 2 - **decase corpuses using linguistic heuristics that leverage morphological data produced by morphological data tagger**\n",
        "\n",
        "> &lt;corpus\\_name&gt; saved to &lt;corpus\\_path&gt;/word\\_&lt;corpus\\_name&gt;. \n",
        "\n",
        "> can retrieve via read\\_tokenized\\_corpuses(prefix='word\\_')\n",
        "\n",
        "\n",
        "step 3 - **segment corpuses of words into corpuses of subwords**\n",
        "\n",
        "> &lt;corpus\\_name&gt; saved to &lt;corpus\\_path&gt;/subword\\_joint\\_&lt;corpus\\_name&gt; or &lt;corpus\\_path&gt;/subword\\_ind\\_&lt;corpus\\_name&gt;, depending on if learn a joint vocabulary or separate, independent vocabularies, respectively, for the source and target languages.\n",
        "\n",
        "> can retrieve via read\\_tokenized\\_corpuses(prefix='subword\\_joint\\_') and read\\_tokenized\\_corpuses(prefix='subword\\_ind\\_')\n",
        "\n",
        "\n",
        "\n",
        "step 4 - **convert corpuses into intelligently batched sets of tensors that can be directly passed to model**\n",
        "\n",
        "> dictionary containing all model data is saved to &lt;data\\_path&gt;/&lt;model\\_name&gt;.pkl, where &lt;model\\_name&gt; is identifier for which model to load. \n",
        "\n",
        "> can retrieve via retrieve\\_model\\_data()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW65cehmKRh2"
      },
      "source": [
        "# only meaningful for unit tests on subsets of corpus data, where _start is starting line number,\n",
        "# (using 1-based indexing) and num is how many lines to extract. if num is None, then extract all lines from _start till end of corpus.\n",
        "_start = 1\n",
        "num = None\n",
        "num = 10 # uncomment this line if unit testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJNv2c22F3U-"
      },
      "source": [
        "# step 1 - tokenize corpuses, and tag with morphological data, \n",
        "apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, _start=_start, num=num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFNueYblGugP"
      },
      "source": [
        "# step 2 - true-case corpuses using linguistic heuristics that leverage morphological data produced by morphological data tagger\n",
        "# e.g., remove capitalization from words that are only capitalized for a syntactic reason, like occurring at beginning of sentence\n",
        "# but retain capitalization in proper nouns, etc. (more sophisticated heuristics employed for German corpuses)\n",
        "truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdPJEq1dVkg"
      },
      "source": [
        "# step 3 - import vocab, training, and model hyperparameter settings from configuration files\n",
        "hyperparams = import_configs(config_path=config_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZH_5k34It3l"
      },
      "source": [
        "# step 4 - segment words of corpuses into subwords\n",
        "# (skip this cell if using a word-level vocabulary)\n",
        "!bash subword_joint.sh $hyperparams[\"num_merge_ops\"] $hyperparams[\"vocab_threshold\"] $corpus_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44V9gcJFIxIo"
      },
      "source": [
        "# step 5 - build intelligently batched sets of tensors that can be directly passed to model\n",
        "construct_model_data(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJy_MNEYI8rA"
      },
      "source": [
        "# step 6 - instantiate model\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "test_batches = model_data[\"test_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0yq7oOnFBC8"
      },
      "source": [
        "# step 7 - run tests to ensure model is correct before expensive train step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4APMWckkI_2u"
      },
      "source": [
        "# step 8 - train model\n",
        "model = train(hyperparams, train_batches, dev_batches, references, idx_to_trg_word, checkpoint_path, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEGpCZHaSp9g"
      },
      "source": [
        "# step 9 - predict test set\n",
        "# can load a checkpoint rather than using prev cell's model:\n",
        "if hyperparams[\"early_stopping\"]:\n",
        "    model = load_checkpoint(hyperparams, checkpoint_path, \"best_model\")\n",
        "else:\n",
        "    model, _ = load_checkpoint(hyperparams, checkpoint_path, \"most_recent_model\")\n",
        "\n",
        "# change to test_batches\n",
        "predict(model, dev_batches, references, idx_to_trg_word, checkpoint_path, 1000, inference_alg=\"beam_search\", write=True):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uz30CBUFOjx"
      },
      "source": [
        "translations_file = '' # name this the file containing set of predictions to evaluate\n",
        "# step 10 - evaluate model\n",
        "# step 10a - desegment predictions\n",
        "# skip this cell if used word-level vocab    \n",
        "!sed -E 's/(@@ )|(@@ ?$)//g' <dev.BPE.en > desegmented_translations.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8soCJ1mE5Mg"
      },
      "source": [
        "# step 10b - evaluate\n",
        "!bash eval.sh '/content/gdrive/My Drive/iwslt16_en_de/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxK_nCavJBgI"
      },
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "\n",
        "\n",
        "# REFERENCE_FILE=corpus_path+\"dev.en\" # replace with test.en\n",
        "\n",
        "# TRANSLATED_FILE=checkpoint_path+\"beampreds16\"\n",
        "# perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
        "# PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
        "# sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}