{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_driver.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNigkiAzBy+1Wm7xi1ohrM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/recurrent-NMT/blob/master/NMT_driver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qngiJLEPEzWv"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXApLY8E9xw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVINnIj9E_N0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w_rcYx8JVY5"
      },
      "source": [
        "# overwrite these with your own path, and make sure folders already exist.\n",
        "path = '/content/gdrive/My Drive/NMT/\n",
        "corpus_path = path + 'iwslt16_en_de/'\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0UF-KkzJJV0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBipTReKPYM"
      },
      "source": [
        "below, steps 1 thru 4 only ever need to be run once (they save their outputs to text and pickle files).\n",
        "\n",
        "\n",
        "step 1 - **apply stanza processors to tokenize and pos-tag the corpuses**\n",
        "\n",
        "\n",
        "> &lt;corpus_name&gt; saved to &lt;corpus_path&gt;/stanza_&lt;corpus_name&gt;.pkl, e.g., train.en saved to /content/gdrive/My Drive/iwslt16_en_de/stanza_train.en.pkl.\n",
        "\n",
        "> can retrieve via retrieve_stanza_outputs().\n",
        "\n",
        "\n",
        "step 2 - **decase corpuses using linguistic heuristics that leverage morphological data produced by morphological data tagger**\n",
        "\n",
        "> &lt;corpus_name&gt; saved to &lt;corpus_path&gt;/word_&lt;corpus_name&gt;. \n",
        "\n",
        "> can retrieve via read_tokenized_corpuses(prefix='word_')\n",
        "\n",
        "\n",
        "step 3 - **segment corpuses of words into corpuses of subwords**\n",
        "\n",
        "> &lt;corpus_name&gt; saved to &lt;corpus_path&gt;/subword_joint_&lt;corpus_name&gt; or &lt;corpus_path&gt;/subword_ind_&lt;corpus_name&gt;, depending on if learn a joint vocabulary or separate, independent vocabularies, respectively, for the source and target languages.\n",
        "\n",
        "> can retrieve via read_tokenized_corpuses(prefix='subword_joint_') and read_tokenized_corpuses(prefix='subword_ind_')\n",
        "\n",
        "\n",
        "\n",
        "step 4 - **convert corpuses into batches of tensors that can directly be passed to model**\n",
        "\n",
        "> dictionary containing all model data is saved to &lt;data_path&gt;/&lt;model_name&gt;.pkl, where &lt;model_name&gt; is identifier for which model to load. \n",
        "\n",
        "> can retrieve via retrieve_model_data()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW65cehmKRh2"
      },
      "source": [
        "# only meaningful for unit tests on subsets of corpus data, where _start is starting line number,\n",
        "# (using 1-based indexing) and num is how many lines to extract. if num is None, then extract all lines from _start till end of corpus.\n",
        "_start = 1\n",
        "num = 10\n",
        "#num = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJNv2c22F3U-"
      },
      "source": [
        "# step 1 - tokenize corpuses, and tag with morphological data, \n",
        "#apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFNueYblGugP"
      },
      "source": [
        "# step 2 - true-case corpuses using linguistic heuristics that leverage morphological data produced by morphological data tagger\n",
        "# e.g., remove capitalization from words that are only capitalized for a syntactic reason, like occurring at beginning of sentence\n",
        "# but retain capitalization in proper nouns, etc. (more sophisticated heuristics employed for German corpuses)\n",
        "#truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, _start=_start, num=num):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZH_5k34It3l"
      },
      "source": [
        "# step 3 - segment words of corpuses into subwords\n",
        "# (skip this cell if using a word-level vocabulary)\n",
        "!pip install subword-nmt\n",
        "\n",
        "# arg1 is num_merge_ops, arg2 is vocab_threshold.\n",
        "#!bash subword_joint.sh 30000 10 '/content/gdrive/My Drive/iwslt16_en_de/'\n",
        "\n",
        "# produces bpe_codes, vocab.de, vocab.en, and text files containing the segmented corpuses."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44V9gcJFIxIo"
      },
      "source": [
        "# step 4 - build batches of tensors that can be directly pass to model\n",
        "# construct_model_data(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\",\n",
        "#                      corpus_path=corpus_path, config_path=config_path, data_path=data_path, model_name=model_name,\n",
        "#                      vocab_threshold=10, src_vocab_file='vocab.de', trg_vocab_file='vocab.en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJy_MNEYI8rA"
      },
      "source": [
        "# step 5 - instantiate model\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "test_batches = model_data[\"test_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4APMWckkI_2u"
      },
      "source": [
        "# step 6 - train model\n",
        "model = train(hyperparams, train_batches, dev_batches, references, idx_to_trg_word, checkpoint_path, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEGpCZHaSp9g"
      },
      "source": [
        "# step 7 - predict test set\n",
        "# can load a checkpoint rather than using prev cell's model:\n",
        "name = \"best_model\" if early_stopping else \"most_recent_model\"\n",
        "load_checkpoint(hyperparams, checkpoint_path, name)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxK_nCavJBgI"
      },
      "source": [
        "# step 8 - evaluate model\n",
        "!pip install sacrebleu\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziN8KSZF2LNI"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1,2,3], device=\"cuda:0\")\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kh46nu2SNu"
      },
      "source": [
        "x.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOYf5ltN2Xhm"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oqFF-zO2YMs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}