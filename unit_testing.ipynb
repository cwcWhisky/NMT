{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unit_testing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJOXrd1pSwYwxQdRXpDtIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/unit_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6KIv7q2KVam"
      },
      "source": [
        "# environment for running unit tests, observing model outputs, etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNvi50C-Kdni"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDGBdjKWK2N1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxUSJ1EK4FI"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacremoses # for detokenizing model predictions\n",
        "!pip install sacrebleu # for evaluation\n",
        "#!git clone https://github.com/moses-smt/mosesdecoder.git # for detokenizing model outputs prior to evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Q3oAsiLAZv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkw_ARBLC0T"
      },
      "source": [
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "#corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_0512TbLFUp"
      },
      "source": [
        "%cd /content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDrW0uoTNQg"
      },
      "source": [
        "from NMT.src.preprocessing.apply_stanza_processors import apply_stanza_processors, retrieve_stanza_outputs\n",
        "from NMT.src.preprocessing.corpus_utils import read_corpuses, print_corpuses, print_processed_corpuses\n",
        "from NMT.src.preprocessing.truecase import truecase_corpuses\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UDUODlRLawH"
      },
      "source": [
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='', _start=1, num=5)\n",
        "print_corpuses(corpuses, num=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_W7c-1_NMXn"
      },
      "source": [
        "# step 1\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, _start=1, num=10)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "\n",
        "print_processed_corpuses(corpuses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRng63k2OMaL"
      },
      "source": [
        "# step 2\n",
        "truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mak6yGkMfinY"
      },
      "source": [
        "# step 3\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "for hp in hyperparams:\n",
        "   print(f\"{hp}: {hyperparams[hp]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAV3NAvJbbzt"
      },
      "source": [
        "# step 4\n",
        "subword_corpus_path = '/content/gdrive/My\\ Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "num_merge_ops = 1000 # for unit testing, overwrite to smaller values\n",
        "vocab_threshold = 2\n",
        "!bash ./NMT/src/preprocessing/subword_joint.sh 1000 2 '/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "#!bash ./NMT/src/preprocessing/subword_joint.sh $num_merge_ops $vocab_threshold $subword_corpus_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVx7jjr5p1i"
      },
      "source": [
        "# corpuses, ref_corpuses = read_tokenized_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path='/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/', prefix='word_')\n",
        "# print_corpuses(corpuses)\n",
        "# print_corpuses(ref_corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO8GQjMq7CXG"
      },
      "source": [
        "# toy corpuses\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='')\n",
        "print_corpuses(corpuses)\n",
        "\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", path=corpus_path)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", path=corpus_path)\n",
        "\n",
        "print_processed_corpuses(corpuses)\n",
        "\n",
        "truecase_corpuses(\"train.de\", \"train.en\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxXubYkvQ3O"
      },
      "source": [
        "# ensure batches of tensors constructed correctly.\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "#corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path\n",
        "\n",
        "\n",
        "%cd /content/gdrive/My Drive/\n",
        "\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n",
        "\n",
        "# step 5\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "hyperparams[\"vocab_type\"] = \"word\"\n",
        "hyperparams[\"trim_type\"] = \"top_k\"\n",
        "hyperparams[\"src_k\"] = 10\n",
        "hyperparams[\"trg_k\"] = 10\n",
        "hyperparams[\"train_bsz\"] = 2\n",
        "hyperparams[\"dev_bsz\"] = 2\n",
        "hyperparams[\"decode_slack\"] = 30\n",
        "\n",
        "vocabs, corpuses, ref_corpuses = construct_model_data(\"train.de\", \"train.en\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name, overfit=True\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# step 6\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "test_batches = model_data[\"test_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]\n",
        "device = hyperparams[\"device\"]\n",
        "\n",
        "\n",
        "print(f'src vocab:{vocabs[\"src_word_to_idx\"]}')\n",
        "print(f'trg vocab:{vocabs[\"trg_word_to_idx\"]}')\n",
        "\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "### train_batches:\n",
        "# train batch 1\n",
        "encoder_inputs, decoder_inputs, decoder_targets = train_batches[0]\n",
        "assert torch.all(torch.eq(encoder_inputs['in'], torch.tensor([[3, 6, 4, 8], [9, 2, 7, 0]], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([4, 3], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([1, 0], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['in'], torch.tensor([[ 2,  7,  4,  9,  5], [ 2, 11,  6, 10,  0]], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['lengths'], torch.tensor([5, 4], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False,  True]], [[False, False, False, False]]], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_targets, torch.tensor([ 7, 11,  4,  6,  9, 10,  5,  3,  3], device=device)))\n",
        "\n",
        "\n",
        "# train batch 2\n",
        "encoder_inputs, decoder_inputs, decoder_targets = train_batches[1]\n",
        "assert torch.all(torch.eq(encoder_inputs['in'], torch.tensor([[10,  5,  11, 11, 11]], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([5], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['in'], torch.tensor([[2, 8]], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['lengths'], torch.tensor([2], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False, False, False]]], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_targets, torch.tensor([8, 3], device=device)))\n",
        "\n",
        "\n",
        "### dev_batches:\n",
        "# dev batch 1\n",
        "encoder_inputs, decoder_inputs, corpus_indices = dev_batches[0]\n",
        "assert torch.all(torch.eq(encoder_inputs['in'], torch.tensor([[10,  5, 11, 11, 11], [ 3,  6,  4,  8,  0]], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([5, 4], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0, 1], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False, False, False]], [[False, False, False, False,  True]]], device='cuda:0')))\n",
        "assert decoder_inputs['max_src_len'] == 5\n",
        "\n",
        "assert torch.all(torch.eq(corpus_indices, torch.tensor([2, 0], device=device)))\n",
        "\n",
        "\n",
        "# dev batch 2\n",
        "encoder_inputs, decoder_inputs, corpus_indices = dev_batches[1]\n",
        "assert torch.all(torch.eq(encoder_inputs['in'], torch.tensor([[9, 2, 7]], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([3], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False]]], device=device)))\n",
        "assert decoder_inputs['max_src_len'] == 3\n",
        "\n",
        "assert torch.all(torch.eq(corpus_indices, torch.tensor([1], device=device)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "# print(\"##################### dev_batches:\")\n",
        "# for i, dev_batch in enumerate(dev_batches):\n",
        "#     print(f\"dev batch {i+1}:\")\n",
        "#     encoder_inputs, decoder_inputs, corpus_indices = dev_batch\n",
        "#     print(\"encoder_inputs:\")\n",
        "#     print(f\"in: {encoder_inputs['in']}\")\n",
        "#     print(f\"sorted_lengths: {encoder_inputs['sorted_lengths']}\")\n",
        "#     print(f\"idxs_in_sorted: {encoder_inputs['idxs_in_sorted']}\")\n",
        "#     print('\\n\\n')\n",
        "#     print(\"decoder_inputs:\")\n",
        "#     print(f\"mask: {decoder_inputs['mask']}\")\n",
        "#     print(f\"max_src_len: {decoder_inputs['max_src_len']}\")\n",
        "#     print('\\n\\n')\n",
        "#     print(f\"corpus_indices: {corpus_indices}\")\n",
        "#     print('\\n\\n\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQuaPqdBA9o0"
      },
      "source": [
        "# ensure attention mechanism produces correct result, everything is of correct shape, initial loss is reasonable, and \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8SsuG7DAZXQ"
      },
      "source": [
        "\n",
        "\n",
        "# step 8\n",
        "dev_references = ref_corpuses[\"train.en\"]\n",
        "reduction = 'sum' # easier to observe loss decrease each epoch\n",
        "model = train(hyperparams, train_batches, dev_batches, dev_references, idx_to_trg_word, checkpoint_path, save=True, reduction=reduction)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6X1duu2Yaw"
      },
      "source": [
        "import torch\n",
        "a = torch.arange(1,10).cuda().view(3,3)\n",
        "a[2,0] = 20\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3W8isQ82a1Q"
      },
      "source": [
        "a.dim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s71RUIMQ2eve"
      },
      "source": [
        "torch.argmax(a, 1, keepdim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yXYn1HQ6xEg"
      },
      "source": [
        "\n",
        "import sacrebleu\n",
        "refs = [['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "        ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dW-KlqjUCD4"
      },
      "source": [
        "refs = [['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}