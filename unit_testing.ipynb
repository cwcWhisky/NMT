{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unit_testing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhn84f6XZocYeXXDsWs5GY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/unit_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6KIv7q2KVam"
      },
      "source": [
        "# environment for running unit tests, observing model outputs, etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNvi50C-Kdni"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDGBdjKWK2N1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxUSJ1EK4FI"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacremoses # for detokenizing model predictions\n",
        "!pip install sacrebleu # for evaluation\n",
        "!pip install pytest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Q3oAsiLAZv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkw_ARBLC0T"
      },
      "source": [
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "#corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_0512TbLFUp"
      },
      "source": [
        "# %cd /content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDrW0uoTNQg"
      },
      "source": [
        "# from NMT.src.preprocessing.apply_stanza_processors import apply_stanza_processors, retrieve_stanza_outputs\n",
        "# from NMT.src.preprocessing.corpus_utils import read_corpuses, print_corpuses, print_processed_corpuses\n",
        "# from NMT.src.preprocessing.truecase import truecase_corpuses\n",
        "# from NMT.src.import_configs import import_configs\n",
        "# from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "# from NMT.src.train import train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdynmhgHn82z"
      },
      "source": [
        "%cd /content/gdrive/My Drive/NMT\n",
        "\n",
        "from src.preprocessing.apply_stanza_processors import apply_stanza_processors, retrieve_stanza_outputs\n",
        "from src.preprocessing.corpus_utils import read_corpuses, print_corpuses, print_processed_corpuses\n",
        "from src.preprocessing.truecase import truecase_corpuses\n",
        "from src.import_configs import import_configs\n",
        "from src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from src.train import train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqNENnJJnf5e"
      },
      "source": [
        "!python -m pytest unittests/test_batches.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpb5X9nxngji"
      },
      "source": [
        "!python -m pytest -s -v unittests/test_model.py::test_default_word_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BdJt3nWnhwl"
      },
      "source": [
        "!python -m pytest -s -v unittests/test_model.py::test_default_subword_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8h_ztfAnhcq"
      },
      "source": [
        "# before running this cell, ensure using cpu\n",
        "# (Runtime -> Change runtime type -> Hardware accelerator = None)\n",
        "!python -m pytest -s -v unittests/test_model.py::test_default_word_model_cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHD_PvLEnhZ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2SiTEIpnhXL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjjQbkmnhTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH0l2P8Png9_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2gQJ958ng7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59RbfD4Ong4P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UDUODlRLawH"
      },
      "source": [
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='', _start=1, num=5)\n",
        "print_corpuses(corpuses, num=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_W7c-1_NMXn"
      },
      "source": [
        "# step 1\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, _start=1, num=10)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "\n",
        "print_processed_corpuses(corpuses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRng63k2OMaL"
      },
      "source": [
        "# step 2\n",
        "truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mak6yGkMfinY"
      },
      "source": [
        "# step 3\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "for hp in hyperparams:\n",
        "   print(f\"{hp}: {hyperparams[hp]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAV3NAvJbbzt"
      },
      "source": [
        "# step 4\n",
        "subword_corpus_path = '/content/gdrive/My\\ Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "num_merge_ops = 1000 # for unit testing, overwrite to smaller values\n",
        "vocab_threshold = 2\n",
        "!bash ./NMT/src/preprocessing/subword_joint.sh 1000 2 '/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "#!bash ./NMT/src/preprocessing/subword_joint.sh $num_merge_ops $vocab_threshold $subword_corpus_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVx7jjr5p1i"
      },
      "source": [
        "# corpuses, ref_corpuses = read_tokenized_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path='/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/', prefix='word_')\n",
        "# print_corpuses(corpuses)\n",
        "# print_corpuses(ref_corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO8GQjMq7CXG"
      },
      "source": [
        "# toy corpuses\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='')\n",
        "print_corpuses(corpuses)\n",
        "\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", path=corpus_path)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", path=corpus_path)\n",
        "\n",
        "print_processed_corpuses(corpuses)\n",
        "\n",
        "truecase_corpuses(\"train.de\", \"train.en\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxXubYkvQ3O"
      },
      "source": [
        "# ensure batches of tensors constructed correctly.\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "#corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path\n",
        "\n",
        "\n",
        "%cd /content/gdrive/My Drive/\n",
        "\n",
        "import torch\n",
        "\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n",
        "from NMT.src.preprocessing.corpus_utils import read_tokenized_corpuses\n",
        "\n",
        "# step 5\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "hyperparams[\"vocab_type\"] = \"word\"\n",
        "hyperparams[\"trim_type\"] = \"top_k\"\n",
        "hyperparams[\"src_k\"] = 10\n",
        "hyperparams[\"trg_k\"] = 10\n",
        "hyperparams[\"train_bsz\"] = 2\n",
        "hyperparams[\"dev_bsz\"] = 2\n",
        "hyperparams[\"decode_slack\"] = 30\n",
        "hyperparams[\"early_stopping\"] = False\n",
        "\n",
        "\n",
        "vocabs, corpuses, ref_corpuses = construct_model_data(\"train.de\", \"train.en\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name, overfit=True\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# step 6\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "test_batches = model_data[\"test_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]\n",
        "device = hyperparams[\"device\"]\n",
        "\n",
        "\n",
        "print(f'src vocab:{vocabs[\"src_word_to_idx\"]}')\n",
        "print(f'trg vocab:{vocabs[\"trg_word_to_idx\"]}')\n",
        "src_word_to_idx = vocabs[\"src_word_to_idx\"]\n",
        "trg_word_to_idx = vocabs[\"trg_word_to_idx\"]\n",
        "\n",
        "# tokenized, truecased toy corpuses:\n",
        "# train.de:\n",
        "# [['das', 'ist', 'wahr', '.'], ['mache', 'ich', 'Ja'], ['heute', 'Abend', '!', '!', '!']]\n",
        "\n",
        "# train.en:\n",
        "# [['it', \"'s\", 'true'], ['do', 'I', '?', 'yes'], ['tonight']]}\n",
        "\n",
        "\n",
        "train_encoder_inputs_in1 = [['das', 'ist', 'wahr', '.'], ['mache', 'ich', 'Ja', '<pad>']]\n",
        "train_encoder_inputs_in1 = [[src_word_to_idx[word] for word in sent] for sent in train_encoder_inputs_in1]\n",
        "train_encoder_inputs_in2 = [['heute', 'Abend', '!', '!', '!']]\n",
        "train_encoder_inputs_in2 = [[src_word_to_idx[word] for word in sent] for sent in train_encoder_inputs_in2]\n",
        "\n",
        "train_decoder_inputs_in1 = [['<sos>', 'do', 'I', '?', 'yes'], ['<sos>', 'it', \"'s\", 'true', '<pad>']]\n",
        "train_decoder_inputs_in1 = [[trg_word_to_idx[word] for word in sent] for sent in train_decoder_inputs_in1]\n",
        "train_decoder_inputs_in2 = [['<sos>', 'tonight']]\n",
        "train_decoder_inputs_in2 = [[trg_word_to_idx[word] for word in sent] for sent in train_decoder_inputs_in2]\n",
        "\n",
        "train_decoder_targets1 = ['do', 'it', 'I', \"'s\", '?', 'true', 'yes', '<eos>', '<eos>']\n",
        "train_decoder_targets1 = [trg_word_to_idx[word] for word in train_decoder_targets1]\n",
        "train_decoder_targets2 = ['tonight', '<eos>']\n",
        "train_decoder_targets2 = [trg_word_to_idx[word] for word in train_decoder_targets2]\n",
        "\n",
        "dev_encoder_inputs_in1 = [['heute', 'Abend', '!', '!', '!'], ['das', 'ist', 'wahr', '.', '<pad>']]\n",
        "dev_encoder_inputs_in1 = [[src_word_to_idx[word] for word in sent] for sent in dev_encoder_inputs_in1]\n",
        "dev_encoder_inputs_in2 = [['mache', 'ich', 'Ja']]\n",
        "dev_encoder_inputs_in2 = [[src_word_to_idx[word] for word in sent] for sent in dev_encoder_inputs_in2]\n",
        "\n",
        "\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "### train_batches:\n",
        "# train batch 1\n",
        "encoder_inputs, decoder_inputs, decoder_targets = train_batches[0]\n",
        "assert encoder_inputs['in'].tolist() == train_encoder_inputs_in1\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([4, 3], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([1, 0], device=device)))\n",
        "\n",
        "assert decoder_inputs['in'].tolist() == train_decoder_inputs_in1\n",
        "assert torch.all(torch.eq(decoder_inputs['lengths'], torch.tensor([5, 4], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False,  True]], [[False, False, False, False]]], device=device)))\n",
        "\n",
        "assert decoder_targets.tolist() == train_decoder_targets1\n",
        "\n",
        "\n",
        "# train batch 2\n",
        "encoder_inputs, decoder_inputs, decoder_targets = train_batches[1]\n",
        "assert encoder_inputs['in'].tolist() == train_encoder_inputs_in2\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([5], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0], device=device)))\n",
        "\n",
        "assert decoder_inputs['in'].tolist() == train_decoder_inputs_in2\n",
        "assert torch.all(torch.eq(decoder_inputs['lengths'], torch.tensor([2], device=device)))\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False, False, False]]], device=device)))\n",
        "\n",
        "assert decoder_targets.tolist() == train_decoder_targets2\n",
        "\n",
        "\n",
        "### dev_batches:\n",
        "# dev batch 1\n",
        "encoder_inputs, decoder_inputs, corpus_indices = dev_batches[0]\n",
        "assert encoder_inputs['in'].tolist() == dev_encoder_inputs_in1\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([5, 4], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0, 1], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False, False, False]], [[False, False, False, False,  True]]], device='cuda:0')))\n",
        "assert decoder_inputs['max_src_len'] == 5\n",
        "\n",
        "assert torch.all(torch.eq(corpus_indices, torch.tensor([2, 0], device=device)))\n",
        "\n",
        "\n",
        "# dev batch 2\n",
        "encoder_inputs, decoder_inputs, corpus_indices = dev_batches[1]\n",
        "assert encoder_inputs['in'].tolist() == dev_encoder_inputs_in2\n",
        "assert torch.all(torch.eq(encoder_inputs['sorted_lengths'], torch.tensor([3], device=device)))\n",
        "assert torch.all(torch.eq(encoder_inputs['idxs_in_sorted'], torch.tensor([0], device=device)))\n",
        "\n",
        "assert torch.all(torch.eq(decoder_inputs['mask'], torch.tensor([[[False, False, False]]], device=device)))\n",
        "assert decoder_inputs['max_src_len'] == 3\n",
        "\n",
        "assert torch.all(torch.eq(corpus_indices, torch.tensor([1], device=device)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# step 8\n",
        "dev_references = ref_corpuses[\"train.en\"]\n",
        "print(dev_references)\n",
        "reduction = 'sum' # easier to observe loss decrease each epoch\n",
        "model = train(hyperparams, train_batches, dev_batches, dev_references, idx_to_trg_word, checkpoint_path, save=True, reduction=reduction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQuaPqdBA9o0"
      },
      "source": [
        "# ensure attention mechanism produces correct result, everything is of correct shape, initial loss is reasonable, and \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8SsuG7DAZXQ"
      },
      "source": [
        "[1,2,\"hi\"] == [1,2,\"hi\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd6X1duu2Yaw"
      },
      "source": [
        "import torch\n",
        "a = torch.arange(1,10).cuda().view(3,3)\n",
        "a[2,0] = 20\n",
        "a.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3W8isQ82a1Q"
      },
      "source": [
        "a.dim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s71RUIMQ2eve"
      },
      "source": [
        "torch.argmax(a, 1, keepdim=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yXYn1HQ6xEg"
      },
      "source": [
        "\n",
        "import sacrebleu\n",
        "refs = [['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "        ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dW-KlqjUCD4"
      },
      "source": [
        "refs = [['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']]\n",
        "sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZxR6afLNSD8"
      },
      "source": [
        "b = torch.full((1, 1), 5, dtype=torch.long)\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_wtrc2fNm0C"
      },
      "source": [
        "b.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGpEArleS9HU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEKalVK4S_H8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0jq46qXTAo4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iuLrpDrTAsY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SJntVrETA3d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juFyI7emTA0a"
      },
      "source": [
        "# overfit to first 10 sentences of training set\n",
        "\n",
        "from NMT.src.preprocessing.apply_stanza_processors import apply_stanza_processors, retrieve_stanza_outputs\n",
        "from NMT.src.preprocessing.corpus_utils import read_corpuses, print_corpuses, print_processed_corpuses\n",
        "from NMT.src.preprocessing.truecase import truecase_corpuses\n",
        "\n",
        "\n",
        "\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='', num=10)\n",
        "print_corpuses(corpuses)\n",
        "\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", path=corpus_path, num=10)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", path=corpus_path)\n",
        "print_processed_corpuses(corpuses)\n",
        "\n",
        "truecase_corpuses(\"train.de\", \"train.en\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS632AOzTAyL"
      },
      "source": [
        "# overfit to first 10 sentences of training set\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "#corpus_path = path + 'corpuses/toy_corpuses/'\n",
        "\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path\n",
        "\n",
        "\n",
        "%cd /content/gdrive/My Drive/\n",
        "\n",
        "import torch\n",
        "\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n",
        "from NMT.src.preprocessing.corpus_utils import read_tokenized_corpuses\n",
        "\n",
        "# step 5\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "hyperparams[\"vocab_type\"] = \"word\"\n",
        "hyperparams[\"trim_type\"] = \"top_k\"\n",
        "hyperparams[\"src_k\"] = 200\n",
        "hyperparams[\"trg_k\"] = 200\n",
        "hyperparams[\"train_bsz\"] = 10\n",
        "hyperparams[\"dev_bsz\"] = 10\n",
        "hyperparams[\"decode_slack\"] = 30\n",
        "hyperparams[\"early_stopping\"] = False\n",
        "hyperparams[\"total_epochs\"] = 50\n",
        "hyperparams[\"enc_hidden_size\"] = 1000\n",
        "hyperparams[\"dec_hidden_size\"] = 1000\n",
        "\n",
        "\n",
        "\n",
        "vocabs, corpuses, ref_corpuses = construct_model_data(\"train.de\", \"train.en\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name, overfit=True\n",
        "                    )\n",
        "\n",
        "\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]\n",
        "\n",
        "\n",
        "print(f'src vocab:{vocabs[\"src_word_to_idx\"]}')\n",
        "print(f'trg vocab:{vocabs[\"trg_word_to_idx\"]}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dev_references = ref_corpuses[\"train.en\"]\n",
        "print(dev_references)\n",
        "#reduction = 'sum' # easier to observe loss decrease each epoch\n",
        "# may actually impair convergence if takes too big a step...\n",
        "model, loss = train(hyperparams, train_batches, dev_batches, dev_references, idx_to_trg_word, checkpoint_path, save=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efWnk6nOTAwQ"
      },
      "source": [
        "# predict train set\n",
        "\n",
        "from NMT.src.predict import predict\n",
        "# use beam search instead of greedy search.\n",
        "model.decoder.set_inference_alg(\"beam_search\")\n",
        "\n",
        "# change to test_batches\n",
        "bleu, preds_time, post_time = predict(model, dev_batches, dev_references, idx_to_trg_word, checkpoint_path)\n",
        "print(round(bleu, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPAx_QrTFkOL"
      },
      "source": [
        "round?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csq9xAyrFlRK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiimNrdBFlZw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elP6GBSHFlXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA_vz2B7FlVR"
      },
      "source": [
        "just_finished = torch.tensor([0,1,0,1], dtype=torch.bool)\n",
        "# entry j is True if seq j finished being translated this timestep.\n",
        "# obtain indices to extract the sequences that just finished.\n",
        "print(torch.nonzero(just_finished, as_tuple=True))\n",
        "print(torch.nonzero(just_finished, as_tuple=False))\n",
        "\n",
        "print(torch.nonzero(just_finished).squeeze(0))\n",
        "print(torch.nonzero(just_finished).squeeze(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbpENIVzGOBo"
      },
      "source": [
        "x = torch.tensor([2])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KhbwhNqMGYy"
      },
      "source": [
        "x.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjn_0IM0GGCD"
      },
      "source": [
        "y = torch.tensor(3)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f44C7JlkRHNT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoICsGhJRHK3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJFYJfPbRHIe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnMqAcMuRG31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pd2BbvoRG1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNxRhCGVRGxw"
      },
      "source": [
        "################ now, overfit using subword vocab\n",
        "\n",
        "# overfit to first 10 sentences of training set\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path\n",
        "\n",
        "\n",
        "%cd /content/gdrive/My Drive/\n",
        "\n",
        "import torch\n",
        "\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n",
        "from NMT.src.preprocessing.corpus_utils import read_tokenized_corpuses\n",
        "\n",
        "# step 3\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "hyperparams[\"num_merge_ops\"] = 300\n",
        "hyperparams[\"vocab_threshold\"] = 0\n",
        "hyperparams[\"train_bsz\"] = 10\n",
        "hyperparams[\"dev_bsz\"] = 10\n",
        "hyperparams[\"decode_slack\"] = 30\n",
        "hyperparams[\"early_stopping\"] = False\n",
        "hyperparams[\"total_epochs\"] = 50\n",
        "hyperparams[\"enc_hidden_size\"] = 1000\n",
        "hyperparams[\"dec_hidden_size\"] = 1000\n",
        "\n",
        "num_merge_ops = hyperparams[\"num_merge_ops\"]\n",
        "vocab_threshold = hyperparams[\"vocab_threshold\"]\n",
        "# print(\"start\")\n",
        "# !echo $num_merge_ops\n",
        "# !echo $vocab_threshold\n",
        "# !echo \"$corpus_path\"\n",
        "# print(\"finish\")\n",
        "\n",
        "# step 4\n",
        "!bash ./NMT/src/preprocessing/subword_joint.sh $num_merge_ops $vocab_threshold \"$corpus_path\"\n",
        "\n",
        "\n",
        "# step 5\n",
        "vocabs, corpuses, ref_corpuses = construct_model_data(\"train.de\", \"train.en\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name, overfit=True\n",
        "                    )\n",
        "\n",
        "\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]\n",
        "\n",
        "\n",
        "print(f'src vocab:{vocabs[\"src_word_to_idx\"]}')\n",
        "print(f'trg vocab:{vocabs[\"trg_word_to_idx\"]}')\n",
        "\n",
        "\n",
        "dev_references = ref_corpuses[\"train.en\"]\n",
        "print(dev_references)\n",
        "model = train(hyperparams, train_batches, dev_batches, dev_references, idx_to_trg_word, checkpoint_path, save=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjxN8iG4RGtW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPE_WOa7wPiQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQRVaDOnwPfp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8H141GpwPca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAY3OEefwPZw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2dYuVonwPSC"
      },
      "source": [
        "hyperparams = import_configs(config_path=config_path)\n",
        "# overwrite default settings for hyperparams such that conforms to test conditions\n",
        "hyperparams[\"vocab_type\"] = \"word\"\n",
        "hyperparams[\"trim_type\"] = \"top_k\"\n",
        "hyperparams[\"src_k\"] = 200 # set large enough such that no <unk> tokens (or else will not achieve BLEU of 100)\n",
        "hyperparams[\"trg_k\"] = 200\n",
        "hyperparams[\"train_bsz\"] = 3\n",
        "hyperparams[\"dev_bsz\"] = 3\n",
        "hyperparams[\"decode_slack\"] = 30 # set large enough such that can finish predicting each of the 10 target sentences (or else will not achieve BLEU of 100)\n",
        "hyperparams[\"early_stopping\"] = False # let the loss go down to zero.\n",
        "hyperparams[\"total_epochs\"] = 50\n",
        "hyperparams[\"enc_hidden_size\"] = 1000 # ensure model is of sufficient capacity\n",
        "hyperparams[\"dec_hidden_size\"] = 1000\n",
        "hyperparams[\"enc_dropout\"] = 0 # ensure regularization turned off\n",
        "hyperparams[\"dec_dropout\"] = 0\n",
        "hyperparams[\"L2_reg\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}