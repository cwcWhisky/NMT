{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unit_testing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOmFPPW3aUuXLWOQIko4EAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/unit_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6KIv7q2KVam"
      },
      "source": [
        "# environment for running unit tests, observing model outputs, etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNvi50C-Kdni"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDGBdjKWK2N1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z01FTlPRPUj"
      },
      "source": [
        "from NMT.src.preprocessing.apply_stanza_processors import apply_stanza_processors, retrieve_stanza_outputs\n",
        "from NMT.src.preprocessing.corpus_utils import read_corpuses, print_corpuses, print_processed_corpuses\n",
        "from NMT.src.preprocessing.truecase import truecase_corpuses\n",
        "from NMT.src.import_configs import import_configs\n",
        "from NMT.src.preprocessing.preprocess import construct_model_data, retrieve_model_data\n",
        "from NMT.src.train import train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxUSJ1EK4FI"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacrebleu # for evaluation\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git # for detokenizing model outputs prior to evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Q3oAsiLAZv"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkw_ARBLC0T"
      },
      "source": [
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "config_path = path + 'configs/'\n",
        "data_path = path + 'data/'\n",
        "checkpoint_path = path + 'checkpoints/'\n",
        "\n",
        "model_name = 'my_model' # name of model tensor batches, hyperparameters, etc., saved as pickle file inside data_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_0512TbLFUp"
      },
      "source": [
        "%cd /content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UDUODlRLawH"
      },
      "source": [
        "\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='', _start=1, num=5)\n",
        "print_corpuses(corpuses, num=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_W7c-1_NMXn"
      },
      "source": [
        "# step 1\n",
        "apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, _start=1, num=5)\n",
        "corpuses = retrieve_stanza_outputs(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "\n",
        "print_processed_corpuses(corpuses, num=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRng63k2OMaL"
      },
      "source": [
        "# step 2\n",
        "truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path)\n",
        "corpuses = read_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path=corpus_path, prefix='word_')\n",
        "print_corpuses(corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mak6yGkMfinY"
      },
      "source": [
        "# step 3\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "for hp in hyperparams:\n",
        "   print(f\"{hp}: {hyperparams[hp]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAV3NAvJbbzt"
      },
      "source": [
        "# step 4\n",
        "subword_corpus_path = '/content/gdrive/My\\ Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "num_merge_ops = 1000 # for unit testing, overwrite to smaller values\n",
        "vocab_threshold = 2\n",
        "!bash ./NMT/src/preprocessing/subword_joint.sh 1000 2 '/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/'\n",
        "#!bash ./NMT/src/preprocessing/subword_joint.sh $num_merge_ops $vocab_threshold $subword_corpus_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVx7jjr5p1i"
      },
      "source": [
        "# corpuses, ref_corpuses = read_tokenized_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", path='/content/gdrive/My Drive/NMT/corpuses/iwslt16_en_de/', prefix='word_')\n",
        "# print_corpuses(corpuses)\n",
        "# print_corpuses(ref_corpuses)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVxXubYkvQ3O"
      },
      "source": [
        "# step 5\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "hyperparams[\"vocab_type\"] = \"word\"\n",
        "hyperparams[\"trim_type\"] = \"top_k\"\n",
        "hyperparams[\"src_k\"] = 50\n",
        "hyperparams[\"trg_k\"] = 50\n",
        "vocabs, corpuses, ref_corpuses = construct_model_data(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", \"test.de\", hyperparams=hyperparams,\n",
        "                     corpus_path=corpus_path, data_path=data_path, model_name=model_name\n",
        "                    )\n",
        "\n",
        "# step 6\n",
        "model_data = retrieve_model_data(data_path=data_path, model_name=model_name)\n",
        "\n",
        "train_batches = model_data[\"train_batches\"]\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "test_batches = model_data[\"test_batches\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "ref_corpuses = model_data[\"ref_corpuses\"]\n",
        "hyperparams = model_data[\"hyperparams\"]\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(vocabs)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(corpuses)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(train_batches)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(dev_batches)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(test_batches)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(idx_to_trg_word)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "print(ref_corpuses)\n",
        "print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "# print(hyperparams)\n",
        "# print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA33AdNwCgBk"
      },
      "source": [
        "# step 8\n",
        "# overfit on training set of 5 sentences\n",
        "references = ref_corpuses[\"train.en\"]\n",
        "model = train(hyperparams, train_batches, dev_batches, references, idx_to_trg_word, checkpoint_path, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}