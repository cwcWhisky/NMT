{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "LdgTryCSH3VW",
    "outputId": "5e1e31f8-cccb-49ef-d80d-11d8184fb4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from encoderdecoder import *\n",
    "from batch import *\n",
    "from traintest import *\n",
    "from processCorpuses import *\n",
    "\n",
    "# from batch import *\n",
    "# from processCorpuses import *\n",
    "# from unrolled_encdec import *\n",
    "# from unrolled_traintest import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "abNQ9Pk-CbQ9",
    "outputId": "64ea3c1e-5f45-424a-d1e9-c8c7a2921008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 13 21:59:03 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "BmyqPOpNC_cn",
    "outputId": "e4841a26-5de6-4946-b58a-14b69275b660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting subword-nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
      "Installing collected packages: subword-nmt\n",
      "Successfully installed subword-nmt-0.3.6\n"
     ]
    }
   ],
   "source": [
    "pip install subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2d5CtGg8-jdx"
   },
   "outputs": [],
   "source": [
    "path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
    "norm_texts = normalizeCorpuses(path)\n",
    "train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = norm_texts[0], norm_texts[1], norm_texts[2], norm_texts[3], norm_texts[4]\n",
    "\n",
    "references = load_docs(path, tok=True)\n",
    "\n",
    "train_references = []\n",
    "for ref in references[1]:\n",
    "    ref = ref.replace(\" - \", \"-\")\n",
    "    train_references.append([ref.split()])\n",
    "    \n",
    "dev_references = []\n",
    "for ref in references[3]:\n",
    "    ref = ref.replace(\" - \", \"-\")\n",
    "    dev_references.append([ref.split()])\n",
    "    \n",
    "\n",
    "for sent in train_trg_sentences[:5]:\n",
    "    print(sent)\n",
    "for sent in train_references[:5]:\n",
    "    print(sent)    \n",
    "print()\n",
    "for sent in dev_trg_sentences[:5]:\n",
    "    print(sent)    \n",
    "for sent in dev_references[:5]:\n",
    "    print(sent)   \n",
    "print()\n",
    "\n",
    "train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
    "train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
    "dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
    "dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
    "test_src_sentences = [sent.split() for sent in test_src_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LiZZZslF3H1d"
   },
   "outputs": [],
   "source": [
    "for sent in train_trg_sentences[:5]:\n",
    "    print(sent)\n",
    "for sent in train_references[:5]:\n",
    "    print(sent)    \n",
    "print()\n",
    "for sent in dev_trg_sentences[:5]:\n",
    "    print(sent)    \n",
    "for sent in dev_references[:5]:\n",
    "    print(sent)   \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxE2E54aq9Ms"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embType = 'jointBPE'\n",
    "#embType = 'word'\n",
    "\n",
    "if embType == 'word':\n",
    "  \n",
    "    \n",
    "\n",
    "    src_counter, trg_counter = to_counters(train_src_sentences, train_trg_sentences)\n",
    "    #print(trg_counter)\n",
    "    #trim_type = \"topK\"\n",
    "    trim_type = \"threshold\"\n",
    "    if trim_type == \"threshold\":\n",
    "        st = 5\n",
    "        tt = 5\n",
    "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs(src_counter, trg_counter, srcThres=st, trgThres=tt)\n",
    "    elif trim_type == \"topK\":\n",
    "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs2(src_counter, trg_counter, srcK=60000, trgK=45000)\n",
    "\n",
    "    train_src_sentences = removeOOV(train_src_sentences, trimmed_src_vocab)\n",
    "    train_trg_sentences = removeOOV(train_trg_sentences, trimmed_trg_vocab)\n",
    "    dev_src_sentences = removeOOV(dev_src_sentences, trimmed_src_vocab)\n",
    "    test_src_sentences = removeOOV(test_src_sentences, trimmed_src_vocab)\n",
    "\n",
    "    train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
    "    srcV, trgV, idx_to_src_word, idx_to_trg_word = computeVocabs(train_src_sentences, train_trg_sentences)\n",
    "\n",
    "    train_src_sentences = toIndices(train_src_sentences, srcV)\n",
    "    train_trg_sentences = toIndices(train_trg_sentences, trgV)\n",
    "    dev_src_sentences = toIndices(dev_src_sentences, srcV)   \n",
    "    test_src_sentences = toIndices(test_src_sentences, srcV)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "z93BFGKrYKXY",
    "outputId": "712538df-8b01-4a3b-9ce6-ab888c382838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning joint bpe and vocab using 30000 merge operations...\n",
      "applying bpe with vocab threshold of 10 to train...\n",
      "applying bpe with vocab threshold of 10 to dev and test...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if embType == 'jointBPE':\n",
    "    alreadyExists = False\n",
    "    if not alreadyExists:\n",
    "        numMerges = 30000\n",
    "        vocabThreshold = 10\n",
    "        !bash jointBPE.sh 30000 10 '/content/gdrive/My Drive/iwslt16_en_de/'\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "id": "OPITtnP8YYm7",
    "outputId": "4ffffe44-50ed-4f1a-cd06-86dcf96c20df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Gall@@ o : this is Bill L@@ ange . I'm Dave Gall@@ o .\n",
      "and we're going to tell you some stories from the sea here in video .\n",
      "we've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it .\n",
      "the truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea .\n",
      "and the problem , I think , is that we take the ocean for granted .\n",
      "[['David', 'Gall@@', 'o', ':', 'this', 'is', 'Bill', 'L@@', 'ange', '.', \"I'm\", 'Dave', 'Gall@@', 'o', '.']]\n",
      "[['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
      "[[\"we've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
      "[['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
      "[['and', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
      "\n",
      "when I was in my 20s , I saw my very first psycho@@ therapy client .\n",
      "I was a Ph.D. student in clinical psychology at Berkeley .\n",
      "she was a 26 - year - old woman named Alex .\n",
      "now Alex walked into her first session wearing je@@ ans and a big sl@@ ou@@ chy top , and she dropped onto the couch in my office and kicked off her fl@@ ats and told me she was there to talk about guy problems .\n",
      "now when I heard this , I was so relie@@ ved .\n",
      "[['when', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psycho@@', 'therapy', 'client', '.']]\n",
      "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
      "[['she', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
      "[['now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'je@@', 'ans', 'and', 'a', 'big', 'sl@@', 'ou@@', 'chy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'fl@@', 'ats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
      "[['now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relie@@', 'ved', '.']]\n",
      "\n",
      "number of german subwords is 23056\n",
      "number of english subwords is 6822\n",
      "total number of subwords is 29881\n",
      "error: unknown word: û@@ in sent: ['mein', 'Favor@@', 'it', 'ist', 'der', 'in', 'der', 'Mitte', '--', 'MP3', '-', 'Player', '-', ',', 'N@@', 'asen', '-', 'Haar', '-', 'Tri@@', 'mmer', 'und', 'Cr@@', 'è@@', 'me', 'Br@@', 'û@@', 'l@@', 'é@@', 'e', 'F@@', 'ack@@', 'el', '.']\n",
      "error: unknown word: ê in sent: ['ash@@', 'ê', 'Ol@@', 'ê@@', 'n', '.', 'in', 'meiner', 'Sprache', 'bedeutet', 'das', ':', 'ich', 'danke', 'Ihnen', 'sehr', '.']\n",
      "error: unknown word: τ in sent: ['und', 'die', 'Antwort', ',', 'glaube', 'ich', ',', 'ist', 'ja', '.', '[', '\"', 'f', 'T', 'S@@', 'τ', '\"', ']', '.', 'was', 'Sie', 'gerade', 'sehen', ',', 'ist', 'wahrscheinlich', 'die', 'beste', 'Ent@@', 'spre@@', 'chung', 'zu', 'E', 'm@@', 'c@@', '²', 'für', 'Intelligenz', ',', 'die', 'ich', 'gesehen', 'habe', '.']\n",
      "error: unknown word: Œ@@ in sent: ['ich', 'begann', 'ein', 'neues', 'Œ@@', 'u', 'v', 're', '.']\n"
     ]
    }
   ],
   "source": [
    "path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
    "\n",
    "bpe_texts = load_docs(path, bpe=True)\n",
    "train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = bpe_texts[0], bpe_texts[1], bpe_texts[2], bpe_texts[3], bpe_texts[4]\n",
    "\n",
    "train_references = []\n",
    "for ref in train_trg_sentences[:]: # until think of better way\n",
    "    ref = ref.replace(\" - \", \"-\")\n",
    "    train_references.append([ref.split()])\n",
    "    \n",
    "dev_references = []\n",
    "for ref in dev_trg_sentences[:]:\n",
    "    ref = ref.replace(\" - \", \"-\")\n",
    "    dev_references.append([ref.split()])\n",
    "    \n",
    "\n",
    "for sent in train_trg_sentences[:5]:\n",
    "    print(sent)\n",
    "for sent in train_references[:5]:\n",
    "    print(sent)    \n",
    "print()\n",
    "for sent in dev_trg_sentences[:5]:\n",
    "    print(sent)    \n",
    "for sent in dev_references[:5]:\n",
    "    print(sent)   \n",
    "print()\n",
    "\n",
    "train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
    "train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
    "dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
    "dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
    "test_src_sentences = [sent.split() for sent in test_src_sentences]\n",
    "\n",
    "\n",
    "train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
    "\n",
    "vocab, idx_to_subword = computeBPEvocabs(path + \"vocab.de\", path + \"vocab.en\")\n",
    "\n",
    "train_src_sentences = toIndices(train_src_sentences, vocab)\n",
    "train_trg_sentences = toIndices(train_trg_sentences, vocab)\n",
    "dev_src_sentences = toIndices(dev_src_sentences, vocab)   \n",
    "test_src_sentences = toIndices(test_src_sentences, vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OOI1bMHrcwpZ",
    "outputId": "0fdc242a-fdc3-4db1-ce38-9928e3adc96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "overfit_on_train = False\n",
    "\n",
    "if overfit_on_train:\n",
    "    b = 10\n",
    "    #dev_bsz = 10\n",
    "    dev_bsz = 1\n",
    "else:\n",
    "    b = 64\n",
    "    dev_bsz = 64\n",
    "    #dev_bsz = 1\n",
    "num_ep = 30 # number of epochs\n",
    "IS = 300 # input size\n",
    "enc_hs = 600 # encoder hidden size\n",
    "dec_hs = 600 # decoder hidden size\n",
    "enc_NL = 1 # number of layers\n",
    "dec_NL = 1\n",
    "enc_DR = .0 # dropout\n",
    "dec_DR = .0\n",
    "bi_enc = True\n",
    "project = True\n",
    "reverse_src = False\n",
    "tie_weights = True\n",
    "customLSTM = False\n",
    "embType = \"jointBPE\"\n",
    "trim_type = None\n",
    "inputFeeding = True\n",
    "K = .7\n",
    "LR = .001 # learning rate\n",
    "WD = .0\n",
    "\n",
    "enc_emb_drop = .1\n",
    "dec_emb_drop = .1\n",
    "dropconnect = .5\n",
    "\n",
    "opt = \"ADAM\" # optimization algorithm\n",
    "model_name = 'BPE_h600/'\n",
    "msg = ' \\n'\n",
    "\n",
    "\n",
    "encoder_params = {}\n",
    "if embType == \"word\":\n",
    "    encoder_params['vocab_size'] = len(srcV)\n",
    "elif embType == \"jointBPE\":\n",
    "    encoder_params['vocab_size'] = len(vocab)\n",
    "encoder_params['input_size'] = IS\n",
    "encoder_params['hidden_size'] = enc_hs\n",
    "encoder_params['num_layers'] = enc_NL\n",
    "encoder_params['dropout'] = enc_DR\n",
    "encoder_params['dev'] = device\n",
    "encoder_params['bi_enc'] = bi_enc\n",
    "encoder_params['project'] = project # project concated enc states to dim of decoder\n",
    "#encoder_params['src_emb_drop'] = enc_emb_drop\n",
    "#encoder_params['dropconnect'] = dropconnect\n",
    "encoder_params['reverse_src'] = reverse_src\n",
    "#encoder_params['variational_drop'] = .5\n",
    "#encoder_params['i0_drop'] = .2\n",
    "#encoder_params['i1_drop'] = .5\n",
    "encoder_params['out_drop'] = .5\n",
    "encoder_params['init_scheme'] = 'layer_to_layer'\n",
    "#encoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
    "encoder_params['customLSTM'] = customLSTM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_params = {}\n",
    "if embType == \"word\":\n",
    "    decoder_params['sosIdx'] = trgV['<sos>']\n",
    "    #print(decoder_params['sosIdx'])\n",
    "    decoder_params['eosIdx'] = trgV['<eos>']\n",
    "    #print(decoder_params['eosIdx'])\n",
    "\n",
    "    decoder_params['padIdx'] = trgV['<pad>']\n",
    "    #print(decoder_params['padIdx'])\n",
    "\n",
    "    decoder_params['vocab_size'] = len(trgV)\n",
    "    decoder_params['idx_to_trg_word'] = idx_to_trg_word\n",
    "\n",
    "elif embType == \"jointBPE\":\n",
    "\n",
    "    decoder_params['sosIdx'] = vocab['<sos>']\n",
    "    #print(decoder_params['sosIdx'])\n",
    "    decoder_params['eosIdx'] = vocab['<eos>']\n",
    "    #print(decoder_params['eosIdx'])\n",
    "\n",
    "    decoder_params['padIdx'] = vocab['<pad>']\n",
    "    #print(decoder_params['padIdx'])\n",
    "\n",
    "    decoder_params['vocab_size'] = len(vocab)\n",
    "    decoder_params['idx_to_trg_word'] = idx_to_subword\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_params['input_size'] = IS\n",
    "decoder_params['hidden_size'] = dec_hs\n",
    "decoder_params['num_layers'] = dec_NL\n",
    "decoder_params['dropout'] = dec_DR\n",
    "#decoder_params['trg_emb_drop'] = dec_emb_drop\n",
    "#decoder_params['dropconnect'] = dropconnect\n",
    "\n",
    "decoder_params['dev'] = device\n",
    "decoder_params['attention'] = \"global_att\"\n",
    "decoder_params['inf_alg'] = \"greedy_search\"\n",
    "#decoder_params['inf_alg'] = \"beam_search\"\n",
    "\n",
    "decoder_params['beam_size'] = 10\n",
    "decoder_params['decode_slack'] = 8\n",
    "#decoder_params['variational_drop'] = .5\n",
    "#decoder_params['i0_drop'] = .0 # applied to trg embeddings prior to entering decoder lstm\n",
    "#decoder_params['i1_drop'] = .5\n",
    "decoder_params['out_drop'] = .5 # applied after final layer of decoder lstm\n",
    "#decoder_params['att_drop'] = .3 # applied after project back to input size\n",
    "#decoder_params['rec_drop'] = .5\n",
    "\n",
    "decoder_params['tie_weights'] = tie_weights\n",
    "#decoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
    "decoder_params['customLSTM'] = customLSTM\n",
    "#decoder_params['input_feeding'] = inputFeeding\n",
    "#decoder_params['K'] = K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = Encoder(encoder_params)\n",
    "decoder = Decoder(decoder_params)\n",
    "translator = RNNencdec(encoder, decoder, embType) # initialize model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    translator.cuda()\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(translator.parameters(), lr=LR, weight_decay=WD) # initialize optimizer\n",
    "\n",
    "# folder to write checkpoints (train outputs) and translations (inference outputs)\n",
    "folder = '/content/gdrive/My Drive/MToutputs/' + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GR15Lmr1tht"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6naTP-71riD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbBgLZHAjfHH"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "x = !nvidia-smi | head -n 8 | tail -n 1 | cut -d ' ' -f 6-7\n",
    "current_gpu = ' '.join(x)\n",
    "with open(folder + 'model_train_stats.txt', 'w') as f:\n",
    "    f.write(msg)\n",
    "    if trim_type == \"threshold\":\n",
    "        f.write(\"srcThres: {}\\n\".format(st))\n",
    "        f.write(\"trgThres: {}\\n\".format(tt))\n",
    "\n",
    "    f.write(\"encoder_params:\\n\")\n",
    "    for key in encoder_params:\n",
    "        f.write(key + \": {}\\n\".format(encoder_params[key]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"decoder_params:\\n\")\n",
    "    for key in decoder_params:\n",
    "        if key != \"idx_to_trg_word\":\n",
    "            f.write(key + \": {}\\n\".format(decoder_params[key]))\n",
    "        \n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"embType: {}\\n\".format(embType))\n",
    "    f.write(\"lr: {}\\n\".format(LR))\n",
    "    f.write(\"wd: {}\\n\".format(WD))\n",
    "    f.write(\"opt: {}\\n\".format(opt))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"bsz: {}\\n\".format(b))\n",
    "    f.write(\"dev_bsz: {}\\n\".format(dev_bsz))\n",
    "    f.write(\"current gpu: {}\\n\".format(current_gpu))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "qZnY73CfjlZu",
    "outputId": "0a878fc6-4dd6-4d03-af4d-632a1e5a5bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting by trg length\n",
      "took 32.94 seconds to get all the batches\n",
      "took 0.69 seconds to get all the devBatches\n"
     ]
    }
   ],
   "source": [
    "if reverse_src:\n",
    "    if embType == \"word\":\n",
    "\n",
    "        for sent in train_src_sentences[:3]:\n",
    "            print([idx_to_src_word[idx] for idx in sent])    \n",
    "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
    "        for sent in train_src_sentences[:3]:\n",
    "            print([idx_to_src_word[idx] for idx in sent])    \n",
    "\n",
    "        print()\n",
    "\n",
    "        for sent in dev_src_sentences[:3]:\n",
    "            print([idx_to_src_word[idx] for idx in sent])\n",
    "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
    "        for sent in dev_src_sentences[:3]:\n",
    "            print([idx_to_src_word[idx] for idx in sent])    \n",
    "\n",
    "    elif embType == \"jointBPE\":\n",
    "        for sent in train_src_sentences[:3]:\n",
    "            print([idx_to_subword[idx] for idx in sent])    \n",
    "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
    "        for sent in train_src_sentences[:3]:\n",
    "            print([idx_to_subword[idx] for idx in sent]) \n",
    "\n",
    "        print()\n",
    "\n",
    "        for sent in dev_src_sentences[:3]:\n",
    "            print([idx_to_subword[idx] for idx in sent])\n",
    "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
    "        for sent in dev_src_sentences[:3]:\n",
    "            print([idx_to_subword[idx] for idx in sent])  \n",
    "\n",
    "trainingPairs = list(zip(train_src_sentences, train_trg_sentences))\n",
    "\n",
    "unrolled = False\n",
    "if overfit_on_train: \n",
    "    trainBatches = getBatches(trainingPairs[:10], b, device, unrolledLSTM=unrolled)\n",
    "    devBatches = getDevBatches(train_src_sentences[:10], dev_bsz, device, unrolledLSTM=unrolled)\n",
    "else:\n",
    "    trainBatches = getBatches(trainingPairs, b, device, unrolledLSTM=unrolled)\n",
    "    devBatches = getDevBatches(dev_src_sentences, dev_bsz, device, unrolledLSTM=unrolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "jofhcYlQ7tvh",
    "outputId": "aa2183ef-6db3-4495-8676-7b24ef644f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 00, loss: 18938.91, ep_t: 214.10 sec, t_t: 8.88 sec, bleu: 0.0087\n",
      "ep: 01, loss: 13301.22, ep_t: 225.76 sec, t_t: 8.73 sec, bleu: 0.0969\n",
      "ep: 02, loss: 9616.84, ep_t: 226.41 sec, t_t: 8.72 sec, bleu: 0.1728\n",
      "ep: 03, loss: 8263.96, ep_t: 225.97 sec, t_t: 8.70 sec, bleu: 0.1928\n",
      "ep: 04, loss: 7567.79, ep_t: 226.07 sec, t_t: 8.72 sec, bleu: 0.2111\n",
      "ep: 05, loss: 7111.67, ep_t: 226.01 sec, t_t: 8.69 sec, bleu: 0.2174\n",
      "ep: 06, loss: 6771.92, ep_t: 225.70 sec, t_t: 8.74 sec, bleu: 0.2274\n",
      "ep: 07, loss: 6504.22, ep_t: 226.34 sec, t_t: 8.69 sec, bleu: 0.2328\n",
      "ep: 08, loss: 6284.37, ep_t: 225.97 sec, t_t: 8.71 sec, bleu: 0.2370\n",
      "ep: 09, loss: 6092.84, ep_t: 225.98 sec, t_t: 9.41 sec, bleu: 0.2411\n",
      "ep: 10, loss: 5933.34, ep_t: 226.00 sec, t_t: 8.71 sec, bleu: 0.2374\n",
      "ep: 11, loss: 5787.65, ep_t: 226.00 sec, t_t: 8.81 sec, bleu: 0.2406\n",
      "ep: 12, loss: 5658.57, ep_t: 226.16 sec, t_t: 8.75 sec, bleu: 0.2411\n",
      "ep: 13, loss: 5547.45, ep_t: 225.94 sec, t_t: 8.72 sec, bleu: 0.2403\n",
      "ep: 14, loss: 5443.08, ep_t: 225.93 sec, t_t: 8.73 sec, bleu: 0.2412\n",
      "ep: 15, loss: 5346.23, ep_t: 226.16 sec, t_t: 8.80 sec, bleu: 0.2432\n",
      "ep: 16, loss: 5256.16, ep_t: 226.21 sec, t_t: 8.73 sec, bleu: 0.2402\n",
      "ep: 17, loss: 5175.51, ep_t: 226.41 sec, t_t: 8.72 sec, bleu: 0.2428\n",
      "ep: 18, loss: 5098.38, ep_t: 226.07 sec, t_t: 8.69 sec, bleu: 0.2442\n",
      "ep: 19, loss: 5026.27, ep_t: 226.15 sec, t_t: 8.71 sec, bleu: 0.2404\n",
      "ep: 20, loss: 4958.06, ep_t: 226.21 sec, t_t: 8.73 sec, bleu: 0.2400\n",
      "ep: 21, loss: 4893.57, ep_t: 226.06 sec, t_t: 8.70 sec, bleu: 0.2427\n",
      "ep: 22, loss: 4835.25, ep_t: 225.95 sec, t_t: 8.74 sec, bleu: 0.2405\n",
      "ep: 23, loss: 4777.14, ep_t: 226.10 sec, t_t: 8.74 sec, bleu: 0.2445\n",
      "ep: 24, loss: 4724.10, ep_t: 225.89 sec, t_t: 8.71 sec, bleu: 0.2391\n",
      "ep: 25, loss: 4670.40, ep_t: 225.94 sec, t_t: 9.39 sec, bleu: 0.2427\n",
      "ep: 26, loss: 4622.09, ep_t: 225.99 sec, t_t: 8.76 sec, bleu: 0.2387\n",
      "ep: 27, loss: 4570.37, ep_t: 226.13 sec, t_t: 8.77 sec, bleu: 0.2366\n",
      "ep: 28, loss: 4529.01, ep_t: 226.21 sec, t_t: 8.75 sec, bleu: 0.2393\n",
      "ep: 29, loss: 4486.18, ep_t: 226.49 sec, t_t: 8.71 sec, bleu: 0.2394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if overfit_on_train:\n",
    "    translator = train(translator, optimizer, trainBatches, devBatches, train_references[:10], num_epochs=num_ep, cur_ep=0, folder=folder, save=False)\n",
    "else:\n",
    "    translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=0, folder=folder, save=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_duJkeSv_C0P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSp8N_w8h9ug"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYcGOvvhh67b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "gKF9f65B_C5I",
    "outputId": "a4edad2c-bb20-41cb-ae90-49277c7a1628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "ep_loss: 5310.45263671875\n",
      "last_finished_epoch: 16\n",
      "t_t: 264.22928047180176, bleu: 0.25813851467550597\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "model_name = 'BPE_h500/'\n",
    "folder = '/content/gdrive/My Drive/MToutputs/' + model_name\n",
    "\n",
    "epoch = 16 # desired checkpoint to load\n",
    "checkpoint = torch.load(folder + 'cp' + str(epoch) + '.tar') \n",
    "translator.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "ep_loss = checkpoint['ep_loss']\n",
    "last_finished_epoch = checkpoint['epoch']\n",
    "next_ep = last_finished_epoch + 1 # becomes first cur_ep value, if continue training this model\n",
    "print(\"ep_loss: {}\".format(ep_loss))\n",
    "print(\"last_finished_epoch: {}\".format(last_finished_epoch))\n",
    "\n",
    "# perform beam search on best model:\n",
    "translator.decoder.inf_alg = \"beam_search\"\n",
    "translator.decoder.beam_size = 10\n",
    "\n",
    "translator.eval()\n",
    "test_time, drop_bleu, bleu_time = test(translator, devBatches, dev_references, folder, last_finished_epoch, write=True)\n",
    "print(\"t_t: {}, bleu: {}\".format(test_time, drop_bleu))\n",
    "\n",
    "#!!!new experiment: overwrite the LR of the optimizer to implement a quasi-LR schedule\n",
    "#for g in optimizer.param_groups:\n",
    "#    g['lr'] = .00005\n",
    "#optimizer.state_dict()\n",
    "# continue training\n",
    "#translator.train()\n",
    "#translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=next_ep, folder=folder, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvLpjfzq_C9r"
   },
   "outputs": [],
   "source": [
    "# compare predictions with targets\n",
    "\n",
    "with open('/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt', 'r') as f:\n",
    "    preds = f.read()\n",
    "    \n",
    "    \n",
    "preds_list = to_sentences([preds])\n",
    "preds = preds_list[0]\n",
    "for i in range(250, 300):\n",
    "    print(\"pred:  \", preds[i])\n",
    "    print(\"target:\", ' '.join(dev_references[i][0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a3mE__fRA2Rr",
    "outputId": "23ade307-1418-49df-9981-e0594cdf148b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281:that's why instagram is fat .\n"
     ]
    }
   ],
   "source": [
    "!grep -n \"instagram\" \"/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7vAf_tU_Qg-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zv77gLd0_DDs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-f7hjOX6_DG1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yA_nYBd_DLw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvA4GnQY_DOR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaNfh_2y_DJ_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1sXMYPT_C8E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wsvTGiX_C3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "JFgvMjYH1P8B",
    "outputId": "153de8ce-12ab-441b-da2e-f67cefc8e6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david gallo this is bill lange . i'm dave gallo .\n",
      "and we're going to tell you some stories from the sea here in video .\n",
      "we've got some of the most incredible video of titanic that's ever been seen , and we're not going to show you any of it .\n",
      "the truth of the about is that the titanic - - even though it's breaking all sorts of box office records - - it's not the most exciting story from the sea .\n",
      "and the problem , i think , is that we take the ocean for granted .\n",
      "when you think about it , the oceans are 75 percent of the planet .\n",
      "most of the planet is ocean water .\n",
      "the average depth is about two miles .\n",
      "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
      "and in the oceans , there are the longest mountain ranges on the planet .\n"
     ]
    }
   ],
   "source": [
    "# now use parallelized version:\n",
    "translator.cuda()\n",
    "translator.encoder.dev = \"cuda:0\"\n",
    "translator.decoder.dev = \"cuda:0\"\n",
    "translator.eval()\n",
    "#print(\"making greedy predictions on the training set...\")\n",
    "translator.decoder.alg = \"greedy\"\n",
    "train_g_translations, test_time, bleu, bleu_time = test(translator, devBatches, train_references[:10])\n",
    "\n",
    "for trans in train_g_translations:\n",
    "    print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0roBdyGf2Rus"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "id": "ecJqDlfY1nTS",
    "outputId": "dc06809f-d71a-4eb5-b0bd-e1bd94e39a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david gallo this is bill lange . i'm dave gallo .\n",
      "and we're going to tell you some stories from the sea here in video .\n",
      "part of the problem , i think , is we take the beach , or we see images like this of the ocean , and you look out at the great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there .\n",
      "the truth of the matter is that the titanic - - even though it's breaking all sorts of box office - - - the not not the most exciting story from the sea .\n",
      "and the problem , i think , is that we take the ocean for granted .\n",
      "and the problem about it , the oceans are 75 percent of the planet .\n",
      "most of the planet is ocean water .\n",
      "the average depth is about two miles .\n",
      "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
      "and in the oceans , there are the longest mountain ranges on the planet .\n",
      "most of the animals are in the oceans .\n",
      "most of the earthquakes and volcanoes are in the sea , at the bottom of the sea , at the bottom of the sea , at the\n",
      "the biodiversity and the <unk> in the ocean is higher , in places , than it is in the rainforests .\n",
      "it's mostly unexplored , and yet there are beautiful sights like this that <unk> us and make us become familiar with it .\n",
      "but when you're standing at the beach , i want you to think that you're standing at the edge of a very unfamiliar world .\n",
      "we have to have a very special technology to get into that unfamiliar world .\n",
      "we use the submarine alvin and we use cameras , and the cameras are something that that lange has developed with the help of sony .\n",
      "people that have partnered with us given us new eyes , not only on what exists - - the new landscapes as having having new eyes .\n",
      "people that have partnered with us have given us new eyes , not only on what exists - - the new landscapes at the bottom of the sea - - but also how we think about life on the planet itself .\n",
      "here's a jelly .\n",
      "it's one of my favorites , because it's got all sorts of working parts .\n",
      "this turns out to be the longest creature in the oceans .\n",
      "it gets up to about 150 feet long .\n",
      "but see all those different working things ?\n",
      "i love that kind of stuff .\n",
      "this turns out fishing <unk> on the bottom . they're going up and down .\n",
      "it's got tentacles dangling , swirling around like that .\n",
      "here's a that jelly .\n",
      "these are all individual animals banding together to make this one creature .\n",
      "and it's got these jet thrusters up in front that it'll use in a moment , and a little light .\n"
     ]
    }
   ],
   "source": [
    "for trans in train_g_translations:\n",
    "    print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKQTJXz8gfZt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Etb4QHe61nb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ivpDqfV1nfD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGUUCRWN1nk9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "RR1_3O_3bFij",
    "outputId": "ada868fc-1cae-48d0-8263-0b8222d91394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model 0...\n",
      "epoch: 0, loss: 7208.15, time: 192.49 sec\n",
      "making greedy predictions on the dev set with model 0...\n",
      "...took 118.42 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### early-stopping workaround\n",
    "fold = \"/content/gdrive/My Drive/models/\" # folder\n",
    "# for epoch 0, train the model from scratch\n",
    "print(\"training model 0...\")\n",
    "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=1, cur_ep=0, learn_rate=LR, folder=fold)\n",
    "\n",
    "# perform inference with this from-scratch model\n",
    "if torch.cuda.is_available():\n",
    "    translator.cpu()\n",
    "    translator.encoder.dev = \"cpu\"\n",
    "    translator.decoder.dev = \"cpu\"\n",
    "translator.eval()\n",
    "print(\"making greedy predictions on the dev set with model 0...\")\n",
    "start_time = time.time()\n",
    "#dev_g_translations = test(translator, dev_src_sentences)\n",
    "# only estimate the BLEU using ~1/8 of the dev set:\n",
    "dev_g_translations = test(translator, dev_src_sentences[:1000])\n",
    "\n",
    "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "print()\n",
    "# Create a local file to upload.\n",
    "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds0.txt', 'w') as f:\n",
    "    for translation in dev_g_translations:\n",
    "        f.write(translation + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4L2Mum2gwHY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1075
    },
    "colab_type": "code",
    "id": "KGfS4qoBayLV",
    "outputId": "91b32ef7-d2bd-4868-d457-964423832eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7208.1484, device='cuda:0', requires_grad=True)\n",
      "training model 1...\n",
      "epoch: 1, loss: 5146.83, time: 187.54 sec\n",
      "making greedy predictions on the dev set with model 1...\n",
      "...took 111.95 seconds\n",
      "\n",
      "tensor(5146.8330, device='cuda:0', requires_grad=True)\n",
      "training model 2...\n",
      "epoch: 2, loss: 4453.28, time: 186.93 sec\n",
      "making greedy predictions on the dev set with model 2...\n",
      "...took 106.26 seconds\n",
      "\n",
      "tensor(4453.2798, device='cuda:0', requires_grad=True)\n",
      "training model 3...\n",
      "epoch: 3, loss: 4072.48, time: 187.05 sec\n",
      "making greedy predictions on the dev set with model 3...\n",
      "...took 102.36 seconds\n",
      "\n",
      "tensor(4072.4817, device='cuda:0', requires_grad=True)\n",
      "training model 4...\n",
      "epoch: 4, loss: 3811.85, time: 186.74 sec\n",
      "making greedy predictions on the dev set with model 4...\n",
      "...took 107.40 seconds\n",
      "\n",
      "tensor(3811.8545, device='cuda:0', requires_grad=True)\n",
      "training model 5...\n",
      "epoch: 5, loss: 3609.07, time: 186.73 sec\n",
      "making greedy predictions on the dev set with model 5...\n",
      "...took 110.42 seconds\n",
      "\n",
      "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
      "training model 6...\n",
      "epoch: 6, loss: 3442.49, time: 185.58 sec\n",
      "making greedy predictions on the dev set with model 6...\n",
      "...took 87.56 seconds\n",
      "\n",
      "tensor(3442.4900, device='cuda:0', requires_grad=True)\n",
      "training model 7...\n",
      "epoch: 7, loss: 3300.97, time: 185.10 sec\n",
      "making greedy predictions on the dev set with model 7...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4c0ee8171b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#dev_g_translations = test(new_translator, dev_src_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# only estimate the BLEU using ~1/8 of the dev set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdev_g_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_translator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_src_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...took %0.2f seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(translator, testSrcSentences)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mencoder_inputs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSrcSent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2D tensor of size (1 x src_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mencoder_inputs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mtranslations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# don't need the input (decoder_inputs_batch = None), so skip its preprocessing steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"greedy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGreedyTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"beam_search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBeamSearchTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mgetGreedyTranslation\u001b[0;34m(self, encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;31m# so reshape to accomodate hidden state for a single time step for a single sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;31m# get_attStates() also expects a mask, so pass it (1 x 1 x Lsrc) mask of all 0s (merely leaves scores intact)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mdecoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# get_attStates() returns (q x hs), which is (1 x hs), in this case, so of correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;31m###!!!new pytorch no longer returns the lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m#packed_contexts, _ = pack_padded_sequence(padded_contexts, trg_lengths, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# for epochs 1 thru 6, load previous model from file\n",
    "for i in range(1,16): \n",
    "  \n",
    "    # intialize models so that can load into them\n",
    "    new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
    "    new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
    "    new_translator = RNNencdec(new_encoder, new_decoder)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        new_translator.cuda()\n",
    "    \n",
    "    if opt == \"RMS\":\n",
    "        optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "    elif opt == \"ADAM\":\n",
    "        optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "\n",
    "    checkpoint = torch.load(fold + 'train_checkpoint' + str(i-1) + '.tar') # load previous checkpoint\n",
    "\n",
    "    new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    ep_loss = checkpoint['ep_loss']\n",
    "    print(ep_loss)\n",
    "\n",
    "    new_translator.train()\n",
    "    # train for an additional epoch\n",
    "    print(\"training model \" + str(i) + \"...\")\n",
    "\n",
    "    new_translator, final_loss = train2(new_translator, optimizer, batches, bsz=b, num_epochs=i+1, cur_ep=i, folder=fold)\n",
    "\n",
    "    \n",
    "    # perform inference with the new model\n",
    "    if torch.cuda.is_available():\n",
    "        new_translator.cpu()\n",
    "        new_translator.encoder.dev = \"cpu\"\n",
    "        new_translator.decoder.dev = \"cpu\"\n",
    "    new_translator.eval()\n",
    "    print(\"making greedy predictions on the dev set with model \" + str(i) + \"...\")\n",
    "    start_time = time.time()\n",
    "    #dev_g_translations = test(new_translator, dev_src_sentences)\n",
    "    # only estimate the BLEU using ~1/8 of the dev set:\n",
    "    dev_g_translations = test(new_translator, dev_src_sentences[:1000])\n",
    "    print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "    print()\n",
    "    # Create a local file to upload.\n",
    "    with open('/content/gdrive/My Drive/predictions/dev_greedy_preds' + str(i) + '.txt', 'w') as f:\n",
    "        for translation in dev_g_translations:\n",
    "            f.write(translation + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cHe83oAavCz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1442
    },
    "colab_type": "code",
    "id": "8UrmM0vLF2Ab",
    "outputId": "00f28580-ab6d-4bd6-988a-a68ee70d953c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6262.86, time: 937.87 sec\n",
      "epoch: 1, loss: 4415.96, time: 937.92 sec\n",
      "epoch: 2, loss: 3752.01, time: 938.88 sec\n",
      "epoch: 3, loss: 3260.38, time: 938.36 sec\n",
      "epoch: 4, loss: 2819.71, time: 938.74 sec\n",
      "epoch: 5, loss: 2420.68, time: 939.27 sec\n",
      "epoch: 6, loss: 2073.30, time: 939.06 sec\n",
      "epoch: 7, loss: 1775.67, time: 937.98 sec\n",
      "epoch: 8, loss: 1537.47, time: 937.98 sec\n",
      "epoch: 9, loss: 1343.11, time: 938.27 sec\n",
      "epoch: 10, loss: 1189.45, time: 937.89 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e69b9dd42246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(translator, optimizer, batches, bsz, num_epochs, cur_ep, learn_rate, folder, shuf)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mpackedDists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0mpackedTargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackedDists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackedTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# packed_input is a tuple, whose first comp is the packed sequence, and second comp is \"batchsizes\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# which holds, at position i, the number of inputs for time step i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeProbDists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mcomputeProbDists\u001b[0;34m(self, packed_input, padded_encoder_states, decoder_initial_state, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;31m# convert states to attentional states before projecting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mpacked_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# concatenate contexts with the original decoder states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    145\u001b[0m                       \u001b[0;34m'the trace incorrect for any other combination of lengths.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train a model from scratch\n",
    "### NOTE - this will overwrite any existing checkpoints unless you change the path to where this train loop stores its checkpoints \n",
    "### (must change the fold param)\n",
    "\n",
    "#srcV=20,000, trgV=15,000, dropout=.3\n",
    "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=num_ep, folder=fold) # train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oAB0QVG4Tc37",
    "outputId": "8ed2cd30-ff2f-4693-bd15-dfd2fc64a1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making greedy predictions on the dev set...\n",
      "...took 1905.13 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# perform inference with this from-scratch model\n",
    "\n",
    "# switch to CPU at test time:\n",
    "if torch.cuda.is_available():\n",
    "    translator.cpu()\n",
    "    translator.encoder.dev = \"cpu\"\n",
    "    translator.decoder.dev = \"cpu\"\n",
    "\n",
    "translator.eval()\n",
    "print(\"making greedy predictions on the dev set...\")\n",
    "start_time = time.time()\n",
    "dev_g_translations = test(translator, dev_src_sentences)\n",
    "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "#g_test_accuracy = computeAccuracy(dev_g_translations, dev_trg_sentences, idx_to_trg_word)\n",
    "#print(\"g test accuracy: \" + g_test_accuracy)\n",
    "print()\n",
    "# Create a local file to upload.\n",
    "#with open('dev_greedy_preds.txt', 'w') as f:\n",
    "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds.txt', 'w') as f:\n",
    "    for translation in dev_g_translations:\n",
    "        f.write(translation + '\\n')\n",
    "        print(translation)\n",
    "\n",
    "translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
    "print(\"making beam search predictions on the dev set...\")\n",
    "start_time = time.time()\n",
    "dev_b_translations = test(translator, dev_src_sentences)\n",
    "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "#b_test_accuracy = computeAccuracy(dev_b_translations, dev_trg_sentences, idx_to_trg_word)\n",
    "#print(\"b test accuracy: \" + b_test_accuracy)\n",
    "\n",
    "# Create a local file to upload.\n",
    "with open('/content/gdrive/My Drive/predictions/dev_beam_preds.txt', 'w') as f:\n",
    "    for translation in dev_b_translations:\n",
    "        f.write(translation + '\\n')\n",
    "        print(translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_W_SR2JVHPYE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ctf-I1fsRA7o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "dxgeg_plRClu",
    "outputId": "58f23b56-6afd-4dcd-af1c-9376808d9886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
      "making beam search predictions on the dev set with model 5...\n",
      "...took 1464.46 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint for inference only\n",
    "\n",
    "# intialize models so that can load into them\n",
    "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
    "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
    "new_translator = RNNencdec(new_encoder, new_decoder)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    new_translator.cuda()\n",
    "\n",
    "if opt == \"RMS\":\n",
    "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "elif opt == \"ADAM\":\n",
    "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "\n",
    "checkpoint = torch.load(fold + 'train_checkpoint' + str(5) + '.tar') # load previous checkpoint\n",
    "\n",
    "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "ep_loss = checkpoint['ep_loss']\n",
    "print(ep_loss)\n",
    "\n",
    "# perform inference with the new model\n",
    "if torch.cuda.is_available():\n",
    "    new_translator.cpu()\n",
    "    new_translator.encoder.dev = \"cpu\"\n",
    "    new_translator.decoder.dev = \"cpu\"\n",
    "new_translator.eval()\n",
    "new_translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
    "new_translator.decoder.beam_size = 10\n",
    "print(\"making beam search predictions on the dev set with model \" + str(5) + \"...\")\n",
    "\n",
    "#print(\"making greedy predictions on the dev set with model \" + str(5) + \"...\")\n",
    "start_time = time.time()\n",
    "dev_b_translations = test(new_translator, dev_src_sentences[:1000])\n",
    "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "print()\n",
    "# Create a local file to upload.\n",
    "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_bs10' + str(5) + '.txt', 'w') as f:\n",
    "    for translation in dev_b_translations:\n",
    "        f.write(translation + '\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVRqgImXg7P4"
   },
   "outputs": [],
   "source": [
    "# load checkpoint for inference only\n",
    "###BPE beam search\n",
    "\n",
    "\n",
    "# intialize models so that can load into them\n",
    "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
    "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
    "new_translator = RNNencdec(new_encoder, new_decoder)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    new_translator.cuda()\n",
    "\n",
    "if opt == \"RMS\":\n",
    "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "elif opt == \"ADAM\":\n",
    "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
    "\n",
    "checkpoint = torch.load(fold + 'train_checkpoint' + str(19) + '.tar') # load previous checkpoint\n",
    "\n",
    "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "ep_loss = checkpoint['ep_loss']\n",
    "print(ep_loss)\n",
    "\n",
    "# perform inference with the new model\n",
    "if torch.cuda.is_available():\n",
    "    new_translator.cpu()\n",
    "    new_translator.encoder.dev = \"cpu\"\n",
    "    new_translator.decoder.dev = \"cpu\"\n",
    "new_translator.eval()\n",
    "print(\"making greedy predictions on the test set with model \" + str(19) + \"...\")\n",
    "start_time = time.time()\n",
    "dev_g_translations = test(new_translator, dev_src_sentences)\n",
    "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
    "print()\n",
    "# Create a local file to upload.\n",
    "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_BPE' + str(19) + '.txt', 'w') as f:\n",
    "    for translation in dev_g_translations:\n",
    "        f.write(translation + '\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "690Dproject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
