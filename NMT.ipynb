{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/recurrent-NMT/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdgTryCSH3VW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e2f57a10-8fa1-4b8e-f4b8-6ee66591eed2"
      },
      "source": [
        "from encoderdecoder import *\n",
        "from batch import *\n",
        "from traintest import *\n",
        "from processCorpuses import *\n",
        "\n",
        "# from batch import *\n",
        "# from processCorpuses import *\n",
        "# from unrolled_encdec import *\n",
        "# from unrolled_traintest import *\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abNQ9Pk-CbQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0c50681e-19b9-4aa3-9beb-f72285a7f1b5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 27 22:02:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gs-iwTJEHgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inserted a cell!!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmyqPOpNC_cn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "e4841a26-5de6-4946-b58a-14b69275b660"
      },
      "source": [
        "pip install subword-nmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d5CtGg8-jdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "d4445599-3466-4e64-9ecd-6d4416cbedf6"
      },
      "source": [
        "#path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
        "#path = '/content/gdrive/My Drive/toy_corpuses/' # path to toy corpuses\n",
        "path = '/content/gdrive/My Drive/toy_corpuses2/' # compare with toy_corpuses to see if dict version the same\n",
        "#path = '/content/gdrive/My Drive/iwslt16_en_de2/' # compare with iwlst to see if dict version the same\n",
        "\n",
        "#corpuses = {\"train.de\":None, \"train.en\":None} # for debugging\n",
        "\n",
        "\n",
        "# when debugging by overfitting to trainset, latter 3 vals overwritten with empty strings (i.e., stay the same as init)\n",
        "corpuses = {\"train.de\":'', \"train.en\":'', \"dev.de\":'', \"dev.en\":'', \"test.de\":''}\n",
        "# e.g., now access train_src_sentences via corpuses[\"train.de\"]\n",
        "load_docs(path, corpuses)\n",
        "\n",
        "#print(corpuses[\"test.de\"])\n",
        "print_corpuses(corpuses)\n",
        "\n",
        "\n",
        "normalizeCorpuses(path, corpuses)\n",
        "\n",
        "\n",
        "print_corpuses(corpuses)\n",
        "  \n",
        "\n",
        "\n",
        "#norm_texts = normalizeCorpuses(path, corpuses)\n",
        "\n",
        "#train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = norm_texts[0], norm_texts[1], norm_texts[2], norm_texts[3], norm_texts[4]\n",
        "# train_src_sentences, train_trg_sentences = norm_texts[0], norm_texts[1]\n",
        "\n",
        "# references = load_docs(path, corpuses, tok=True)\n",
        "\n",
        "# #print(\"train_references:\")\n",
        "# #print(references[1])\n",
        "\n",
        "# train_references = []\n",
        "# for ref in references[1]:\n",
        "#     # # hacky workaround: dn include newline that tokenizeCorpuses added to tok_train\n",
        "#     # for ref in references[1][:-1]:\n",
        "#     #print(ref)\n",
        "#     ref = ref.replace(\" - \", \"-\")\n",
        "#     train_references.append([ref.split()])\n",
        "#     #print(train_references)\n",
        "    \n",
        "# dev_references = []\n",
        "# for ref in references[3]:\n",
        "#     ref = ref.replace(\" - \", \"-\")\n",
        "#     dev_references.append([ref.split()])\n",
        "    \n",
        "# print()\n",
        "# for sent in train_trg_sentences[:5]:\n",
        "#     print(sent)\n",
        "# for sent in train_references[:5]:\n",
        "#     print(sent)    \n",
        "# print()\n",
        "# for sent in dev_trg_sentences[:5]:\n",
        "#     print(sent)    \n",
        "# for sent in dev_references[:5]:\n",
        "#     print(sent)   \n",
        "# print()\n",
        "\n",
        "# train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
        "# train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
        "# dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
        "# dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
        "# test_src_sentences = [sent.split() for sent in test_src_sentences]\n",
        "\n",
        "##!!!do this in processCorpuses.py\n",
        "for corpus_name in corpuses:\n",
        "  corpus = corpuses[corpus_name]\n",
        "  for i, sent in enumerate(corpus):\n",
        "    corpus[i] = sent.split()\n",
        "\n",
        "\n",
        "print_corpuses(corpuses)\n",
        "# for corpus_name in corpuses:\n",
        "#   corpus = corpuses[corpus_name]\n",
        "#   for sent in corpus[:10]: # print first 10 sentences of each corpus\n",
        "#     print(sent)\n",
        "#   print()\n",
        "  \n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Etre\n",
            "\"Xyz\"\n",
            "Ce soir\n",
            "\n",
            "To be free\n",
            "Hey.\n",
            "E\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizing train.de...\n",
            "tokenizing train.en...\n",
            "tokenizing dev.de...\n",
            "tokenizing dev.en...\n",
            "tokenizing test.de...\n",
            "Etre\n",
            "\" Xyz \"\n",
            "Ce soir\n",
            "\n",
            "To be free\n",
            "Hey .\n",
            "E\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['Etre']\n",
            "['\"', 'Xyz', '\"']\n",
            "['Ce', 'soir']\n",
            "\n",
            "['To', 'be', 'free']\n",
            "['Hey', '.']\n",
            "['E']\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVZX4Wa-JdCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H1NUICV5Xrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Jn7bIy5TWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qHgJ0DYnW8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3b5a7e4-7fa7-4a1f-e249-d946532967be"
      },
      "source": [
        "print(train_src_sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Etre'], ['X', 'y', 'z'], ['Ce', 'soir']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb4icA9RnaZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55985def-ac12-4d2c-f82a-0645d3cc07a9"
      },
      "source": [
        "print(train_trg_sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['To', 'be', 'free'], ['C', 'd'], ['E']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiZZZslF3H1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "fb8de96c-ccb9-437b-c216-da7085b6fb66"
      },
      "source": [
        "for sent in train_trg_sentences[:5]:\n",
        "    print(sent)\n",
        "for sent in train_references[:5]:\n",
        "    print(sent)    \n",
        "print()\n",
        "for sent in dev_trg_sentences[:5]:\n",
        "    print(sent)    \n",
        "for sent in dev_references[:5]:\n",
        "    print(sent)   \n",
        "print()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['David', 'Gallo', ':', 'this', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.']\n",
            "['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']\n",
            "[\"we've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']\n",
            "['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']\n",
            "['and', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']\n",
            "[['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.']]\n",
            "[['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
            "[[\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
            "[['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
            "[['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
            "\n",
            "['when', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psychotherapy', 'client', '.']\n",
            "['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']\n",
            "['she', 'was', 'a', '26', '-', 'year', '-', 'old', 'woman', 'named', 'Alex', '.']\n",
            "['now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'jeans', 'and', 'a', 'big', 'slouchy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'flats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']\n",
            "['now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relieved', '.']\n",
            "[['When', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psychotherapy', 'client', '.']]\n",
            "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
            "[['She', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
            "[['Now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'jeans', 'and', 'a', 'big', 'slouchy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'flats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
            "[['Now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relieved', '.']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxE2E54aq9Ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7195b55-33cf-4cad-c048-10cb0be13f47"
      },
      "source": [
        "\n",
        "\n",
        "#embType = 'jointBPE'\n",
        "embType = 'word'\n",
        "\n",
        "if embType == 'word':\n",
        "  \n",
        "    \n",
        "\n",
        "    src_counter, trg_counter = to_counters(corpuses)\n",
        "    print(\"counters:\")\n",
        "    print(src_counter)\n",
        "    print(trg_counter)\n",
        "    print()\n",
        " \n",
        "    #print(trg_counter)\n",
        "    trim_type = \"topK\"\n",
        "    #trim_type = \"threshold\"\n",
        "    if trim_type == \"threshold\":\n",
        "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs(src_counter, trg_counter, srcThres=1, trgThres=1)\n",
        "    elif trim_type == \"topK\":\n",
        "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs2(src_counter, trg_counter, srcK=4, trgK=5)\n",
        "\n",
        "    print(\"trimmed vocabs:\")\n",
        "    print(trimmed_src_vocab)\n",
        "    print(trimmed_trg_vocab)\n",
        "    print()\n",
        "\n",
        "    replace_with_unk_tokens(corpuses, trimmed_src_vocab, trimmed_trg_vocab)\n",
        "    # train_src_sentences = removeOOV(train_src_sentences, trimmed_src_vocab)\n",
        "    # train_trg_sentences = removeOOV(train_trg_sentences, trimmed_trg_vocab)\n",
        "    # dev_src_sentences = removeOOV(dev_src_sentences, trimmed_src_vocab)\n",
        "    # test_src_sentences = removeOOV(test_src_sentences, trimmed_src_vocab)\n",
        "\n",
        "    print(\"corpuses replaced with unk:\")\n",
        "    print_corpuses(corpuses)\n",
        "    print()\n",
        "    # train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
        "    #add_start_end_tokens(corpuses[\"train.en\"])\n",
        "    add_start_end_tokens(corpuses)\n",
        "\n",
        "    print(\"corpuses with start/end tokens:\")\n",
        "    print_corpuses(corpuses)\n",
        "    print()\n",
        "\n",
        "\n",
        "    vocabs = {\"srcV\":{'<pad>':0, '<unk>':1}, \n",
        "              \"trgV\":{'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}, # only trg sent have start or end tokens\n",
        "              \"idx_to_src_word\":{},\n",
        "              \"idx_to_trg_word\":{}\n",
        "              }\n",
        "\n",
        "    # srcV, trgV, idx_to_src_word, idx_to_trg_word = computeVocabs(train_src_sentences, train_trg_sentences)\n",
        "    computeVocabs(vocabs, corpuses[\"train.de\"], corpuses[\"train.en\"])\n",
        "\n",
        "    print(\"vocabs:\")\n",
        "    for v in vocabs:\n",
        "      print(vocabs[v])\n",
        "      print()\n",
        "\n",
        "\n",
        "    replace_with_indices(corpuses, vocabs)\n",
        "\n",
        "    print(\"corpuses as indices:\")\n",
        "    print_corpuses(corpuses)\n",
        "    print()\n",
        "\n",
        "    # train_src_sentences = toIndices(train_src_sentences, srcV)\n",
        "    # train_trg_sentences = toIndices(train_trg_sentences, trgV)\n",
        "    # dev_src_sentences = toIndices(dev_src_sentences, srcV)   \n",
        "    # test_src_sentences = toIndices(test_src_sentences, srcV)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lengths of vocabs before trimming: src: 5, trg: 6\n",
            "counters:\n",
            "Counter({'\"': 2, 'Etre': 1, 'Xyz': 1, 'Ce': 1, 'soir': 1})\n",
            "Counter({'To': 1, 'be': 1, 'free': 1, 'Hey': 1, '.': 1, 'E': 1})\n",
            "\n",
            "lengths of vocabs after trimming: src: 4, trg: 5\n",
            "\n",
            "trimmed vocabs:\n",
            "{'\"', 'Ce', 'Etre', 'Xyz'}\n",
            "{'be', 'free', 'Hey', '.', 'To'}\n",
            "\n",
            "corpuses replaced with unk:\n",
            "['Etre']\n",
            "['\"', 'Xyz', '\"']\n",
            "['Ce', '<unk>']\n",
            "\n",
            "['To', 'be', 'free']\n",
            "['Hey', '.']\n",
            "['<unk>']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "corpuses with start/end tokens:\n",
            "['Etre']\n",
            "['\"', 'Xyz', '\"']\n",
            "['Ce', '<unk>']\n",
            "\n",
            "['<sos>', 'To', 'be', 'free', '<eos>']\n",
            "['<sos>', 'Hey', '.', '<eos>']\n",
            "['<sos>', '<unk>', '<eos>']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "vocabs:\n",
            "{'<pad>': 0, '<unk>': 1, 'Etre': 2, '\"': 3, 'Xyz': 4, 'Ce': 5}\n",
            "\n",
            "{'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, 'To': 4, 'be': 5, 'free': 6, 'Hey': 7, '.': 8}\n",
            "\n",
            "{0: '<pad>', 1: '<unk>', 2: 'Etre', 3: '\"', 4: 'Xyz', 5: 'Ce'}\n",
            "\n",
            "{0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>', 4: 'To', 5: 'be', 6: 'free', 7: 'Hey', 8: '.'}\n",
            "\n",
            "corpuses as indices:\n",
            "[2]\n",
            "[3, 4, 3]\n",
            "[5, 1]\n",
            "\n",
            "[2, 4, 5, 6, 3]\n",
            "[2, 7, 8, 3]\n",
            "[2, 1, 3]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIUo_uKW4SnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY4lSMJLsJdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1ca15e19-77d7-4638-be58-d4991b5a8c0e"
      },
      "source": [
        "print(train_src_sentences[:10])\n",
        "print()\n",
        "print(train_trg_sentences[:10])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 3, 9], [13, 14, 15, 16, 17, 18, 5, 19, 20, 21, 22, 9], [13, 23, 24, 25, 26, 27, 28, 26, 29, 30, 31, 32, 33, 34, 35, 30, 30, 36, 13, 14, 15, 37, 38, 39, 9], [31, 40, 6, 30, 41, 31, 29, 42, 43, 44, 45, 46, 47, 48, 31, 49, 50, 51, 19, 6, 9], [10, 52, 30, 5, 53, 6, 30, 41, 13, 5, 19, 54, 55, 56, 57, 9], [58, 32, 59, 60, 30, 61, 31, 62, 63, 64, 65, 66, 9], [26, 67, 26, 68, 6, 69, 9], [31, 70, 71, 6, 72, 73, 74, 9], [24, 75, 64, 76, 6, 30, 41, 13, 77, 78, 79, 80, 81, 82, 83, 84, 85, 36, 86, 31, 87, 88, 89, 90, 30, 36, 43, 91, 36, 92, 93, 30, 94, 95, 96, 30, 97, 36, 98, 30, 99, 13, 23, 100, 101, 30, 102, 103, 104, 6, 9], [20, 105, 106, 107, 93, 31, 108, 109, 64, 65, 9]]\n",
            "\n",
            "[[2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 5, 11, 3], [2, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 11, 3], [2, 28, 29, 20, 30, 23, 31, 32, 27, 30, 33, 34, 35, 36, 37, 38, 14, 15, 39, 16, 17, 40, 19, 41, 30, 42, 11, 3], [2, 23, 43, 30, 23, 44, 8, 45, 23, 33, 46, 47, 48, 49, 50, 51, 52, 30, 53, 54, 55, 46, 49, 39, 23, 31, 56, 57, 22, 23, 24, 11, 3], [2, 14, 23, 58, 38, 59, 60, 38, 8, 45, 61, 62, 23, 63, 64, 65, 11, 3], [2, 66, 19, 60, 67, 42, 38, 23, 68, 69, 70, 71, 30, 23, 72, 11, 3], [2, 31, 30, 23, 72, 8, 63, 73, 11, 3], [2, 23, 74, 75, 8, 67, 76, 77, 11, 3], [2, 78, 30, 23, 58, 38, 59, 60, 38, 8, 61, 79, 80, 23, 81, 38, 82, 61, 83, 84, 85, 7, 30, 23, 63, 38, 14, 19, 86, 87, 80, 7, 88, 89, 90, 91, 38, 14, 49, 92, 14, 49, 93, 14, 94, 95, 14, 94, 96, 14, 94, 97, 38, 98, 19, 99, 100, 101, 64, 102, 103, 26, 104, 11, 3], [2, 14, 26, 23, 68, 38, 104, 69, 23, 105, 106, 107, 108, 23, 72, 11, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9YBOOKxshoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(srcV):\n",
        "  if i == 120:\n",
        "    break\n",
        "  print(word, \": \", srcV[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APXtMPK9FLhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(trgV):\n",
        "  if i == 120:\n",
        "    break\n",
        "  print(word, \": \", trgV[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93BFGKrYKXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "712538df-8b01-4a3b-9ce6-ab888c382838"
      },
      "source": [
        "if embType == 'jointBPE':\n",
        "    alreadyExists = False\n",
        "    if not alreadyExists:\n",
        "        numMerges = 30000\n",
        "        vocabThreshold = 10\n",
        "        !bash jointBPE.sh 30000 10 '/content/gdrive/My Drive/iwslt16_en_de/'\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning joint bpe and vocab using 30000 merge operations...\n",
            "applying bpe with vocab threshold of 10 to train...\n",
            "applying bpe with vocab threshold of 10 to dev and test...\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPITtnP8YYm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "5f2e12e8-7015-4a24-b83e-285c39d3882b"
      },
      "source": [
        "path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
        "\n",
        "bpe_texts = load_docs(path, bpe=True)\n",
        "train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = bpe_texts[0], bpe_texts[1], bpe_texts[2], bpe_texts[3], bpe_texts[4]\n",
        "\n",
        "train_references = []\n",
        "for ref in train_trg_sentences[:]: # until think of better way\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    train_references.append([ref.split()])\n",
        "    \n",
        "dev_references = []\n",
        "for ref in dev_trg_sentences[:]:\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    dev_references.append([ref.split()])\n",
        "    \n",
        "\n",
        "for sent in train_trg_sentences[:5]:\n",
        "    print(sent)\n",
        "for sent in train_references[:5]:\n",
        "    print(sent)    \n",
        "print()\n",
        "for sent in dev_trg_sentences[:5]:\n",
        "    print(sent)    \n",
        "for sent in dev_references[:5]:\n",
        "    print(sent)   \n",
        "print()\n",
        "\n",
        "train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
        "train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
        "dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
        "dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
        "test_src_sentences = [sent.split() for sent in test_src_sentences]\n",
        "\n",
        "\n",
        "train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
        "\n",
        "vocab, idx_to_subword = computeBPEvocabs(path + \"vocab.de\", path + \"vocab.en\")\n",
        "\n",
        "train_src_sentences = toIndices(train_src_sentences, vocab)\n",
        "train_trg_sentences = toIndices(train_trg_sentences, vocab)\n",
        "dev_src_sentences = toIndices(dev_src_sentences, vocab)   \n",
        "test_src_sentences = toIndices(test_src_sentences, vocab)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "David Gall@@ o : this is Bill L@@ ange . I'm Dave Gall@@ o .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "we've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it .\n",
            "the truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea .\n",
            "and the problem , I think , is that we take the ocean for granted .\n",
            "[['David', 'Gall@@', 'o', ':', 'this', 'is', 'Bill', 'L@@', 'ange', '.', \"I'm\", 'Dave', 'Gall@@', 'o', '.']]\n",
            "[['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
            "[[\"we've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
            "[['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
            "[['and', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
            "\n",
            "when I was in my 20s , I saw my very first psycho@@ therapy client .\n",
            "I was a Ph.D. student in clinical psychology at Berkeley .\n",
            "she was a 26 - year - old woman named Alex .\n",
            "now Alex walked into her first session wearing je@@ ans and a big sl@@ ou@@ chy top , and she dropped onto the couch in my office and kicked off her fl@@ ats and told me she was there to talk about guy problems .\n",
            "now when I heard this , I was so relie@@ ved .\n",
            "[['when', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psycho@@', 'therapy', 'client', '.']]\n",
            "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
            "[['she', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
            "[['now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'je@@', 'ans', 'and', 'a', 'big', 'sl@@', 'ou@@', 'chy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'fl@@', 'ats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
            "[['now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relie@@', 'ved', '.']]\n",
            "\n",
            "number of german subwords is 23056\n",
            "number of english subwords is 6822\n",
            "total number of subwords is 29881\n",
            "warning: unknown word: û@@ in sent: ['mein', 'Favor@@', 'it', 'ist', 'der', 'in', 'der', 'Mitte', '--', 'MP3', '-', 'Player', '-', ',', 'N@@', 'asen', '-', 'Haar', '-', 'Tri@@', 'mmer', 'und', 'Cr@@', 'è@@', 'me', 'Br@@', 'û@@', 'l@@', 'é@@', 'e', 'F@@', 'ack@@', 'el', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: ê in sent: ['ash@@', 'ê', 'Ol@@', 'ê@@', 'n', '.', 'in', 'meiner', 'Sprache', 'bedeutet', 'das', ':', 'ich', 'danke', 'Ihnen', 'sehr', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: τ in sent: ['und', 'die', 'Antwort', ',', 'glaube', 'ich', ',', 'ist', 'ja', '.', '[', '\"', 'f', 'T', 'S@@', 'τ', '\"', ']', '.', 'was', 'Sie', 'gerade', 'sehen', ',', 'ist', 'wahrscheinlich', 'die', 'beste', 'Ent@@', 'spre@@', 'chung', 'zu', 'E', 'm@@', 'c@@', '²', 'für', 'Intelligenz', ',', 'die', 'ich', 'gesehen', 'habe', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: Œ@@ in sent: ['ich', 'begann', 'ein', 'neues', 'Œ@@', 'u', 'v', 're', '.']\n",
            "removed it from the sentence.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOI1bMHrcwpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6579f53-3710-4e9d-ff7d-10d42e449e31"
      },
      "source": [
        "# build model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "overfit_on_train = True\n",
        "\n",
        "if overfit_on_train:\n",
        "    b = 10\n",
        "    #dev_bsz = 10\n",
        "    dev_bsz = 1\n",
        "else:\n",
        "    b = 64\n",
        "    dev_bsz = 64\n",
        "    #dev_bsz = 1\n",
        "num_ep = 300 # number of epochs\n",
        "IS = 300 # input size\n",
        "enc_hs = 600 # encoder hidden size\n",
        "dec_hs = 600 # decoder hidden size\n",
        "enc_NL = 1 # number of layers\n",
        "dec_NL = 1\n",
        "enc_DR = .0 # dropout\n",
        "dec_DR = .0\n",
        "bi_enc = True\n",
        "project = True\n",
        "reverse_src = False\n",
        "#tie_weights = True\n",
        "tie_weights = False\n",
        "\n",
        "\n",
        "customLSTM = False\n",
        "#embType = \"jointBPE\"\n",
        "#embType = \"word\"\n",
        "#trim_type = \"threshold\"\n",
        "#trim_type = \"topK\"\n",
        "\n",
        "inputFeeding = True\n",
        "K = .7\n",
        "LR = .01 # learning rate\n",
        "WD = .0\n",
        "\n",
        "enc_emb_drop = .1\n",
        "dec_emb_drop = .1\n",
        "dropconnect = .5\n",
        "\n",
        "opt = \"ADAM\" # optimization algorithm\n",
        "model_name = 'debug_model/'\n",
        "msg = ' \\n'\n",
        "\n",
        "\n",
        "encoder_params = {}\n",
        "if embType == \"word\":\n",
        "    encoder_params['vocab_size'] = len(srcV)\n",
        "elif embType == \"jointBPE\":\n",
        "    encoder_params['vocab_size'] = len(vocab)\n",
        "encoder_params['input_size'] = IS\n",
        "encoder_params['hidden_size'] = enc_hs\n",
        "encoder_params['num_layers'] = enc_NL\n",
        "encoder_params['dropout'] = enc_DR\n",
        "encoder_params['dev'] = device\n",
        "encoder_params['bi_enc'] = bi_enc\n",
        "encoder_params['project'] = project # project concated enc states to dim of decoder\n",
        "#encoder_params['src_emb_drop'] = enc_emb_drop\n",
        "#encoder_params['dropconnect'] = dropconnect\n",
        "encoder_params['reverse_src'] = reverse_src\n",
        "#encoder_params['variational_drop'] = .5\n",
        "#encoder_params['i0_drop'] = .2\n",
        "#encoder_params['i1_drop'] = .5\n",
        "encoder_params['out_drop'] = .5\n",
        "encoder_params['init_scheme'] = 'layer_to_layer'\n",
        "#encoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
        "encoder_params['customLSTM'] = customLSTM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_params = {}\n",
        "if embType == \"word\":\n",
        "    decoder_params['sosIdx'] = trgV['<sos>']\n",
        "    #print(decoder_params['sosIdx'])\n",
        "    decoder_params['eosIdx'] = trgV['<eos>']\n",
        "    #print(decoder_params['eosIdx'])\n",
        "\n",
        "    decoder_params['padIdx'] = trgV['<pad>']\n",
        "    #print(decoder_params['padIdx'])\n",
        "\n",
        "    decoder_params['vocab_size'] = len(trgV)\n",
        "    decoder_params['idx_to_trg_word'] = idx_to_trg_word\n",
        "\n",
        "elif embType == \"jointBPE\":\n",
        "\n",
        "    decoder_params['sosIdx'] = vocab['<sos>']\n",
        "    #print(decoder_params['sosIdx'])\n",
        "    decoder_params['eosIdx'] = vocab['<eos>']\n",
        "    #print(decoder_params['eosIdx'])\n",
        "\n",
        "    decoder_params['padIdx'] = vocab['<pad>']\n",
        "    #print(decoder_params['padIdx'])\n",
        "\n",
        "    decoder_params['vocab_size'] = len(vocab)\n",
        "    decoder_params['idx_to_trg_word'] = idx_to_subword\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_params['input_size'] = IS\n",
        "decoder_params['hidden_size'] = dec_hs\n",
        "decoder_params['num_layers'] = dec_NL\n",
        "decoder_params['dropout'] = dec_DR\n",
        "#decoder_params['trg_emb_drop'] = dec_emb_drop\n",
        "#decoder_params['dropconnect'] = dropconnect\n",
        "\n",
        "decoder_params['dev'] = device\n",
        "decoder_params['attention'] = \"global_att\"\n",
        "#decoder_params['attention'] = None\n",
        "\n",
        "\n",
        "decoder_params['inf_alg'] = \"greedy_search\"\n",
        "#decoder_params['inf_alg'] = \"beam_search\"\n",
        "\n",
        "decoder_params['beam_size'] = 10\n",
        "#decoder_params['decode_slack'] = 8\n",
        "decoder_params['decode_slack'] = 20\n",
        "\n",
        "#decoder_params['variational_drop'] = .5\n",
        "#decoder_params['i0_drop'] = .0 # applied to trg embeddings prior to entering decoder lstm\n",
        "#decoder_params['i1_drop'] = .5\n",
        "decoder_params['out_drop'] = .5 # applied after final layer of decoder lstm\n",
        "#decoder_params['att_drop'] = .3 # applied after project back to input size\n",
        "#decoder_params['rec_drop'] = .5\n",
        "\n",
        "decoder_params['tie_weights'] = tie_weights\n",
        "#decoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
        "decoder_params['customLSTM'] = customLSTM\n",
        "#decoder_params['input_feeding'] = inputFeeding\n",
        "#decoder_params['K'] = K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(encoder_params)\n",
        "decoder = Decoder(decoder_params)\n",
        "translator = RNNencdec(encoder, decoder, embType) # initialize model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    translator.cuda()\n",
        "    \n",
        "\n",
        "optimizer = torch.optim.Adam(translator.parameters(), lr=LR, weight_decay=WD) # initialize optimizer\n",
        "\n",
        "# folder to write checkpoints (train outputs) and translations (inference outputs)\n",
        "folder = '/content/gdrive/My Drive/MToutputs/' + model_name"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GR15Lmr1tht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6naTP-71riD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBgLZHAjfHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "x = !nvidia-smi | head -n 8 | tail -n 1 | cut -d ' ' -f 6-7\n",
        "current_gpu = ' '.join(x)\n",
        "with open(folder + 'model_train_stats.txt', 'w') as f:\n",
        "    f.write(msg)\n",
        "    if trim_type == \"threshold\":\n",
        "        f.write(\"srcThres: {}\\n\".format(st))\n",
        "        f.write(\"trgThres: {}\\n\".format(tt))\n",
        "\n",
        "    f.write(\"encoder_params:\\n\")\n",
        "    for key in encoder_params:\n",
        "        f.write(key + \": {}\\n\".format(encoder_params[key]))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"decoder_params:\\n\")\n",
        "    for key in decoder_params:\n",
        "        if key != \"idx_to_trg_word\":\n",
        "            f.write(key + \": {}\\n\".format(decoder_params[key]))\n",
        "        \n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"embType: {}\\n\".format(embType))\n",
        "    f.write(\"lr: {}\\n\".format(LR))\n",
        "    f.write(\"wd: {}\\n\".format(WD))\n",
        "    f.write(\"opt: {}\\n\".format(opt))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"bsz: {}\\n\".format(b))\n",
        "    f.write(\"dev_bsz: {}\\n\".format(dev_bsz))\n",
        "    f.write(\"current gpu: {}\\n\".format(current_gpu))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZnY73CfjlZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd71fcca-a4a7-4dac-8239-3f59983387b8"
      },
      "source": [
        "if reverse_src:\n",
        "    if embType == \"word\":\n",
        "\n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "\n",
        "        print()\n",
        "\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])\n",
        "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "\n",
        "    elif embType == \"jointBPE\":\n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])    \n",
        "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent]) \n",
        "\n",
        "        print()\n",
        "\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])\n",
        "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])  \n",
        "\n",
        "trainingPairs = list(zip(train_src_sentences, train_trg_sentences))\n",
        "\n",
        "unrolled = False\n",
        "if overfit_on_train: \n",
        "    trainBatches = getBatches(trainingPairs[:10], b, device)\n",
        "    devBatches = getDevBatches(train_src_sentences[:10], dev_bsz, device)\n",
        "else:\n",
        "    trainBatches = getBatches(trainingPairs, b, device)\n",
        "    devBatches = getDevBatches(dev_src_sentences, dev_bsz, device)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sorting by trg length\n",
            "encoder_inputs_tensor:\n",
            "tensor([[ 24,  75,  64,  76,   6,  30,  41,  13,  77,  78,  79,  80,  81,  82,\n",
            "          83,  84,  85,  36,  86,  31,  87,  88,  89,  90,  30,  36,  43,  91,\n",
            "          36,  92,  93,  30,  94,  95,  96,  30,  97,  36,  98,  30,  99,  13,\n",
            "          23, 100, 101,  30, 102, 103, 104,   6,   9],\n",
            "        [ 31,  40,   6,  30,  41,  31,  29,  42,  43,  44,  45,  46,  47,  48,\n",
            "          31,  49,  50,  51,  19,   6,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  23,  24,  25,  26,  27,  28,  26,  29,  30,  31,  32,  33,  34,\n",
            "          35,  30,  30,  36,  13,  14,  15,  37,  38,  39,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 10,  52,  30,   5,  53,   6,  30,  41,  13,   5,  19,  54,  55,  56,\n",
            "          57,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  14,  15,  16,  17,  18,   5,  19,  20,  21,  22,   9,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 58,  32,  59,  60,  30,  61,  31,  62,  63,  64,  65,  66,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 20, 105, 106, 107,  93,  31, 108, 109,  64,  65,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   3,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 26,  67,  26,  68,   6,  69,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  70,  71,   6,  72,  73,  74,   9,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "decoder_inputs_tensor:\n",
            "tensor([[  2,  78,  30,  23,  58,  38,  59,  60,  38,   8,  61,  79,  80,  23,\n",
            "          81,  38,  82,  61,  83,  84,  85,   7,  30,  23,  63,  38,  14,  19,\n",
            "          86,  87,  80,   7,  88,  89,  90,  91,  38,  14,  49,  92,  14,  49,\n",
            "          93,  14,  94,  95,  14,  94,  96,  14,  94,  97,  38,  98,  19,  99,\n",
            "         100, 101,  64, 102, 103,  26, 104,  11],\n",
            "        [  2,  23,  43,  30,  23,  44,   8,  45,  23,  33,  46,  47,  48,  49,\n",
            "          50,  51,  52,  30,  53,  54,  55,  46,  49,  39,  23,  31,  56,  57,\n",
            "          22,  23,  24,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  28,  29,  20,  30,  23,  31,  32,  27,  30,  33,  34,  35,  36,\n",
            "          37,  38,  14,  15,  39,  16,  17,  40,  19,  41,  30,  42,  11,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  23,  58,  38,  59,  60,  38,   8,  45,  61,  62,  23,  63,\n",
            "          64,  65,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
            "          27,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  66,  19,  60,  67,  42,  38,  23,  68,  69,  70,  71,  30,  23,\n",
            "          72,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  26,  23,  68,  38, 104,  69,  23, 105, 106, 107, 108,  23,\n",
            "          72,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,   5,  11,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  31,  30,  23,  72,   8,  63,  73,  11,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  23,  74,  75,   8,  67,  76,  77,  11,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "target_indices_tensor:\n",
            "tensor([[ 78,  30,  23,  58,  38,  59,  60,  38,   8,  61,  79,  80,  23,  81,\n",
            "          38,  82,  61,  83,  84,  85,   7,  30,  23,  63,  38,  14,  19,  86,\n",
            "          87,  80,   7,  88,  89,  90,  91,  38,  14,  49,  92,  14,  49,  93,\n",
            "          14,  94,  95,  14,  94,  96,  14,  94,  97,  38,  98,  19,  99, 100,\n",
            "         101,  64, 102, 103,  26, 104,  11,   3],\n",
            "        [ 23,  43,  30,  23,  44,   8,  45,  23,  33,  46,  47,  48,  49,  50,\n",
            "          51,  52,  30,  53,  54,  55,  46,  49,  39,  23,  31,  56,  57,  22,\n",
            "          23,  24,  11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 28,  29,  20,  30,  23,  31,  32,  27,  30,  33,  34,  35,  36,  37,\n",
            "          38,  14,  15,  39,  16,  17,  40,  19,  41,  30,  42,  11,   3,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  23,  58,  38,  59,  60,  38,   8,  45,  61,  62,  23,  63,  64,\n",
            "          65,  11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 66,  19,  60,  67,  42,  38,  23,  68,  69,  70,  71,  30,  23,  72,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  26,  23,  68,  38, 104,  69,  23, 105, 106, 107, 108,  23,  72,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  4,   5,   6,   7,   8,   9,  10,  11,  12,  13,   5,  11,   3,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  30,  23,  72,   8,  63,  73,  11,   3,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 23,  74,  75,   8,  67,  76,  77,  11,   3,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "sorted_src_lengths:\n",
            "tensor([51, 25, 21, 16, 13, 13, 12, 11,  8,  7], device='cuda:0')\n",
            "\n",
            "idxs_in_enc_inputs_tensor:\n",
            "tensor([0, 2, 1, 3, 7, 5, 4, 6, 9, 8], device='cuda:0')\n",
            "\n",
            "(sorted) encoder_inputs_tensor:\n",
            "tensor([[ 24,  75,  64,  76,   6,  30,  41,  13,  77,  78,  79,  80,  81,  82,\n",
            "          83,  84,  85,  36,  86,  31,  87,  88,  89,  90,  30,  36,  43,  91,\n",
            "          36,  92,  93,  30,  94,  95,  96,  30,  97,  36,  98,  30,  99,  13,\n",
            "          23, 100, 101,  30, 102, 103, 104,   6,   9],\n",
            "        [ 13,  23,  24,  25,  26,  27,  28,  26,  29,  30,  31,  32,  33,  34,\n",
            "          35,  30,  30,  36,  13,  14,  15,  37,  38,  39,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  40,   6,  30,  41,  31,  29,  42,  43,  44,  45,  46,  47,  48,\n",
            "          31,  49,  50,  51,  19,   6,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 10,  52,  30,   5,  53,   6,  30,  41,  13,   5,  19,  54,  55,  56,\n",
            "          57,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   3,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 58,  32,  59,  60,  30,  61,  31,  62,  63,  64,  65,  66,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  14,  15,  16,  17,  18,   5,  19,  20,  21,  22,   9,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 20, 105, 106, 107,  93,  31, 108, 109,  64,  65,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  70,  71,   6,  72,  73,  74,   9,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 26,  67,  26,  68,   6,  69,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "idxs_in_sorted_enc_inputs_tensor:\n",
            "tensor([0, 2, 1, 3, 6, 5, 7, 4, 9, 8], device='cuda:0')\n",
            "\n",
            "took 0.05 seconds to get all the batches\n",
            "took 0.00 seconds to get all the devBatches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJrmWhsS5J5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e2b191a5-fd3d-4e8a-898a-35df800b27e3"
      },
      "source": [
        "print(trainBatches)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[((tensor([[3, 4, 5],\n",
            "        [6, 7, 0],\n",
            "        [2, 0, 0]], device='cuda:0'), tensor([3, 2, 1], device='cuda:0'), tensor([2, 0, 1], device='cuda:0')), (tensor([[2, 4, 5, 6],\n",
            "        [2, 7, 8, 0],\n",
            "        [2, 9, 0, 0]], device='cuda:0'), tensor([4, 3, 2], device='cuda:0'), tensor([[[False, False, False],\n",
            "         [False, False, False],\n",
            "         [False, False, False],\n",
            "         [False, False, False]],\n",
            "\n",
            "        [[False, False,  True],\n",
            "         [False, False,  True],\n",
            "         [False, False,  True],\n",
            "         [False, False,  True]],\n",
            "\n",
            "        [[False,  True,  True],\n",
            "         [False,  True,  True],\n",
            "         [False,  True,  True],\n",
            "         [False,  True,  True]]], device='cuda:0')), (tensor([[4, 5, 6, 3],\n",
            "        [7, 8, 3, 0],\n",
            "        [9, 3, 0, 0]], device='cuda:0'), tensor([4, 3, 2], device='cuda:0')))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jofhcYlQ7tvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ef3bacb-2534-4943-c790-20d86381a4b6"
      },
      "source": [
        "\n",
        "if overfit_on_train:\n",
        "    translator = train(translator, optimizer, trainBatches, devBatches, train_references[:10], num_epochs=num_ep, cur_ep=0, folder=folder, save=False)\n",
        "else:\n",
        "    translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=0, folder=folder, save=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 00, loss: 2345.38305664, ep_t: 0.05 sec, t_t: 0.39 sec, bleu: 0.0188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ep: 01, loss: 1946.16418457, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 0.4738\n",
            "ep: 02, loss: 1085.13732910, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 0.3209\n",
            "ep: 03, loss: 1945.15270996, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 0.4036\n",
            "ep: 04, loss: 1080.20617676, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.4400\n",
            "ep: 05, loss: 1111.77160645, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.3691\n",
            "ep: 06, loss: 1098.97229004, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.4224\n",
            "ep: 07, loss: 1050.42272949, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.3030\n",
            "ep: 08, loss: 1024.43554688, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.4608\n",
            "ep: 09, loss: 1010.91802979, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.5004\n",
            "ep: 10, loss: 1010.91259766, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.4674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ep: 11, loss: 990.36614990, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.1319\n",
            "ep: 12, loss: 971.81860352, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.4133\n",
            "ep: 13, loss: 975.10662842, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.1293\n",
            "ep: 14, loss: 921.18273926, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.5403\n",
            "ep: 15, loss: 906.11761475, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.5415\n",
            "ep: 16, loss: 883.88537598, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.1669\n",
            "ep: 17, loss: 857.51208496, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.1483\n",
            "ep: 18, loss: 835.66253662, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.2045\n",
            "ep: 19, loss: 823.03668213, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.1498\n",
            "ep: 20, loss: 832.75085449, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.1517\n",
            "ep: 21, loss: 788.71392822, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.1867\n",
            "ep: 22, loss: 765.89105225, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.5817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ep: 23, loss: 744.37493896, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.0474\n",
            "ep: 24, loss: 734.81878662, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.1906\n",
            "ep: 25, loss: 724.10253906, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.0584\n",
            "ep: 26, loss: 694.66070557, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.0218\n",
            "ep: 27, loss: 678.60400391, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.0332\n",
            "ep: 28, loss: 659.33587646, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.0251\n",
            "ep: 29, loss: 641.06152344, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.0286\n",
            "ep: 30, loss: 625.10870361, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.0266\n",
            "ep: 31, loss: 608.81170654, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.0322\n",
            "ep: 32, loss: 586.68933105, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.0535\n",
            "ep: 33, loss: 566.73352051, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.0600\n",
            "ep: 34, loss: 552.89642334, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.0689\n",
            "ep: 35, loss: 539.61682129, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.0778\n",
            "ep: 36, loss: 518.19317627, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.0835\n",
            "ep: 37, loss: 498.14682007, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.1002\n",
            "ep: 38, loss: 483.21661377, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.1358\n",
            "ep: 39, loss: 465.78952026, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.1071\n",
            "ep: 40, loss: 444.46054077, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.1190\n",
            "ep: 41, loss: 425.62823486, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.1357\n",
            "ep: 42, loss: 412.23516846, ep_t: 0.03 sec, t_t: 0.47 sec, bleu: 0.1845\n",
            "ep: 43, loss: 397.56683350, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.1529\n",
            "ep: 44, loss: 379.66851807, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.1905\n",
            "ep: 45, loss: 361.12875366, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.1900\n",
            "ep: 46, loss: 343.68978882, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.2118\n",
            "ep: 47, loss: 326.77169800, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.3085\n",
            "ep: 48, loss: 312.42742920, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.3214\n",
            "ep: 49, loss: 296.60891724, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.3344\n",
            "ep: 50, loss: 282.00811768, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.3788\n",
            "ep: 51, loss: 265.84207153, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.3497\n",
            "ep: 52, loss: 252.36529541, ep_t: 0.03 sec, t_t: 0.49 sec, bleu: 0.3899\n",
            "ep: 53, loss: 238.42187500, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.4737\n",
            "ep: 54, loss: 225.37640381, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.4946\n",
            "ep: 55, loss: 215.01684570, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.4510\n",
            "ep: 56, loss: 207.60163879, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.4602\n",
            "ep: 57, loss: 206.28625488, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.5093\n",
            "ep: 58, loss: 188.36647034, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.5652\n",
            "ep: 59, loss: 173.95855713, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.5720\n",
            "ep: 60, loss: 165.70777893, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.5917\n",
            "ep: 61, loss: 152.07423401, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.5880\n",
            "ep: 62, loss: 146.69813538, ep_t: 0.03 sec, t_t: 0.49 sec, bleu: 0.6679\n",
            "ep: 63, loss: 133.63005066, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.6558\n",
            "ep: 64, loss: 127.16918182, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.7499\n",
            "ep: 65, loss: 117.92866516, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.7134\n",
            "ep: 66, loss: 111.47325134, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.7146\n",
            "ep: 67, loss: 103.77147675, ep_t: 0.04 sec, t_t: 0.33 sec, bleu: 0.7345\n",
            "ep: 68, loss: 97.81565094, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.6712\n",
            "ep: 69, loss: 92.14190674, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.7511\n",
            "ep: 70, loss: 86.13206482, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.6896\n",
            "ep: 71, loss: 81.51248169, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.7196\n",
            "ep: 72, loss: 76.45591736, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.7354\n",
            "ep: 73, loss: 72.41239929, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.7498\n",
            "ep: 74, loss: 68.00321960, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.7550\n",
            "ep: 75, loss: 64.38853455, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.7550\n",
            "ep: 76, loss: 60.57250595, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.7222\n",
            "ep: 77, loss: 57.59146118, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.7222\n",
            "ep: 78, loss: 54.66670990, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.7550\n",
            "ep: 79, loss: 51.56555557, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.7550\n",
            "ep: 80, loss: 49.13896561, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8192\n",
            "ep: 81, loss: 46.79687500, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.7994\n",
            "ep: 82, loss: 44.59743500, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8544\n",
            "ep: 83, loss: 42.57040405, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8685\n",
            "ep: 84, loss: 40.70286560, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8128\n",
            "ep: 85, loss: 39.01343155, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8128\n",
            "ep: 86, loss: 37.35152054, ep_t: 0.03 sec, t_t: 0.39 sec, bleu: 0.7994\n",
            "ep: 87, loss: 35.81429672, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.7994\n",
            "ep: 88, loss: 34.43265915, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 0.8128\n",
            "ep: 89, loss: 33.04349899, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8678\n",
            "ep: 90, loss: 31.76237488, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8685\n",
            "ep: 91, loss: 30.55504227, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8544\n",
            "ep: 92, loss: 29.37388992, ep_t: 0.04 sec, t_t: 0.40 sec, bleu: 0.8544\n",
            "ep: 93, loss: 28.28386307, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.8678\n",
            "ep: 94, loss: 27.25324631, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.8685\n",
            "ep: 95, loss: 26.31074905, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.8544\n",
            "ep: 96, loss: 25.40937424, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8544\n",
            "ep: 97, loss: 24.52659988, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8544\n",
            "ep: 98, loss: 23.73110390, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.9532\n",
            "ep: 99, loss: 22.96538162, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9532\n",
            "ep: 100, loss: 22.25706673, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.8544\n",
            "ep: 101, loss: 21.57451439, ep_t: 0.03 sec, t_t: 0.50 sec, bleu: 0.8544\n",
            "ep: 102, loss: 20.92512131, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.8544\n",
            "ep: 103, loss: 20.32324028, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.8678\n",
            "ep: 104, loss: 19.76434708, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8678\n",
            "ep: 105, loss: 19.23042107, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8544\n",
            "ep: 106, loss: 18.73280144, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 0.8544\n",
            "ep: 107, loss: 18.25294113, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 108, loss: 17.79193306, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8678\n",
            "ep: 109, loss: 17.37104034, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 110, loss: 16.97143745, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8544\n",
            "ep: 111, loss: 16.58551407, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 112, loss: 16.22072029, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 113, loss: 15.87412167, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9695\n",
            "ep: 114, loss: 15.53981113, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 115, loss: 15.22011089, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 116, loss: 14.92074776, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.9695\n",
            "ep: 117, loss: 14.64217091, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 118, loss: 14.37545872, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.8829\n",
            "ep: 119, loss: 14.11795902, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9695\n",
            "ep: 120, loss: 13.86458588, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 121, loss: 13.62687016, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.8829\n",
            "ep: 122, loss: 13.39700603, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 123, loss: 13.16505241, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9695\n",
            "ep: 124, loss: 12.94478035, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.9695\n",
            "ep: 125, loss: 12.72596073, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.8829\n",
            "ep: 126, loss: 12.51413536, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9695\n",
            "ep: 127, loss: 12.31119156, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 128, loss: 12.10694981, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8829\n",
            "ep: 129, loss: 11.91890335, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 130, loss: 11.79663277, ep_t: 0.03 sec, t_t: 0.48 sec, bleu: 0.8544\n",
            "ep: 131, loss: 11.87243271, ep_t: 0.05 sec, t_t: 0.38 sec, bleu: 0.9532\n",
            "ep: 132, loss: 11.64738464, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 133, loss: 11.32269001, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 134, loss: 11.01832485, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 135, loss: 10.83444405, ep_t: 0.04 sec, t_t: 0.33 sec, bleu: 0.8957\n",
            "ep: 136, loss: 10.66439056, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 137, loss: 10.49130344, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 138, loss: 10.33992767, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 139, loss: 10.14903355, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 140, loss: 10.01779747, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 141, loss: 9.83595848, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.8957\n",
            "ep: 142, loss: 9.70306587, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 143, loss: 9.54609680, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 144, loss: 9.40654182, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 145, loss: 9.27469826, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 146, loss: 9.12944603, ep_t: 0.03 sec, t_t: 0.39 sec, bleu: 1.0000\n",
            "ep: 147, loss: 9.00235367, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 148, loss: 8.87627411, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 149, loss: 8.74516773, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.8957\n",
            "ep: 150, loss: 8.62233734, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 151, loss: 8.50427723, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 152, loss: 8.38231373, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 153, loss: 8.26370049, ep_t: 0.04 sec, t_t: 0.33 sec, bleu: 0.8957\n",
            "ep: 154, loss: 8.15679836, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 1.0000\n",
            "ep: 155, loss: 8.07896423, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8829\n",
            "ep: 156, loss: 8.13055992, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8843\n",
            "ep: 157, loss: 8.13681126, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 158, loss: 8.10797024, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 159, loss: 7.65697432, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 0.8957\n",
            "ep: 160, loss: 7.68096972, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 161, loss: 7.92019939, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 162, loss: 7.41586256, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8843\n",
            "ep: 163, loss: 7.67611551, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8829\n",
            "ep: 164, loss: 8.46428013, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 165, loss: 8.46338272, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8843\n",
            "ep: 166, loss: 8.25258160, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 167, loss: 7.91027308, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 168, loss: 7.29803562, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9695\n",
            "ep: 169, loss: 7.85557938, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8829\n",
            "ep: 170, loss: 7.53997040, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8843\n",
            "ep: 171, loss: 6.98895884, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 0.9841\n",
            "ep: 172, loss: 7.35297251, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 0.9841\n",
            "ep: 173, loss: 6.53651571, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 0.8829\n",
            "ep: 174, loss: 7.20631886, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 175, loss: 6.61022377, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 176, loss: 6.38818359, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.8957\n",
            "ep: 177, loss: 6.39816093, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 178, loss: 6.37202501, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 179, loss: 6.09217024, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 180, loss: 6.08175707, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 181, loss: 6.07386637, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 182, loss: 5.76177549, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 183, loss: 5.90023947, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 184, loss: 5.71059084, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 185, loss: 5.69267511, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 186, loss: 5.53487444, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.9841\n",
            "ep: 187, loss: 5.53269196, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 188, loss: 5.46001196, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 189, loss: 5.34461451, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.9841\n",
            "ep: 190, loss: 5.33248854, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 191, loss: 5.21242523, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 0.8957\n",
            "ep: 192, loss: 5.22444344, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 193, loss: 5.11101484, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 194, loss: 5.09247828, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 195, loss: 5.00485802, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 196, loss: 4.96668577, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 0.9841\n",
            "ep: 197, loss: 4.92361116, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 198, loss: 4.85459089, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 199, loss: 4.83158827, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 200, loss: 4.75586557, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 201, loss: 4.73496962, ep_t: 0.03 sec, t_t: 0.37 sec, bleu: 0.8957\n",
            "ep: 202, loss: 4.67314720, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 203, loss: 4.63698006, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 204, loss: 4.59749413, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 205, loss: 4.54471588, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 206, loss: 4.51762629, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.9841\n",
            "ep: 207, loss: 4.46260834, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 208, loss: 4.43243885, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 209, loss: 4.38753319, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 210, loss: 4.34210062, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 211, loss: 4.31327534, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 0.8957\n",
            "ep: 212, loss: 4.27149200, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 213, loss: 4.22807217, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 214, loss: 4.18811750, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 215, loss: 4.16806984, ep_t: 0.04 sec, t_t: 0.46 sec, bleu: 0.9841\n",
            "ep: 216, loss: 4.12602043, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.8957\n",
            "ep: 217, loss: 4.07878828, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 218, loss: 4.02124310, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 219, loss: 4.00903749, ep_t: 0.04 sec, t_t: 0.33 sec, bleu: 0.9841\n",
            "ep: 220, loss: 4.03329086, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 0.8957\n",
            "ep: 221, loss: 4.07426882, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 222, loss: 3.99191904, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 223, loss: 3.87344146, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 224, loss: 3.83978462, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 225, loss: 3.98826051, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.9841\n",
            "ep: 226, loss: 3.87970042, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 227, loss: 3.72757626, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 1.0000\n",
            "ep: 228, loss: 3.75301456, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.8957\n",
            "ep: 229, loss: 3.64622927, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 230, loss: 3.86282945, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 231, loss: 4.08293104, ep_t: 0.03 sec, t_t: 0.38 sec, bleu: 0.9841\n",
            "ep: 232, loss: 4.01042366, ep_t: 0.04 sec, t_t: 0.33 sec, bleu: 0.9841\n",
            "ep: 233, loss: 4.06774569, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.8957\n",
            "ep: 234, loss: 3.92106891, ep_t: 0.04 sec, t_t: 0.49 sec, bleu: 0.8957\n",
            "ep: 235, loss: 3.96437311, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 236, loss: 3.95013237, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 237, loss: 3.83693361, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 238, loss: 3.85477185, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 239, loss: 3.71755552, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 240, loss: 3.78122020, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 241, loss: 3.52574205, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 0.9841\n",
            "ep: 242, loss: 3.71215773, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 243, loss: 3.83312082, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 1.0000\n",
            "ep: 244, loss: 3.46437955, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 1.0000\n",
            "ep: 245, loss: 3.41922641, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.9841\n",
            "ep: 246, loss: 3.60260177, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 247, loss: 3.18625426, ep_t: 0.04 sec, t_t: 0.40 sec, bleu: 0.8957\n",
            "ep: 248, loss: 3.44201660, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 1.0000\n",
            "ep: 249, loss: 3.12773228, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 250, loss: 3.33929443, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 0.9841\n",
            "ep: 251, loss: 3.35068703, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 252, loss: 3.01341844, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 0.8957\n",
            "ep: 253, loss: 3.45874548, ep_t: 0.03 sec, t_t: 0.48 sec, bleu: 1.0000\n",
            "ep: 254, loss: 3.08429742, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 255, loss: 2.94958091, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 256, loss: 2.82631445, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 257, loss: 2.71919346, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 258, loss: 2.75436091, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 259, loss: 2.66162872, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 260, loss: 2.61463141, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 261, loss: 2.64090395, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 262, loss: 2.52100372, ep_t: 0.03 sec, t_t: 0.33 sec, bleu: 1.0000\n",
            "ep: 263, loss: 2.57712913, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 264, loss: 2.47833419, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 265, loss: 2.47043610, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 266, loss: 2.44878364, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 267, loss: 2.33981943, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 268, loss: 2.39218974, ep_t: 0.03 sec, t_t: 0.40 sec, bleu: 1.0000\n",
            "ep: 269, loss: 2.28809714, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 270, loss: 2.27615619, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 271, loss: 2.21999311, ep_t: 0.03 sec, t_t: 0.48 sec, bleu: 1.0000\n",
            "ep: 272, loss: 2.20511532, ep_t: 0.04 sec, t_t: 0.39 sec, bleu: 1.0000\n",
            "ep: 273, loss: 2.18406630, ep_t: 0.04 sec, t_t: 0.37 sec, bleu: 1.0000\n",
            "ep: 274, loss: 2.13089204, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 275, loss: 2.11662269, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 276, loss: 2.06261086, ep_t: 0.04 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 277, loss: 2.06762147, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 278, loss: 2.02582788, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 279, loss: 2.00239587, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 280, loss: 1.98132312, ep_t: 0.03 sec, t_t: 0.48 sec, bleu: 1.0000\n",
            "ep: 281, loss: 1.94573283, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 282, loss: 1.93697917, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 283, loss: 1.91080737, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 284, loss: 1.88839364, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 285, loss: 1.87557495, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 286, loss: 1.84851396, ep_t: 0.03 sec, t_t: 0.36 sec, bleu: 1.0000\n",
            "ep: 287, loss: 1.82966900, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 288, loss: 1.81672871, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 289, loss: 1.79567862, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 290, loss: 1.77899277, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 291, loss: 1.76592314, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 292, loss: 1.74803746, ep_t: 0.03 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 293, loss: 1.73126209, ep_t: 0.04 sec, t_t: 0.36 sec, bleu: 1.0000\n",
            "ep: 294, loss: 1.71815956, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n",
            "ep: 295, loss: 1.70347595, ep_t: 0.03 sec, t_t: 0.35 sec, bleu: 1.0000\n",
            "ep: 296, loss: 1.68808699, ep_t: 0.03 sec, t_t: 0.39 sec, bleu: 1.0000\n",
            "ep: 297, loss: 1.67529237, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 1.0000\n",
            "ep: 298, loss: 1.66267312, ep_t: 0.04 sec, t_t: 0.38 sec, bleu: 1.0000\n",
            "ep: 299, loss: 1.64844584, ep_t: 0.04 sec, t_t: 0.34 sec, bleu: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_duJkeSv_C0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSp8N_w8h9ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYcGOvvhh67b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKF9f65B_C5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "a4edad2c-bb20-41cb-ae90-49277c7a1628"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "model_name = 'BPE_h500/'\n",
        "folder = '/content/gdrive/My Drive/MToutputs/' + model_name\n",
        "\n",
        "epoch = 16 # desired checkpoint to load\n",
        "checkpoint = torch.load(folder + 'cp' + str(epoch) + '.tar') \n",
        "translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "last_finished_epoch = checkpoint['epoch']\n",
        "next_ep = last_finished_epoch + 1 # becomes first cur_ep value, if continue training this model\n",
        "print(\"ep_loss: {}\".format(ep_loss))\n",
        "print(\"last_finished_epoch: {}\".format(last_finished_epoch))\n",
        "\n",
        "# perform beam search on best model:\n",
        "translator.decoder.inf_alg = \"beam_search\"\n",
        "translator.decoder.beam_size = 10\n",
        "\n",
        "translator.eval()\n",
        "test_time, drop_bleu, bleu_time = test(translator, devBatches, dev_references, folder, last_finished_epoch, write=True)\n",
        "print(\"t_t: {}, bleu: {}\".format(test_time, drop_bleu))\n",
        "\n",
        "#!!!new experiment: overwrite the LR of the optimizer to implement a quasi-LR schedule\n",
        "#for g in optimizer.param_groups:\n",
        "#    g['lr'] = .00005\n",
        "#optimizer.state_dict()\n",
        "# continue training\n",
        "#translator.train()\n",
        "#translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=next_ep, folder=folder, save=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ep_loss: 5310.45263671875\n",
            "last_finished_epoch: 16\n",
            "t_t: 264.22928047180176, bleu: 0.25813851467550597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvLpjfzq_C9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare predictions with targets\n",
        "\n",
        "with open('/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt', 'r') as f:\n",
        "    preds = f.read()\n",
        "    \n",
        "    \n",
        "preds_list = to_sentences([preds])\n",
        "preds = preds_list[0]\n",
        "for i in range(250, 300):\n",
        "    print(\"pred:  \", preds[i])\n",
        "    print(\"target:\", ' '.join(dev_references[i][0]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mE__fRA2Rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23ade307-1418-49df-9981-e0594cdf148b"
      },
      "source": [
        "!grep -n \"instagram\" \"/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281:that's why instagram is fat .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vAf_tU_Qg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv77gLd0_DDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f7hjOX6_DG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yA_nYBd_DLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvA4GnQY_DOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaNfh_2y_DJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1sXMYPT_C8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wsvTGiX_C3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFgvMjYH1P8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "153de8ce-12ab-441b-da2e-f67cefc8e6dc"
      },
      "source": [
        "# now use parallelized version:\n",
        "translator.cuda()\n",
        "translator.encoder.dev = \"cuda:0\"\n",
        "translator.decoder.dev = \"cuda:0\"\n",
        "translator.eval()\n",
        "#print(\"making greedy predictions on the training set...\")\n",
        "translator.decoder.alg = \"greedy\"\n",
        "train_g_translations, test_time, bleu, bleu_time = test(translator, devBatches, train_references[:10])\n",
        "\n",
        "for trans in train_g_translations:\n",
        "    print(trans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david gallo this is bill lange . i'm dave gallo .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "we've got some of the most incredible video of titanic that's ever been seen , and we're not going to show you any of it .\n",
            "the truth of the about is that the titanic - - even though it's breaking all sorts of box office records - - it's not the most exciting story from the sea .\n",
            "and the problem , i think , is that we take the ocean for granted .\n",
            "when you think about it , the oceans are 75 percent of the planet .\n",
            "most of the planet is ocean water .\n",
            "the average depth is about two miles .\n",
            "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
            "and in the oceans , there are the longest mountain ranges on the planet .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0roBdyGf2Rus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecJqDlfY1nTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "dc06809f-d71a-4eb5-b0bd-e1bd94e39a9f"
      },
      "source": [
        "for trans in train_g_translations:\n",
        "    print(trans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david gallo this is bill lange . i'm dave gallo .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "part of the problem , i think , is we take the beach , or we see images like this of the ocean , and you look out at the great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there .\n",
            "the truth of the matter is that the titanic - - even though it's breaking all sorts of box office - - - the not not the most exciting story from the sea .\n",
            "and the problem , i think , is that we take the ocean for granted .\n",
            "and the problem about it , the oceans are 75 percent of the planet .\n",
            "most of the planet is ocean water .\n",
            "the average depth is about two miles .\n",
            "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
            "and in the oceans , there are the longest mountain ranges on the planet .\n",
            "most of the animals are in the oceans .\n",
            "most of the earthquakes and volcanoes are in the sea , at the bottom of the sea , at the bottom of the sea , at the\n",
            "the biodiversity and the <unk> in the ocean is higher , in places , than it is in the rainforests .\n",
            "it's mostly unexplored , and yet there are beautiful sights like this that <unk> us and make us become familiar with it .\n",
            "but when you're standing at the beach , i want you to think that you're standing at the edge of a very unfamiliar world .\n",
            "we have to have a very special technology to get into that unfamiliar world .\n",
            "we use the submarine alvin and we use cameras , and the cameras are something that that lange has developed with the help of sony .\n",
            "people that have partnered with us given us new eyes , not only on what exists - - the new landscapes as having having new eyes .\n",
            "people that have partnered with us have given us new eyes , not only on what exists - - the new landscapes at the bottom of the sea - - but also how we think about life on the planet itself .\n",
            "here's a jelly .\n",
            "it's one of my favorites , because it's got all sorts of working parts .\n",
            "this turns out to be the longest creature in the oceans .\n",
            "it gets up to about 150 feet long .\n",
            "but see all those different working things ?\n",
            "i love that kind of stuff .\n",
            "this turns out fishing <unk> on the bottom . they're going up and down .\n",
            "it's got tentacles dangling , swirling around like that .\n",
            "here's a that jelly .\n",
            "these are all individual animals banding together to make this one creature .\n",
            "and it's got these jet thrusters up in front that it'll use in a moment , and a little light .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKQTJXz8gfZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etb4QHe61nb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ivpDqfV1nfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGUUCRWN1nk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR1_3O_3bFij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ada868fc-1cae-48d0-8263-0b8222d91394"
      },
      "source": [
        "### early-stopping workaround\n",
        "fold = \"/content/gdrive/My Drive/models/\" # folder\n",
        "# for epoch 0, train the model from scratch\n",
        "print(\"training model 0...\")\n",
        "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=1, cur_ep=0, learn_rate=LR, folder=fold)\n",
        "\n",
        "# perform inference with this from-scratch model\n",
        "if torch.cuda.is_available():\n",
        "    translator.cpu()\n",
        "    translator.encoder.dev = \"cpu\"\n",
        "    translator.decoder.dev = \"cpu\"\n",
        "translator.eval()\n",
        "print(\"making greedy predictions on the dev set with model 0...\")\n",
        "start_time = time.time()\n",
        "#dev_g_translations = test(translator, dev_src_sentences)\n",
        "# only estimate the BLEU using ~1/8 of the dev set:\n",
        "dev_g_translations = test(translator, dev_src_sentences[:1000])\n",
        "\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds0.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model 0...\n",
            "epoch: 0, loss: 7208.15, time: 192.49 sec\n",
            "making greedy predictions on the dev set with model 0...\n",
            "...took 118.42 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4L2Mum2gwHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGfS4qoBayLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1075
        },
        "outputId": "91b32ef7-d2bd-4868-d457-964423832eb3"
      },
      "source": [
        "\n",
        "# for epochs 1 thru 6, load previous model from file\n",
        "for i in range(1,16): \n",
        "  \n",
        "    # intialize models so that can load into them\n",
        "    new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "    new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "    new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        new_translator.cuda()\n",
        "    \n",
        "    if opt == \"RMS\":\n",
        "        optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "    elif opt == \"ADAM\":\n",
        "        optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "    checkpoint = torch.load(fold + 'train_checkpoint' + str(i-1) + '.tar') # load previous checkpoint\n",
        "\n",
        "    new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    ep_loss = checkpoint['ep_loss']\n",
        "    print(ep_loss)\n",
        "\n",
        "    new_translator.train()\n",
        "    # train for an additional epoch\n",
        "    print(\"training model \" + str(i) + \"...\")\n",
        "\n",
        "    new_translator, final_loss = train2(new_translator, optimizer, batches, bsz=b, num_epochs=i+1, cur_ep=i, folder=fold)\n",
        "\n",
        "    \n",
        "    # perform inference with the new model\n",
        "    if torch.cuda.is_available():\n",
        "        new_translator.cpu()\n",
        "        new_translator.encoder.dev = \"cpu\"\n",
        "        new_translator.decoder.dev = \"cpu\"\n",
        "    new_translator.eval()\n",
        "    print(\"making greedy predictions on the dev set with model \" + str(i) + \"...\")\n",
        "    start_time = time.time()\n",
        "    #dev_g_translations = test(new_translator, dev_src_sentences)\n",
        "    # only estimate the BLEU using ~1/8 of the dev set:\n",
        "    dev_g_translations = test(new_translator, dev_src_sentences[:1000])\n",
        "    print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "    print()\n",
        "    # Create a local file to upload.\n",
        "    with open('/content/gdrive/My Drive/predictions/dev_greedy_preds' + str(i) + '.txt', 'w') as f:\n",
        "        for translation in dev_g_translations:\n",
        "            f.write(translation + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7208.1484, device='cuda:0', requires_grad=True)\n",
            "training model 1...\n",
            "epoch: 1, loss: 5146.83, time: 187.54 sec\n",
            "making greedy predictions on the dev set with model 1...\n",
            "...took 111.95 seconds\n",
            "\n",
            "tensor(5146.8330, device='cuda:0', requires_grad=True)\n",
            "training model 2...\n",
            "epoch: 2, loss: 4453.28, time: 186.93 sec\n",
            "making greedy predictions on the dev set with model 2...\n",
            "...took 106.26 seconds\n",
            "\n",
            "tensor(4453.2798, device='cuda:0', requires_grad=True)\n",
            "training model 3...\n",
            "epoch: 3, loss: 4072.48, time: 187.05 sec\n",
            "making greedy predictions on the dev set with model 3...\n",
            "...took 102.36 seconds\n",
            "\n",
            "tensor(4072.4817, device='cuda:0', requires_grad=True)\n",
            "training model 4...\n",
            "epoch: 4, loss: 3811.85, time: 186.74 sec\n",
            "making greedy predictions on the dev set with model 4...\n",
            "...took 107.40 seconds\n",
            "\n",
            "tensor(3811.8545, device='cuda:0', requires_grad=True)\n",
            "training model 5...\n",
            "epoch: 5, loss: 3609.07, time: 186.73 sec\n",
            "making greedy predictions on the dev set with model 5...\n",
            "...took 110.42 seconds\n",
            "\n",
            "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
            "training model 6...\n",
            "epoch: 6, loss: 3442.49, time: 185.58 sec\n",
            "making greedy predictions on the dev set with model 6...\n",
            "...took 87.56 seconds\n",
            "\n",
            "tensor(3442.4900, device='cuda:0', requires_grad=True)\n",
            "training model 7...\n",
            "epoch: 7, loss: 3300.97, time: 185.10 sec\n",
            "making greedy predictions on the dev set with model 7...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4c0ee8171b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#dev_g_translations = test(new_translator, dev_src_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# only estimate the BLEU using ~1/8 of the dev set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdev_g_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_translator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_src_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...took %0.2f seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(translator, testSrcSentences)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mencoder_inputs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSrcSent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2D tensor of size (1 x src_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mencoder_inputs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mtranslations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# don't need the input (decoder_inputs_batch = None), so skip its preprocessing steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"greedy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGreedyTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"beam_search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBeamSearchTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mgetGreedyTranslation\u001b[0;34m(self, encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;31m# so reshape to accomodate hidden state for a single time step for a single sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;31m# get_attStates() also expects a mask, so pass it (1 x 1 x Lsrc) mask of all 0s (merely leaves scores intact)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mdecoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# get_attStates() returns (q x hs), which is (1 x hs), in this case, so of correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;31m###!!!new pytorch no longer returns the lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m#packed_contexts, _ = pack_padded_sequence(padded_contexts, trg_lengths, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cHe83oAavCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UrmM0vLF2Ab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1442
        },
        "outputId": "00f28580-ab6d-4bd6-988a-a68ee70d953c"
      },
      "source": [
        "# train a model from scratch\n",
        "### NOTE - this will overwrite any existing checkpoints unless you change the path to where this train loop stores its checkpoints \n",
        "### (must change the fold param)\n",
        "\n",
        "#srcV=20,000, trgV=15,000, dropout=.3\n",
        "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=num_ep, folder=fold) # train model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, loss: 6262.86, time: 937.87 sec\n",
            "epoch: 1, loss: 4415.96, time: 937.92 sec\n",
            "epoch: 2, loss: 3752.01, time: 938.88 sec\n",
            "epoch: 3, loss: 3260.38, time: 938.36 sec\n",
            "epoch: 4, loss: 2819.71, time: 938.74 sec\n",
            "epoch: 5, loss: 2420.68, time: 939.27 sec\n",
            "epoch: 6, loss: 2073.30, time: 939.06 sec\n",
            "epoch: 7, loss: 1775.67, time: 937.98 sec\n",
            "epoch: 8, loss: 1537.47, time: 937.98 sec\n",
            "epoch: 9, loss: 1343.11, time: 938.27 sec\n",
            "epoch: 10, loss: 1189.45, time: 937.89 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e69b9dd42246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(translator, optimizer, batches, bsz, num_epochs, cur_ep, learn_rate, folder, shuf)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mpackedDists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0mpackedTargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackedDists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackedTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# packed_input is a tuple, whose first comp is the packed sequence, and second comp is \"batchsizes\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# which holds, at position i, the number of inputs for time step i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeProbDists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mcomputeProbDists\u001b[0;34m(self, packed_input, padded_encoder_states, decoder_initial_state, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;31m# convert states to attentional states before projecting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mpacked_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# concatenate contexts with the original decoder states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    145\u001b[0m                       \u001b[0;34m'the trace incorrect for any other combination of lengths.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAB0QVG4Tc37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ed2cd30-ff2f-4693-bd15-dfd2fc64a1ac"
      },
      "source": [
        "\n",
        "# perform inference with this from-scratch model\n",
        "\n",
        "# switch to CPU at test time:\n",
        "if torch.cuda.is_available():\n",
        "    translator.cpu()\n",
        "    translator.encoder.dev = \"cpu\"\n",
        "    translator.decoder.dev = \"cpu\"\n",
        "\n",
        "translator.eval()\n",
        "print(\"making greedy predictions on the dev set...\")\n",
        "start_time = time.time()\n",
        "dev_g_translations = test(translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "#g_test_accuracy = computeAccuracy(dev_g_translations, dev_trg_sentences, idx_to_trg_word)\n",
        "#print(\"g test accuracy: \" + g_test_accuracy)\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "#with open('dev_greedy_preds.txt', 'w') as f:\n",
        "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        print(translation)\n",
        "\n",
        "translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
        "print(\"making beam search predictions on the dev set...\")\n",
        "start_time = time.time()\n",
        "dev_b_translations = test(translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "#b_test_accuracy = computeAccuracy(dev_b_translations, dev_trg_sentences, idx_to_trg_word)\n",
        "#print(\"b test accuracy: \" + b_test_accuracy)\n",
        "\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds.txt', 'w') as f:\n",
        "    for translation in dev_b_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        print(translation)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making greedy predictions on the dev set...\n",
            "...took 1905.13 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_SR2JVHPYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctf-I1fsRA7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxgeg_plRClu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "58f23b56-6afd-4dcd-af1c-9376808d9886"
      },
      "source": [
        "# load checkpoint for inference only\n",
        "\n",
        "# intialize models so that can load into them\n",
        "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cuda()\n",
        "\n",
        "if opt == \"RMS\":\n",
        "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "elif opt == \"ADAM\":\n",
        "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "checkpoint = torch.load(fold + 'train_checkpoint' + str(5) + '.tar') # load previous checkpoint\n",
        "\n",
        "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "print(ep_loss)\n",
        "\n",
        "# perform inference with the new model\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cpu()\n",
        "    new_translator.encoder.dev = \"cpu\"\n",
        "    new_translator.decoder.dev = \"cpu\"\n",
        "new_translator.eval()\n",
        "new_translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
        "new_translator.decoder.beam_size = 10\n",
        "print(\"making beam search predictions on the dev set with model \" + str(5) + \"...\")\n",
        "\n",
        "#print(\"making greedy predictions on the dev set with model \" + str(5) + \"...\")\n",
        "start_time = time.time()\n",
        "dev_b_translations = test(new_translator, dev_src_sentences[:1000])\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_bs10' + str(5) + '.txt', 'w') as f:\n",
        "    for translation in dev_b_translations:\n",
        "        f.write(translation + '\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
            "making beam search predictions on the dev set with model 5...\n",
            "...took 1464.46 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVRqgImXg7P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load checkpoint for inference only\n",
        "###BPE beam search\n",
        "\n",
        "\n",
        "# intialize models so that can load into them\n",
        "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cuda()\n",
        "\n",
        "if opt == \"RMS\":\n",
        "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "elif opt == \"ADAM\":\n",
        "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "checkpoint = torch.load(fold + 'train_checkpoint' + str(19) + '.tar') # load previous checkpoint\n",
        "\n",
        "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "print(ep_loss)\n",
        "\n",
        "# perform inference with the new model\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cpu()\n",
        "    new_translator.encoder.dev = \"cpu\"\n",
        "    new_translator.decoder.dev = \"cpu\"\n",
        "new_translator.eval()\n",
        "print(\"making greedy predictions on the test set with model \" + str(19) + \"...\")\n",
        "start_time = time.time()\n",
        "dev_g_translations = test(new_translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_BPE' + str(19) + '.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}