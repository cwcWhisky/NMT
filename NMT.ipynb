{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/recurrent-NMT/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdgTryCSH3VW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "044f9c87-ba06-4776-e884-4842bea2c270"
      },
      "source": [
        "from encoderdecoder import *\n",
        "from batch import *\n",
        "from traintest import *\n",
        "from processCorpuses import *\n",
        "\n",
        "# from batch import *\n",
        "# from processCorpuses import *\n",
        "# from unrolled_encdec import *\n",
        "# from unrolled_traintest import *\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abNQ9Pk-CbQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0c50681e-19b9-4aa3-9beb-f72285a7f1b5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug 27 22:02:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gs-iwTJEHgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inserted a cell!!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmyqPOpNC_cn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "e4841a26-5de6-4946-b58a-14b69275b660"
      },
      "source": [
        "pip install subword-nmt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/26/08/58267cb3ac00f5f895457777ed9e0d106dbb5e6388fa7923d8663b04b849/subword_nmt-0.3.6-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d5CtGg8-jdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "e0fbf3ec-a91e-4433-e188-9a18dfd71943"
      },
      "source": [
        "path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
        "#path = '/content/gdrive/My Drive/toy_corpuses/' # path to toy corpuses\n",
        "\n",
        "norm_texts = normalizeCorpuses(path)\n",
        "\n",
        "train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = norm_texts[0], norm_texts[1], norm_texts[2], norm_texts[3], norm_texts[4]\n",
        "#train_src_sentences, train_trg_sentences = norm_texts[0], norm_texts[1]\n",
        "\n",
        "references = load_docs(path, tok=True)\n",
        "\n",
        "#print(\"train_references:\")\n",
        "#print(references[1])\n",
        "\n",
        "train_references = []\n",
        "for ref in references[1]:\n",
        "    # # hacky workaround: dn include newline that tokenizeCorpuses added to tok_train\n",
        "    # for ref in references[1][:-1]:\n",
        "    #print(ref)\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    train_references.append([ref.split()])\n",
        "    #print(train_references)\n",
        "    \n",
        "dev_references = []\n",
        "for ref in references[3]:\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    dev_references.append([ref.split()])\n",
        "    \n",
        "print()\n",
        "for sent in train_trg_sentences[:5]:\n",
        "    print(sent)\n",
        "for sent in train_references[:5]:\n",
        "    print(sent)    \n",
        "print()\n",
        "for sent in dev_trg_sentences[:5]:\n",
        "    print(sent)    \n",
        "for sent in dev_references[:5]:\n",
        "    print(sent)   \n",
        "print()\n",
        "\n",
        "train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
        "train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
        "dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
        "dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
        "test_src_sentences = [sent.split() for sent in test_src_sentences]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196884 pairs before filtering\n",
            "180 songs, 115 overly long\n",
            "196589 pairs after filtering\n",
            "tokenizing train.de...\n",
            "tokenizing train.en...\n",
            "tokenizing dev.de...\n",
            "tokenizing dev.en...\n",
            "tokenizing test.de...\n",
            "creating de names table...\n",
            "creating en names table...\n",
            "decasing train.de...\n",
            "decasing train.en...\n",
            "decasing dev.de...\n",
            "decasing dev.en...\n",
            "decasing test.de...\n",
            "\n",
            "David Gallo : this is Bill Lange . I'm Dave Gallo .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "we've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it .\n",
            "the truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea .\n",
            "and the problem , I think , is that we take the ocean for granted .\n",
            "[['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.']]\n",
            "[['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
            "[[\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
            "[['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
            "[['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
            "\n",
            "when I was in my 20s , I saw my very first psychotherapy client .\n",
            "I was a Ph.D. student in clinical psychology at Berkeley .\n",
            "she was a 26 - year - old woman named Alex .\n",
            "now Alex walked into her first session wearing jeans and a big slouchy top , and she dropped onto the couch in my office and kicked off her flats and told me she was there to talk about guy problems .\n",
            "now when I heard this , I was so relieved .\n",
            "[['When', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psychotherapy', 'client', '.']]\n",
            "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
            "[['She', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
            "[['Now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'jeans', 'and', 'a', 'big', 'slouchy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'flats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
            "[['Now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relieved', '.']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qHgJ0DYnW8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3b5a7e4-7fa7-4a1f-e249-d946532967be"
      },
      "source": [
        "print(train_src_sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Etre'], ['X', 'y', 'z'], ['Ce', 'soir']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb4icA9RnaZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55985def-ac12-4d2c-f82a-0645d3cc07a9"
      },
      "source": [
        "print(train_trg_sentences)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['To', 'be', 'free'], ['C', 'd'], ['E']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiZZZslF3H1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "fb8de96c-ccb9-437b-c216-da7085b6fb66"
      },
      "source": [
        "for sent in train_trg_sentences[:5]:\n",
        "    print(sent)\n",
        "for sent in train_references[:5]:\n",
        "    print(sent)    \n",
        "print()\n",
        "for sent in dev_trg_sentences[:5]:\n",
        "    print(sent)    \n",
        "for sent in dev_references[:5]:\n",
        "    print(sent)   \n",
        "print()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['David', 'Gallo', ':', 'this', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.']\n",
            "['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']\n",
            "[\"we've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']\n",
            "['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']\n",
            "['and', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']\n",
            "[['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.']]\n",
            "[['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
            "[[\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
            "[['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
            "[['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
            "\n",
            "['when', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psychotherapy', 'client', '.']\n",
            "['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']\n",
            "['she', 'was', 'a', '26', '-', 'year', '-', 'old', 'woman', 'named', 'Alex', '.']\n",
            "['now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'jeans', 'and', 'a', 'big', 'slouchy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'flats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']\n",
            "['now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relieved', '.']\n",
            "[['When', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psychotherapy', 'client', '.']]\n",
            "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
            "[['She', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
            "[['Now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'jeans', 'and', 'a', 'big', 'slouchy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'flats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
            "[['Now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relieved', '.']]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxE2E54aq9Ms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9b635321-9dad-4965-ea8c-afb37430bf60"
      },
      "source": [
        "\n",
        "\n",
        "#embType = 'jointBPE'\n",
        "embType = 'word'\n",
        "\n",
        "if embType == 'word':\n",
        "  \n",
        "    \n",
        "\n",
        "    src_counter, trg_counter = to_counters(train_src_sentences, train_trg_sentences)\n",
        "    #print(trg_counter)\n",
        "    trim_type = \"topK\"\n",
        "    #trim_type = \"threshold\"\n",
        "    if trim_type == \"threshold\":\n",
        "        st = 5\n",
        "        tt = 5\n",
        "        # st = 1 # for debug\n",
        "        # tt = 1\n",
        "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs(src_counter, trg_counter, srcThres=st, trgThres=tt)\n",
        "    elif trim_type == \"topK\":\n",
        "        trimmed_src_vocab, trimmed_trg_vocab = trim_vocabs2(src_counter, trg_counter, srcK=60000, trgK=45000)\n",
        "\n",
        "    train_src_sentences = removeOOV(train_src_sentences, trimmed_src_vocab)\n",
        "    train_trg_sentences = removeOOV(train_trg_sentences, trimmed_trg_vocab)\n",
        "    dev_src_sentences = removeOOV(dev_src_sentences, trimmed_src_vocab)\n",
        "    test_src_sentences = removeOOV(test_src_sentences, trimmed_src_vocab)\n",
        "\n",
        "    train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
        "    srcV, trgV, idx_to_src_word, idx_to_trg_word = computeVocabs(train_src_sentences, train_trg_sentences)\n",
        "\n",
        "    train_src_sentences = toIndices(train_src_sentences, srcV)\n",
        "    train_trg_sentences = toIndices(train_trg_sentences, trgV)\n",
        "    dev_src_sentences = toIndices(dev_src_sentences, srcV)   \n",
        "    test_src_sentences = toIndices(test_src_sentences, srcV)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lengths of vocabs before trimming: src: 120990, trg: 56561\n",
            "lengths of vocabs after trimming: src: 60000, trg: 45000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY4lSMJLsJdF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1ca15e19-77d7-4638-be58-d4991b5a8c0e"
      },
      "source": [
        "print(train_src_sentences[:10])\n",
        "print()\n",
        "print(train_trg_sentences[:10])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 3, 9], [13, 14, 15, 16, 17, 18, 5, 19, 20, 21, 22, 9], [13, 23, 24, 25, 26, 27, 28, 26, 29, 30, 31, 32, 33, 34, 35, 30, 30, 36, 13, 14, 15, 37, 38, 39, 9], [31, 40, 6, 30, 41, 31, 29, 42, 43, 44, 45, 46, 47, 48, 31, 49, 50, 51, 19, 6, 9], [10, 52, 30, 5, 53, 6, 30, 41, 13, 5, 19, 54, 55, 56, 57, 9], [58, 32, 59, 60, 30, 61, 31, 62, 63, 64, 65, 66, 9], [26, 67, 26, 68, 6, 69, 9], [31, 70, 71, 6, 72, 73, 74, 9], [24, 75, 64, 76, 6, 30, 41, 13, 77, 78, 79, 80, 81, 82, 83, 84, 85, 36, 86, 31, 87, 88, 89, 90, 30, 36, 43, 91, 36, 92, 93, 30, 94, 95, 96, 30, 97, 36, 98, 30, 99, 13, 23, 100, 101, 30, 102, 103, 104, 6, 9], [20, 105, 106, 107, 93, 31, 108, 109, 64, 65, 9]]\n",
            "\n",
            "[[2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 5, 11, 3], [2, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 11, 3], [2, 28, 29, 20, 30, 23, 31, 32, 27, 30, 33, 34, 35, 36, 37, 38, 14, 15, 39, 16, 17, 40, 19, 41, 30, 42, 11, 3], [2, 23, 43, 30, 23, 44, 8, 45, 23, 33, 46, 47, 48, 49, 50, 51, 52, 30, 53, 54, 55, 46, 49, 39, 23, 31, 56, 57, 22, 23, 24, 11, 3], [2, 14, 23, 58, 38, 59, 60, 38, 8, 45, 61, 62, 23, 63, 64, 65, 11, 3], [2, 66, 19, 60, 67, 42, 38, 23, 68, 69, 70, 71, 30, 23, 72, 11, 3], [2, 31, 30, 23, 72, 8, 63, 73, 11, 3], [2, 23, 74, 75, 8, 67, 76, 77, 11, 3], [2, 78, 30, 23, 58, 38, 59, 60, 38, 8, 61, 79, 80, 23, 81, 38, 82, 61, 83, 84, 85, 7, 30, 23, 63, 38, 14, 19, 86, 87, 80, 7, 88, 89, 90, 91, 38, 14, 49, 92, 14, 49, 93, 14, 94, 95, 14, 94, 96, 14, 94, 97, 38, 98, 19, 99, 100, 101, 64, 102, 103, 26, 104, 11, 3], [2, 14, 26, 23, 68, 38, 104, 69, 23, 105, 106, 107, 108, 23, 72, 11, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9YBOOKxshoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(srcV):\n",
        "  if i == 120:\n",
        "    break\n",
        "  print(word, \": \", srcV[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APXtMPK9FLhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(trgV):\n",
        "  if i == 120:\n",
        "    break\n",
        "  print(word, \": \", trgV[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93BFGKrYKXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "712538df-8b01-4a3b-9ce6-ab888c382838"
      },
      "source": [
        "if embType == 'jointBPE':\n",
        "    alreadyExists = False\n",
        "    if not alreadyExists:\n",
        "        numMerges = 30000\n",
        "        vocabThreshold = 10\n",
        "        !bash jointBPE.sh 30000 10 '/content/gdrive/My Drive/iwslt16_en_de/'\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning joint bpe and vocab using 30000 merge operations...\n",
            "applying bpe with vocab threshold of 10 to train...\n",
            "applying bpe with vocab threshold of 10 to dev and test...\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPITtnP8YYm7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "5f2e12e8-7015-4a24-b83e-285c39d3882b"
      },
      "source": [
        "path = '/content/gdrive/My Drive/iwslt16_en_de/' # path to corpuses\n",
        "\n",
        "bpe_texts = load_docs(path, bpe=True)\n",
        "train_src_sentences, train_trg_sentences, dev_src_sentences, dev_trg_sentences, test_src_sentences = bpe_texts[0], bpe_texts[1], bpe_texts[2], bpe_texts[3], bpe_texts[4]\n",
        "\n",
        "train_references = []\n",
        "for ref in train_trg_sentences[:]: # until think of better way\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    train_references.append([ref.split()])\n",
        "    \n",
        "dev_references = []\n",
        "for ref in dev_trg_sentences[:]:\n",
        "    ref = ref.replace(\" - \", \"-\")\n",
        "    dev_references.append([ref.split()])\n",
        "    \n",
        "\n",
        "for sent in train_trg_sentences[:5]:\n",
        "    print(sent)\n",
        "for sent in train_references[:5]:\n",
        "    print(sent)    \n",
        "print()\n",
        "for sent in dev_trg_sentences[:5]:\n",
        "    print(sent)    \n",
        "for sent in dev_references[:5]:\n",
        "    print(sent)   \n",
        "print()\n",
        "\n",
        "train_src_sentences = [sent.split() for sent in train_src_sentences]\n",
        "train_trg_sentences = [sent.split() for sent in train_trg_sentences]\n",
        "dev_src_sentences = [sent.split() for sent in dev_src_sentences]\n",
        "dev_trg_sentences = [sent.split() for sent in dev_trg_sentences]\n",
        "test_src_sentences = [sent.split() for sent in test_src_sentences]\n",
        "\n",
        "\n",
        "train_trg_sentences = add_start_end_tokens(train_trg_sentences)\n",
        "\n",
        "vocab, idx_to_subword = computeBPEvocabs(path + \"vocab.de\", path + \"vocab.en\")\n",
        "\n",
        "train_src_sentences = toIndices(train_src_sentences, vocab)\n",
        "train_trg_sentences = toIndices(train_trg_sentences, vocab)\n",
        "dev_src_sentences = toIndices(dev_src_sentences, vocab)   \n",
        "test_src_sentences = toIndices(test_src_sentences, vocab)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "David Gall@@ o : this is Bill L@@ ange . I'm Dave Gall@@ o .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "we've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it .\n",
            "the truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea .\n",
            "and the problem , I think , is that we take the ocean for granted .\n",
            "[['David', 'Gall@@', 'o', ':', 'this', 'is', 'Bill', 'L@@', 'ange', '.', \"I'm\", 'Dave', 'Gall@@', 'o', '.']]\n",
            "[['and', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.']]\n",
            "[[\"we've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.']]\n",
            "[['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', '--', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.']]\n",
            "[['and', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.']]\n",
            "\n",
            "when I was in my 20s , I saw my very first psycho@@ therapy client .\n",
            "I was a Ph.D. student in clinical psychology at Berkeley .\n",
            "she was a 26 - year - old woman named Alex .\n",
            "now Alex walked into her first session wearing je@@ ans and a big sl@@ ou@@ chy top , and she dropped onto the couch in my office and kicked off her fl@@ ats and told me she was there to talk about guy problems .\n",
            "now when I heard this , I was so relie@@ ved .\n",
            "[['when', 'I', 'was', 'in', 'my', '20s', ',', 'I', 'saw', 'my', 'very', 'first', 'psycho@@', 'therapy', 'client', '.']]\n",
            "[['I', 'was', 'a', 'Ph.D.', 'student', 'in', 'clinical', 'psychology', 'at', 'Berkeley', '.']]\n",
            "[['she', 'was', 'a', '26-year-old', 'woman', 'named', 'Alex', '.']]\n",
            "[['now', 'Alex', 'walked', 'into', 'her', 'first', 'session', 'wearing', 'je@@', 'ans', 'and', 'a', 'big', 'sl@@', 'ou@@', 'chy', 'top', ',', 'and', 'she', 'dropped', 'onto', 'the', 'couch', 'in', 'my', 'office', 'and', 'kicked', 'off', 'her', 'fl@@', 'ats', 'and', 'told', 'me', 'she', 'was', 'there', 'to', 'talk', 'about', 'guy', 'problems', '.']]\n",
            "[['now', 'when', 'I', 'heard', 'this', ',', 'I', 'was', 'so', 'relie@@', 'ved', '.']]\n",
            "\n",
            "number of german subwords is 23056\n",
            "number of english subwords is 6822\n",
            "total number of subwords is 29881\n",
            "warning: unknown word: û@@ in sent: ['mein', 'Favor@@', 'it', 'ist', 'der', 'in', 'der', 'Mitte', '--', 'MP3', '-', 'Player', '-', ',', 'N@@', 'asen', '-', 'Haar', '-', 'Tri@@', 'mmer', 'und', 'Cr@@', 'è@@', 'me', 'Br@@', 'û@@', 'l@@', 'é@@', 'e', 'F@@', 'ack@@', 'el', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: ê in sent: ['ash@@', 'ê', 'Ol@@', 'ê@@', 'n', '.', 'in', 'meiner', 'Sprache', 'bedeutet', 'das', ':', 'ich', 'danke', 'Ihnen', 'sehr', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: τ in sent: ['und', 'die', 'Antwort', ',', 'glaube', 'ich', ',', 'ist', 'ja', '.', '[', '\"', 'f', 'T', 'S@@', 'τ', '\"', ']', '.', 'was', 'Sie', 'gerade', 'sehen', ',', 'ist', 'wahrscheinlich', 'die', 'beste', 'Ent@@', 'spre@@', 'chung', 'zu', 'E', 'm@@', 'c@@', '²', 'für', 'Intelligenz', ',', 'die', 'ich', 'gesehen', 'habe', '.']\n",
            "removed it from the sentence.\n",
            "\n",
            "warning: unknown word: Œ@@ in sent: ['ich', 'begann', 'ein', 'neues', 'Œ@@', 'u', 'v', 're', '.']\n",
            "removed it from the sentence.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOI1bMHrcwpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b680dd9a-8f4b-4fa3-d3c5-a9bdf3bbce0b"
      },
      "source": [
        "# build model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "overfit_on_train = True\n",
        "\n",
        "if overfit_on_train:\n",
        "    b = 10\n",
        "    #dev_bsz = 10\n",
        "    dev_bsz = 1\n",
        "else:\n",
        "    b = 64\n",
        "    dev_bsz = 64\n",
        "    #dev_bsz = 1\n",
        "num_ep = 300 # number of epochs\n",
        "IS = 300 # input size\n",
        "enc_hs = 600 # encoder hidden size\n",
        "dec_hs = 600 # decoder hidden size\n",
        "enc_NL = 1 # number of layers\n",
        "dec_NL = 1\n",
        "enc_DR = .0 # dropout\n",
        "dec_DR = .0\n",
        "bi_enc = True\n",
        "project = True\n",
        "reverse_src = False\n",
        "#tie_weights = True\n",
        "tie_weights = False\n",
        "\n",
        "\n",
        "customLSTM = False\n",
        "#embType = \"jointBPE\"\n",
        "embType = \"word\"\n",
        "trim_type = \"threshold\"\n",
        "inputFeeding = True\n",
        "K = .7\n",
        "LR = .01 # learning rate\n",
        "WD = .0\n",
        "\n",
        "enc_emb_drop = .1\n",
        "dec_emb_drop = .1\n",
        "dropconnect = .5\n",
        "\n",
        "opt = \"ADAM\" # optimization algorithm\n",
        "model_name = 'debug_model/'\n",
        "msg = ' \\n'\n",
        "\n",
        "\n",
        "encoder_params = {}\n",
        "if embType == \"word\":\n",
        "    encoder_params['vocab_size'] = len(srcV)\n",
        "elif embType == \"jointBPE\":\n",
        "    encoder_params['vocab_size'] = len(vocab)\n",
        "encoder_params['input_size'] = IS\n",
        "encoder_params['hidden_size'] = enc_hs\n",
        "encoder_params['num_layers'] = enc_NL\n",
        "encoder_params['dropout'] = enc_DR\n",
        "encoder_params['dev'] = device\n",
        "encoder_params['bi_enc'] = bi_enc\n",
        "encoder_params['project'] = project # project concated enc states to dim of decoder\n",
        "#encoder_params['src_emb_drop'] = enc_emb_drop\n",
        "#encoder_params['dropconnect'] = dropconnect\n",
        "encoder_params['reverse_src'] = reverse_src\n",
        "#encoder_params['variational_drop'] = .5\n",
        "#encoder_params['i0_drop'] = .2\n",
        "#encoder_params['i1_drop'] = .5\n",
        "encoder_params['out_drop'] = .5\n",
        "encoder_params['init_scheme'] = 'layer_to_layer'\n",
        "#encoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
        "encoder_params['customLSTM'] = customLSTM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_params = {}\n",
        "if embType == \"word\":\n",
        "    decoder_params['sosIdx'] = trgV['<sos>']\n",
        "    #print(decoder_params['sosIdx'])\n",
        "    decoder_params['eosIdx'] = trgV['<eos>']\n",
        "    #print(decoder_params['eosIdx'])\n",
        "\n",
        "    decoder_params['padIdx'] = trgV['<pad>']\n",
        "    #print(decoder_params['padIdx'])\n",
        "\n",
        "    decoder_params['vocab_size'] = len(trgV)\n",
        "    decoder_params['idx_to_trg_word'] = idx_to_trg_word\n",
        "\n",
        "elif embType == \"jointBPE\":\n",
        "\n",
        "    decoder_params['sosIdx'] = vocab['<sos>']\n",
        "    #print(decoder_params['sosIdx'])\n",
        "    decoder_params['eosIdx'] = vocab['<eos>']\n",
        "    #print(decoder_params['eosIdx'])\n",
        "\n",
        "    decoder_params['padIdx'] = vocab['<pad>']\n",
        "    #print(decoder_params['padIdx'])\n",
        "\n",
        "    decoder_params['vocab_size'] = len(vocab)\n",
        "    decoder_params['idx_to_trg_word'] = idx_to_subword\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_params['input_size'] = IS\n",
        "decoder_params['hidden_size'] = dec_hs\n",
        "decoder_params['num_layers'] = dec_NL\n",
        "decoder_params['dropout'] = dec_DR\n",
        "#decoder_params['trg_emb_drop'] = dec_emb_drop\n",
        "#decoder_params['dropconnect'] = dropconnect\n",
        "\n",
        "decoder_params['dev'] = device\n",
        "#decoder_params['attention'] = \"global_att\"\n",
        "decoder_params['attention'] = None\n",
        "\n",
        "\n",
        "decoder_params['inf_alg'] = \"greedy_search\"\n",
        "#decoder_params['inf_alg'] = \"beam_search\"\n",
        "\n",
        "decoder_params['beam_size'] = 10\n",
        "#decoder_params['decode_slack'] = 8\n",
        "decoder_params['decode_slack'] = 20\n",
        "\n",
        "#decoder_params['variational_drop'] = .5\n",
        "#decoder_params['i0_drop'] = .0 # applied to trg embeddings prior to entering decoder lstm\n",
        "#decoder_params['i1_drop'] = .5\n",
        "decoder_params['out_drop'] = .5 # applied after final layer of decoder lstm\n",
        "#decoder_params['att_drop'] = .3 # applied after project back to input size\n",
        "#decoder_params['rec_drop'] = .5\n",
        "\n",
        "decoder_params['tie_weights'] = tie_weights\n",
        "#decoder_params['weights_to_drop'] = ['weight_hh_l0']\n",
        "decoder_params['customLSTM'] = customLSTM\n",
        "#decoder_params['input_feeding'] = inputFeeding\n",
        "#decoder_params['K'] = K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(encoder_params)\n",
        "decoder = Decoder(decoder_params)\n",
        "translator = RNNencdec(encoder, decoder, embType) # initialize model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    translator.cuda()\n",
        "    \n",
        "\n",
        "optimizer = torch.optim.Adam(translator.parameters(), lr=LR, weight_decay=WD) # initialize optimizer\n",
        "\n",
        "# folder to write checkpoints (train outputs) and translations (inference outputs)\n",
        "folder = '/content/gdrive/My Drive/MToutputs/' + model_name"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GR15Lmr1tht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6naTP-71riD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBgLZHAjfHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "x = !nvidia-smi | head -n 8 | tail -n 1 | cut -d ' ' -f 6-7\n",
        "current_gpu = ' '.join(x)\n",
        "with open(folder + 'model_train_stats.txt', 'w') as f:\n",
        "    f.write(msg)\n",
        "    if trim_type == \"threshold\":\n",
        "        f.write(\"srcThres: {}\\n\".format(st))\n",
        "        f.write(\"trgThres: {}\\n\".format(tt))\n",
        "\n",
        "    f.write(\"encoder_params:\\n\")\n",
        "    for key in encoder_params:\n",
        "        f.write(key + \": {}\\n\".format(encoder_params[key]))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"decoder_params:\\n\")\n",
        "    for key in decoder_params:\n",
        "        if key != \"idx_to_trg_word\":\n",
        "            f.write(key + \": {}\\n\".format(decoder_params[key]))\n",
        "        \n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"embType: {}\\n\".format(embType))\n",
        "    f.write(\"lr: {}\\n\".format(LR))\n",
        "    f.write(\"wd: {}\\n\".format(WD))\n",
        "    f.write(\"opt: {}\\n\".format(opt))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"bsz: {}\\n\".format(b))\n",
        "    f.write(\"dev_bsz: {}\\n\".format(dev_bsz))\n",
        "    f.write(\"current gpu: {}\\n\".format(current_gpu))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZnY73CfjlZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d91aa076-c36d-47bd-ed5d-424a77776882"
      },
      "source": [
        "if reverse_src:\n",
        "    if embType == \"word\":\n",
        "\n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "\n",
        "        print()\n",
        "\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])\n",
        "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_src_word[idx] for idx in sent])    \n",
        "\n",
        "    elif embType == \"jointBPE\":\n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])    \n",
        "        train_src_sentences = [sent[::-1] for sent in train_src_sentences]   \n",
        "        for sent in train_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent]) \n",
        "\n",
        "        print()\n",
        "\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])\n",
        "        dev_src_sentences = [sent[::-1] for sent in dev_src_sentences]\n",
        "        for sent in dev_src_sentences[:3]:\n",
        "            print([idx_to_subword[idx] for idx in sent])  \n",
        "\n",
        "trainingPairs = list(zip(train_src_sentences, train_trg_sentences))\n",
        "\n",
        "unrolled = False\n",
        "if overfit_on_train: \n",
        "    trainBatches = getBatches(trainingPairs[:10], b, device)\n",
        "    devBatches = getDevBatches(train_src_sentences[:10], dev_bsz, device)\n",
        "else:\n",
        "    trainBatches = getBatches(trainingPairs, b, device)\n",
        "    devBatches = getDevBatches(dev_src_sentences, dev_bsz, device)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sorting by trg length\n",
            "encoder_inputs_tensor:\n",
            "tensor([[ 24,  75,  64,  76,   6,  30,  41,  13,  77,  78,  79,  80,  81,  82,\n",
            "          83,  84,  85,  36,  86,  31,  87,  88,  89,  90,  30,  36,  43,  91,\n",
            "          36,  92,  93,  30,  94,  95,  96,  30,  97,  36,  98,  30,  99,  13,\n",
            "          23, 100, 101,  30, 102, 103, 104,   6,   9],\n",
            "        [ 31,  40,   6,  30,  41,  31,  29,  42,  43,  44,  45,  46,  47,  48,\n",
            "          31,  49,  50,  51,  19,   6,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  23,  24,  25,  26,  27,  28,  26,  29,  30,  31,  32,  33,  34,\n",
            "          35,  30,  30,  36,  13,  14,  15,  37,  38,  39,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 10,  52,  30,   5,  53,   6,  30,  41,  13,   5,  19,  54,  55,  56,\n",
            "          57,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  14,  15,  16,  17,  18,   5,  19,  20,  21,  22,   9,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 58,  32,  59,  60,  30,  61,  31,  62,  63,  64,  65,  66,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 20, 105, 106, 107,  93,  31, 108, 109,  64,  65,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   3,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 26,  67,  26,  68,   6,  69,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  70,  71,   6,  72,  73,  74,   9,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "decoder_inputs_tensor:\n",
            "tensor([[  2,  78,  30,  23,  58,  38,  59,  60,  38,   8,  61,  79,  80,  23,\n",
            "          81,  38,  82,  61,  83,  84,  85,   7,  30,  23,  63,  38,  14,  19,\n",
            "          86,  87,  80,   7,  88,  89,  90,  91,  38,  14,  49,  92,  14,  49,\n",
            "          93,  14,  94,  95,  14,  94,  96,  14,  94,  97,  38,  98,  19,  99,\n",
            "         100, 101,  64, 102, 103,  26, 104,  11],\n",
            "        [  2,  23,  43,  30,  23,  44,   8,  45,  23,  33,  46,  47,  48,  49,\n",
            "          50,  51,  52,  30,  53,  54,  55,  46,  49,  39,  23,  31,  56,  57,\n",
            "          22,  23,  24,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  28,  29,  20,  30,  23,  31,  32,  27,  30,  33,  34,  35,  36,\n",
            "          37,  38,  14,  15,  39,  16,  17,  40,  19,  41,  30,  42,  11,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  23,  58,  38,  59,  60,  38,   8,  45,  61,  62,  23,  63,\n",
            "          64,  65,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
            "          27,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  66,  19,  60,  67,  42,  38,  23,  68,  69,  70,  71,  30,  23,\n",
            "          72,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  14,  26,  23,  68,  38, 104,  69,  23, 105, 106, 107, 108,  23,\n",
            "          72,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,   5,  11,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  31,  30,  23,  72,   8,  63,  73,  11,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,  23,  74,  75,   8,  67,  76,  77,  11,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "target_indices_tensor:\n",
            "tensor([[ 78,  30,  23,  58,  38,  59,  60,  38,   8,  61,  79,  80,  23,  81,\n",
            "          38,  82,  61,  83,  84,  85,   7,  30,  23,  63,  38,  14,  19,  86,\n",
            "          87,  80,   7,  88,  89,  90,  91,  38,  14,  49,  92,  14,  49,  93,\n",
            "          14,  94,  95,  14,  94,  96,  14,  94,  97,  38,  98,  19,  99, 100,\n",
            "         101,  64, 102, 103,  26, 104,  11,   3],\n",
            "        [ 23,  43,  30,  23,  44,   8,  45,  23,  33,  46,  47,  48,  49,  50,\n",
            "          51,  52,  30,  53,  54,  55,  46,  49,  39,  23,  31,  56,  57,  22,\n",
            "          23,  24,  11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 28,  29,  20,  30,  23,  31,  32,  27,  30,  33,  34,  35,  36,  37,\n",
            "          38,  14,  15,  39,  16,  17,  40,  19,  41,  30,  42,  11,   3,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  23,  58,  38,  59,  60,  38,   8,  45,  61,  62,  23,  63,  64,\n",
            "          65,  11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 66,  19,  60,  67,  42,  38,  23,  68,  69,  70,  71,  30,  23,  72,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 14,  26,  23,  68,  38, 104,  69,  23, 105, 106, 107, 108,  23,  72,\n",
            "          11,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  4,   5,   6,   7,   8,   9,  10,  11,  12,  13,   5,  11,   3,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  30,  23,  72,   8,  63,  73,  11,   3,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 23,  74,  75,   8,  67,  76,  77,  11,   3,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "sorted_src_lengths:\n",
            "tensor([51, 25, 21, 16, 13, 13, 12, 11,  8,  7], device='cuda:0')\n",
            "\n",
            "idxs_in_enc_inputs_tensor:\n",
            "tensor([0, 2, 1, 3, 7, 5, 4, 6, 9, 8], device='cuda:0')\n",
            "\n",
            "(sorted) encoder_inputs_tensor:\n",
            "tensor([[ 24,  75,  64,  76,   6,  30,  41,  13,  77,  78,  79,  80,  81,  82,\n",
            "          83,  84,  85,  36,  86,  31,  87,  88,  89,  90,  30,  36,  43,  91,\n",
            "          36,  92,  93,  30,  94,  95,  96,  30,  97,  36,  98,  30,  99,  13,\n",
            "          23, 100, 101,  30, 102, 103, 104,   6,   9],\n",
            "        [ 13,  23,  24,  25,  26,  27,  28,  26,  29,  30,  31,  32,  33,  34,\n",
            "          35,  30,  30,  36,  13,  14,  15,  37,  38,  39,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  40,   6,  30,  41,  31,  29,  42,  43,  44,  45,  46,  47,  48,\n",
            "          31,  49,  50,  51,  19,   6,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 10,  52,  30,   5,  53,   6,  30,  41,  13,   5,  19,  54,  55,  56,\n",
            "          57,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [  2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,   3,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 58,  32,  59,  60,  30,  61,  31,  62,  63,  64,  65,  66,   9,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 13,  14,  15,  16,  17,  18,   5,  19,  20,  21,  22,   9,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 20, 105, 106, 107,  93,  31, 108, 109,  64,  65,   9,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 31,  70,  71,   6,  72,  73,  74,   9,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
            "        [ 26,  67,  26,  68,   6,  69,   9,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "idxs_in_sorted_enc_inputs_tensor:\n",
            "tensor([0, 2, 1, 3, 6, 5, 7, 4, 9, 8], device='cuda:0')\n",
            "\n",
            "took 0.05 seconds to get all the batches\n",
            "took 0.00 seconds to get all the devBatches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJrmWhsS5J5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e2b191a5-fd3d-4e8a-898a-35df800b27e3"
      },
      "source": [
        "print(trainBatches)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[((tensor([[3, 4, 5],\n",
            "        [6, 7, 0],\n",
            "        [2, 0, 0]], device='cuda:0'), tensor([3, 2, 1], device='cuda:0'), tensor([2, 0, 1], device='cuda:0')), (tensor([[2, 4, 5, 6],\n",
            "        [2, 7, 8, 0],\n",
            "        [2, 9, 0, 0]], device='cuda:0'), tensor([4, 3, 2], device='cuda:0'), tensor([[[False, False, False],\n",
            "         [False, False, False],\n",
            "         [False, False, False],\n",
            "         [False, False, False]],\n",
            "\n",
            "        [[False, False,  True],\n",
            "         [False, False,  True],\n",
            "         [False, False,  True],\n",
            "         [False, False,  True]],\n",
            "\n",
            "        [[False,  True,  True],\n",
            "         [False,  True,  True],\n",
            "         [False,  True,  True],\n",
            "         [False,  True,  True]]], device='cuda:0')), (tensor([[4, 5, 6, 3],\n",
            "        [7, 8, 3, 0],\n",
            "        [9, 3, 0, 0]], device='cuda:0'), tensor([4, 3, 2], device='cuda:0')))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jofhcYlQ7tvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47dd1778-0d76-4f3b-dec1-664d9a84ea33"
      },
      "source": [
        "\n",
        "if overfit_on_train:\n",
        "    translator = train(translator, optimizer, trainBatches, devBatches, train_references[:10], num_epochs=num_ep, cur_ep=0, folder=folder, save=False)\n",
        "else:\n",
        "    translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=0, folder=folder, save=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ep: 00, loss: 2345.11401367, ep_t: 0.05 sec, t_t: 0.28 sec, bleu: 0.0080\n",
            "ep: 01, loss: 1940.06958008, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 0.0200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ep: 02, loss: 820.52441406, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.1296\n",
            "ep: 03, loss: 656.14984131, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.0144\n",
            "ep: 04, loss: 416.89974976, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.0722\n",
            "ep: 05, loss: 231.33290100, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.1688\n",
            "ep: 06, loss: 127.36117554, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 0.3996\n",
            "ep: 07, loss: 84.70709229, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 0.2172\n",
            "ep: 08, loss: 70.37828827, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.5717\n",
            "ep: 09, loss: 40.88386154, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 0.5125\n",
            "ep: 10, loss: 27.97411346, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.8266\n",
            "ep: 11, loss: 19.75640678, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.8700\n",
            "ep: 12, loss: 14.46535015, ep_t: 0.03 sec, t_t: 0.41 sec, bleu: 0.7200\n",
            "ep: 13, loss: 10.52904892, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 0.7200\n",
            "ep: 14, loss: 8.12210274, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 15, loss: 6.10749912, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 16, loss: 4.86690903, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 17, loss: 3.75145006, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 18, loss: 2.89943790, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 19, loss: 2.26242208, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 20, loss: 1.68551970, ep_t: 0.04 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 21, loss: 1.28771651, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 22, loss: 0.98314488, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 23, loss: 0.77062321, ep_t: 0.04 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 24, loss: 0.62443912, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 25, loss: 0.51360792, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 26, loss: 0.42470747, ep_t: 0.03 sec, t_t: 0.29 sec, bleu: 1.0000\n",
            "ep: 27, loss: 0.35472569, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 28, loss: 0.30124503, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 29, loss: 0.26040223, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 30, loss: 0.22850934, ep_t: 0.04 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 31, loss: 0.20313139, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 32, loss: 0.18245547, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 33, loss: 0.16531239, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 34, loss: 0.15091315, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 35, loss: 0.13866834, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 36, loss: 0.12815358, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 37, loss: 0.11905759, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 38, loss: 0.11112598, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 39, loss: 0.10417800, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 40, loss: 0.09805219, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 41, loss: 0.09263355, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 42, loss: 0.08782063, ep_t: 0.03 sec, t_t: 0.41 sec, bleu: 1.0000\n",
            "ep: 43, loss: 0.08352565, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 44, loss: 0.07967570, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 45, loss: 0.07621010, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 46, loss: 0.07307771, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 47, loss: 0.07023605, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 48, loss: 0.06765429, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 49, loss: 0.06530664, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 50, loss: 0.06315437, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 51, loss: 0.06118441, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 52, loss: 0.05937476, ep_t: 0.04 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 53, loss: 0.05770741, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 54, loss: 0.05617072, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 55, loss: 0.05475315, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 56, loss: 0.05343741, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 57, loss: 0.05221053, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 58, loss: 0.05107265, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 59, loss: 0.05000718, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 60, loss: 0.04900616, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 61, loss: 0.04807246, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 62, loss: 0.04719116, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 63, loss: 0.04636455, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 64, loss: 0.04558334, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 65, loss: 0.04484596, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 66, loss: 0.04414244, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 67, loss: 0.04347988, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 68, loss: 0.04285034, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 69, loss: 0.04225010, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 70, loss: 0.04167788, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 71, loss: 0.04113328, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 72, loss: 0.04061276, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 73, loss: 0.04011321, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 74, loss: 0.03963595, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 75, loss: 0.03917785, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 76, loss: 0.03873609, ep_t: 0.04 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 77, loss: 0.03831327, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 78, loss: 0.03790345, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 79, loss: 0.03750671, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 80, loss: 0.03712453, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 81, loss: 0.03675438, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 82, loss: 0.03639222, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 83, loss: 0.03604625, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 84, loss: 0.03570661, ep_t: 0.03 sec, t_t: 0.29 sec, bleu: 1.0000\n",
            "ep: 85, loss: 0.03537828, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 86, loss: 0.03505686, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 87, loss: 0.03474606, ep_t: 0.04 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 88, loss: 0.03444204, ep_t: 0.04 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 89, loss: 0.03414338, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 90, loss: 0.03385473, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 91, loss: 0.03356978, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 92, loss: 0.03329483, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 93, loss: 0.03302347, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 94, loss: 0.03275937, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 95, loss: 0.03249921, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 96, loss: 0.03224416, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 97, loss: 0.03199341, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 98, loss: 0.03174874, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 99, loss: 0.03150776, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 100, loss: 0.03127225, ep_t: 0.03 sec, t_t: 0.42 sec, bleu: 1.0000\n",
            "ep: 101, loss: 0.03103760, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 102, loss: 0.03080579, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 103, loss: 0.03057958, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 104, loss: 0.03035315, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 105, loss: 0.03012825, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 106, loss: 0.02990729, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 107, loss: 0.02969265, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 108, loss: 0.02948075, ep_t: 0.03 sec, t_t: 0.28 sec, bleu: 1.0000\n",
            "ep: 109, loss: 0.02927349, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n",
            "ep: 110, loss: 0.02906731, ep_t: 0.03 sec, t_t: 0.27 sec, bleu: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-b1bcc6d938c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moverfit_on_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_references\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_ep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_ep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/traintest.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(translator, optimizer, trainBatches, devBatches, references, num_epochs, cur_ep, folder, save)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m#test_time, nodrop_bleu, bleu_time = test(translator, devBatches, references, folder, ep, write=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtest_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevBatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/traintest.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(translator, devBatches, references, folder, ep, write)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mcorpus_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list of (corpus_idx, str(translation)) pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevBatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtranslations_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;31m#for translation in translations:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m#    print(translation) # in case of file writing failures, extract from jupyter display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;31m# don't need the input (decoder_inputs_batch = None), so skip its preprocessing steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf_alg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"greedy_search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGreedyTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf_alg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"beam_search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBeamSearchTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mgetGreedyTranslation\u001b[0;34m(self, decoder_inputs_batch, encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;31m# (bsz x hs) x (hs x V_trg) = (bsz x V_trg):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mdistOverNextWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojectToV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0;31m# normalize into a probability distribution over possible next words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mdistOverNextWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsoftmax1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistOverNextWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_duJkeSv_C0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSp8N_w8h9ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYcGOvvhh67b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKF9f65B_C5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "a4edad2c-bb20-41cb-ae90-49277c7a1628"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "model_name = 'BPE_h500/'\n",
        "folder = '/content/gdrive/My Drive/MToutputs/' + model_name\n",
        "\n",
        "epoch = 16 # desired checkpoint to load\n",
        "checkpoint = torch.load(folder + 'cp' + str(epoch) + '.tar') \n",
        "translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "last_finished_epoch = checkpoint['epoch']\n",
        "next_ep = last_finished_epoch + 1 # becomes first cur_ep value, if continue training this model\n",
        "print(\"ep_loss: {}\".format(ep_loss))\n",
        "print(\"last_finished_epoch: {}\".format(last_finished_epoch))\n",
        "\n",
        "# perform beam search on best model:\n",
        "translator.decoder.inf_alg = \"beam_search\"\n",
        "translator.decoder.beam_size = 10\n",
        "\n",
        "translator.eval()\n",
        "test_time, drop_bleu, bleu_time = test(translator, devBatches, dev_references, folder, last_finished_epoch, write=True)\n",
        "print(\"t_t: {}, bleu: {}\".format(test_time, drop_bleu))\n",
        "\n",
        "#!!!new experiment: overwrite the LR of the optimizer to implement a quasi-LR schedule\n",
        "#for g in optimizer.param_groups:\n",
        "#    g['lr'] = .00005\n",
        "#optimizer.state_dict()\n",
        "# continue training\n",
        "#translator.train()\n",
        "#translator = train(translator, optimizer, trainBatches, devBatches, dev_references, num_epochs=num_ep, cur_ep=next_ep, folder=folder, save=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ep_loss: 5310.45263671875\n",
            "last_finished_epoch: 16\n",
            "t_t: 264.22928047180176, bleu: 0.25813851467550597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvLpjfzq_C9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compare predictions with targets\n",
        "\n",
        "with open('/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt', 'r') as f:\n",
        "    preds = f.read()\n",
        "    \n",
        "    \n",
        "preds_list = to_sentences([preds])\n",
        "preds = preds_list[0]\n",
        "for i in range(250, 300):\n",
        "    print(\"pred:  \", preds[i])\n",
        "    print(\"target:\", ' '.join(dev_references[i][0]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mE__fRA2Rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23ade307-1418-49df-9981-e0594cdf148b"
      },
      "source": [
        "!grep -n \"instagram\" \"/content/gdrive/My Drive/MToutputs/lstm_dr/preds10.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "281:that's why instagram is fat .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vAf_tU_Qg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv77gLd0_DDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f7hjOX6_DG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yA_nYBd_DLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvA4GnQY_DOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaNfh_2y_DJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1sXMYPT_C8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wsvTGiX_C3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFgvMjYH1P8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "153de8ce-12ab-441b-da2e-f67cefc8e6dc"
      },
      "source": [
        "# now use parallelized version:\n",
        "translator.cuda()\n",
        "translator.encoder.dev = \"cuda:0\"\n",
        "translator.decoder.dev = \"cuda:0\"\n",
        "translator.eval()\n",
        "#print(\"making greedy predictions on the training set...\")\n",
        "translator.decoder.alg = \"greedy\"\n",
        "train_g_translations, test_time, bleu, bleu_time = test(translator, devBatches, train_references[:10])\n",
        "\n",
        "for trans in train_g_translations:\n",
        "    print(trans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david gallo this is bill lange . i'm dave gallo .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "we've got some of the most incredible video of titanic that's ever been seen , and we're not going to show you any of it .\n",
            "the truth of the about is that the titanic - - even though it's breaking all sorts of box office records - - it's not the most exciting story from the sea .\n",
            "and the problem , i think , is that we take the ocean for granted .\n",
            "when you think about it , the oceans are 75 percent of the planet .\n",
            "most of the planet is ocean water .\n",
            "the average depth is about two miles .\n",
            "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
            "and in the oceans , there are the longest mountain ranges on the planet .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0roBdyGf2Rus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecJqDlfY1nTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "dc06809f-d71a-4eb5-b0bd-e1bd94e39a9f"
      },
      "source": [
        "for trans in train_g_translations:\n",
        "    print(trans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "david gallo this is bill lange . i'm dave gallo .\n",
            "and we're going to tell you some stories from the sea here in video .\n",
            "part of the problem , i think , is we take the beach , or we see images like this of the ocean , and you look out at the great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there .\n",
            "the truth of the matter is that the titanic - - even though it's breaking all sorts of box office - - - the not not the most exciting story from the sea .\n",
            "and the problem , i think , is that we take the ocean for granted .\n",
            "and the problem about it , the oceans are 75 percent of the planet .\n",
            "most of the planet is ocean water .\n",
            "the average depth is about two miles .\n",
            "part of the problem , i think , is we stand at the beach , or we see images like this of the ocean , and you look out at this great big blue expanse , and it's shimmering and it's moving and there's waves and there's surf and there's tides , but you have no idea for what lies in there\n",
            "and in the oceans , there are the longest mountain ranges on the planet .\n",
            "most of the animals are in the oceans .\n",
            "most of the earthquakes and volcanoes are in the sea , at the bottom of the sea , at the bottom of the sea , at the\n",
            "the biodiversity and the <unk> in the ocean is higher , in places , than it is in the rainforests .\n",
            "it's mostly unexplored , and yet there are beautiful sights like this that <unk> us and make us become familiar with it .\n",
            "but when you're standing at the beach , i want you to think that you're standing at the edge of a very unfamiliar world .\n",
            "we have to have a very special technology to get into that unfamiliar world .\n",
            "we use the submarine alvin and we use cameras , and the cameras are something that that lange has developed with the help of sony .\n",
            "people that have partnered with us given us new eyes , not only on what exists - - the new landscapes as having having new eyes .\n",
            "people that have partnered with us have given us new eyes , not only on what exists - - the new landscapes at the bottom of the sea - - but also how we think about life on the planet itself .\n",
            "here's a jelly .\n",
            "it's one of my favorites , because it's got all sorts of working parts .\n",
            "this turns out to be the longest creature in the oceans .\n",
            "it gets up to about 150 feet long .\n",
            "but see all those different working things ?\n",
            "i love that kind of stuff .\n",
            "this turns out fishing <unk> on the bottom . they're going up and down .\n",
            "it's got tentacles dangling , swirling around like that .\n",
            "here's a that jelly .\n",
            "these are all individual animals banding together to make this one creature .\n",
            "and it's got these jet thrusters up in front that it'll use in a moment , and a little light .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKQTJXz8gfZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etb4QHe61nb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ivpDqfV1nfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGUUCRWN1nk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR1_3O_3bFij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ada868fc-1cae-48d0-8263-0b8222d91394"
      },
      "source": [
        "### early-stopping workaround\n",
        "fold = \"/content/gdrive/My Drive/models/\" # folder\n",
        "# for epoch 0, train the model from scratch\n",
        "print(\"training model 0...\")\n",
        "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=1, cur_ep=0, learn_rate=LR, folder=fold)\n",
        "\n",
        "# perform inference with this from-scratch model\n",
        "if torch.cuda.is_available():\n",
        "    translator.cpu()\n",
        "    translator.encoder.dev = \"cpu\"\n",
        "    translator.decoder.dev = \"cpu\"\n",
        "translator.eval()\n",
        "print(\"making greedy predictions on the dev set with model 0...\")\n",
        "start_time = time.time()\n",
        "#dev_g_translations = test(translator, dev_src_sentences)\n",
        "# only estimate the BLEU using ~1/8 of the dev set:\n",
        "dev_g_translations = test(translator, dev_src_sentences[:1000])\n",
        "\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds0.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model 0...\n",
            "epoch: 0, loss: 7208.15, time: 192.49 sec\n",
            "making greedy predictions on the dev set with model 0...\n",
            "...took 118.42 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4L2Mum2gwHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGfS4qoBayLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1075
        },
        "outputId": "91b32ef7-d2bd-4868-d457-964423832eb3"
      },
      "source": [
        "\n",
        "# for epochs 1 thru 6, load previous model from file\n",
        "for i in range(1,16): \n",
        "  \n",
        "    # intialize models so that can load into them\n",
        "    new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "    new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "    new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        new_translator.cuda()\n",
        "    \n",
        "    if opt == \"RMS\":\n",
        "        optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "    elif opt == \"ADAM\":\n",
        "        optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "    checkpoint = torch.load(fold + 'train_checkpoint' + str(i-1) + '.tar') # load previous checkpoint\n",
        "\n",
        "    new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    ep_loss = checkpoint['ep_loss']\n",
        "    print(ep_loss)\n",
        "\n",
        "    new_translator.train()\n",
        "    # train for an additional epoch\n",
        "    print(\"training model \" + str(i) + \"...\")\n",
        "\n",
        "    new_translator, final_loss = train2(new_translator, optimizer, batches, bsz=b, num_epochs=i+1, cur_ep=i, folder=fold)\n",
        "\n",
        "    \n",
        "    # perform inference with the new model\n",
        "    if torch.cuda.is_available():\n",
        "        new_translator.cpu()\n",
        "        new_translator.encoder.dev = \"cpu\"\n",
        "        new_translator.decoder.dev = \"cpu\"\n",
        "    new_translator.eval()\n",
        "    print(\"making greedy predictions on the dev set with model \" + str(i) + \"...\")\n",
        "    start_time = time.time()\n",
        "    #dev_g_translations = test(new_translator, dev_src_sentences)\n",
        "    # only estimate the BLEU using ~1/8 of the dev set:\n",
        "    dev_g_translations = test(new_translator, dev_src_sentences[:1000])\n",
        "    print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "    print()\n",
        "    # Create a local file to upload.\n",
        "    with open('/content/gdrive/My Drive/predictions/dev_greedy_preds' + str(i) + '.txt', 'w') as f:\n",
        "        for translation in dev_g_translations:\n",
        "            f.write(translation + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7208.1484, device='cuda:0', requires_grad=True)\n",
            "training model 1...\n",
            "epoch: 1, loss: 5146.83, time: 187.54 sec\n",
            "making greedy predictions on the dev set with model 1...\n",
            "...took 111.95 seconds\n",
            "\n",
            "tensor(5146.8330, device='cuda:0', requires_grad=True)\n",
            "training model 2...\n",
            "epoch: 2, loss: 4453.28, time: 186.93 sec\n",
            "making greedy predictions on the dev set with model 2...\n",
            "...took 106.26 seconds\n",
            "\n",
            "tensor(4453.2798, device='cuda:0', requires_grad=True)\n",
            "training model 3...\n",
            "epoch: 3, loss: 4072.48, time: 187.05 sec\n",
            "making greedy predictions on the dev set with model 3...\n",
            "...took 102.36 seconds\n",
            "\n",
            "tensor(4072.4817, device='cuda:0', requires_grad=True)\n",
            "training model 4...\n",
            "epoch: 4, loss: 3811.85, time: 186.74 sec\n",
            "making greedy predictions on the dev set with model 4...\n",
            "...took 107.40 seconds\n",
            "\n",
            "tensor(3811.8545, device='cuda:0', requires_grad=True)\n",
            "training model 5...\n",
            "epoch: 5, loss: 3609.07, time: 186.73 sec\n",
            "making greedy predictions on the dev set with model 5...\n",
            "...took 110.42 seconds\n",
            "\n",
            "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
            "training model 6...\n",
            "epoch: 6, loss: 3442.49, time: 185.58 sec\n",
            "making greedy predictions on the dev set with model 6...\n",
            "...took 87.56 seconds\n",
            "\n",
            "tensor(3442.4900, device='cuda:0', requires_grad=True)\n",
            "training model 7...\n",
            "epoch: 7, loss: 3300.97, time: 185.10 sec\n",
            "making greedy predictions on the dev set with model 7...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4c0ee8171b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#dev_g_translations = test(new_translator, dev_src_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# only estimate the BLEU using ~1/8 of the dev set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdev_g_translations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_translator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_src_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...took %0.2f seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(translator, testSrcSentences)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mencoder_inputs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestSrcSent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2D tensor of size (1 x src_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mencoder_inputs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mtranslations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# don't need the input (decoder_inputs_batch = None), so skip its preprocessing steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"greedy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetGreedyTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"beam_search\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBeamSearchTranslation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mgetGreedyTranslation\u001b[0;34m(self, encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;31m# so reshape to accomodate hidden state for a single time step for a single sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;31m# get_attStates() also expects a mask, so pass it (1 x 1 x Lsrc) mask of all 0s (merely leaves scores intact)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mdecoder_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# get_attStates() returns (q x hs), which is (1 x hs), in this case, so of correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;31m###!!!new pytorch no longer returns the lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m#packed_contexts, _ = pack_padded_sequence(padded_contexts, trg_lengths, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cHe83oAavCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UrmM0vLF2Ab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1442
        },
        "outputId": "00f28580-ab6d-4bd6-988a-a68ee70d953c"
      },
      "source": [
        "# train a model from scratch\n",
        "### NOTE - this will overwrite any existing checkpoints unless you change the path to where this train loop stores its checkpoints \n",
        "### (must change the fold param)\n",
        "\n",
        "#srcV=20,000, trgV=15,000, dropout=.3\n",
        "translator, final_loss = train2(translator, optimizer, batches, bsz=b, num_epochs=num_ep, folder=fold) # train model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, loss: 6262.86, time: 937.87 sec\n",
            "epoch: 1, loss: 4415.96, time: 937.92 sec\n",
            "epoch: 2, loss: 3752.01, time: 938.88 sec\n",
            "epoch: 3, loss: 3260.38, time: 938.36 sec\n",
            "epoch: 4, loss: 2819.71, time: 938.74 sec\n",
            "epoch: 5, loss: 2420.68, time: 939.27 sec\n",
            "epoch: 6, loss: 2073.30, time: 939.06 sec\n",
            "epoch: 7, loss: 1775.67, time: 937.98 sec\n",
            "epoch: 8, loss: 1537.47, time: 937.98 sec\n",
            "epoch: 9, loss: 1343.11, time: 938.27 sec\n",
            "epoch: 10, loss: 1189.45, time: 937.89 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e69b9dd42246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(translator, optimizer, batches, bsz, num_epochs, cur_ep, learn_rate, folder, shuf)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mpackedDists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0mpackedTargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackedDists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackedTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs_batch, decoder_inputs_batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_inputs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_inputs_batch, padded_encoder_states, decoder_initial_state, src_lengths)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# packed_input is a tuple, whose first comp is the packed sequence, and second comp is \"batchsizes\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;31m# which holds, at position i, the number of inputs for time step i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeProbDists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_initial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mcomputeProbDists\u001b[0;34m(self, packed_input, padded_encoder_states, decoder_initial_state, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;31m# convert states to attentional states before projecting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mpacked_decoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoderdecoder.py\u001b[0m in \u001b[0;36mget_attStates\u001b[0;34m(self, padded_decoder_states, padded_encoder_states, src_lengths, trg_lengths, mask)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mnorm_masked_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mpadded_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_masked_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x seq_len x d_hid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mpacked_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mpacked_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_decoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# concatenate contexts with the original decoder states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    145\u001b[0m                       \u001b[0;34m'the trace incorrect for any other combination of lengths.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAB0QVG4Tc37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ed2cd30-ff2f-4693-bd15-dfd2fc64a1ac"
      },
      "source": [
        "\n",
        "# perform inference with this from-scratch model\n",
        "\n",
        "# switch to CPU at test time:\n",
        "if torch.cuda.is_available():\n",
        "    translator.cpu()\n",
        "    translator.encoder.dev = \"cpu\"\n",
        "    translator.decoder.dev = \"cpu\"\n",
        "\n",
        "translator.eval()\n",
        "print(\"making greedy predictions on the dev set...\")\n",
        "start_time = time.time()\n",
        "dev_g_translations = test(translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "#g_test_accuracy = computeAccuracy(dev_g_translations, dev_trg_sentences, idx_to_trg_word)\n",
        "#print(\"g test accuracy: \" + g_test_accuracy)\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "#with open('dev_greedy_preds.txt', 'w') as f:\n",
        "with open('/content/gdrive/My Drive/predictions/dev_greedy_preds.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        print(translation)\n",
        "\n",
        "translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
        "print(\"making beam search predictions on the dev set...\")\n",
        "start_time = time.time()\n",
        "dev_b_translations = test(translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "#b_test_accuracy = computeAccuracy(dev_b_translations, dev_trg_sentences, idx_to_trg_word)\n",
        "#print(\"b test accuracy: \" + b_test_accuracy)\n",
        "\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds.txt', 'w') as f:\n",
        "    for translation in dev_b_translations:\n",
        "        f.write(translation + '\\n')\n",
        "        print(translation)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "making greedy predictions on the dev set...\n",
            "...took 1905.13 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_SR2JVHPYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctf-I1fsRA7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxgeg_plRClu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "58f23b56-6afd-4dcd-af1c-9376808d9886"
      },
      "source": [
        "# load checkpoint for inference only\n",
        "\n",
        "# intialize models so that can load into them\n",
        "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cuda()\n",
        "\n",
        "if opt == \"RMS\":\n",
        "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "elif opt == \"ADAM\":\n",
        "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "checkpoint = torch.load(fold + 'train_checkpoint' + str(5) + '.tar') # load previous checkpoint\n",
        "\n",
        "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "print(ep_loss)\n",
        "\n",
        "# perform inference with the new model\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cpu()\n",
        "    new_translator.encoder.dev = \"cpu\"\n",
        "    new_translator.decoder.dev = \"cpu\"\n",
        "new_translator.eval()\n",
        "new_translator.decoder.alg = \"beam_search\" # now switch to beam search for comparison\n",
        "new_translator.decoder.beam_size = 10\n",
        "print(\"making beam search predictions on the dev set with model \" + str(5) + \"...\")\n",
        "\n",
        "#print(\"making greedy predictions on the dev set with model \" + str(5) + \"...\")\n",
        "start_time = time.time()\n",
        "dev_b_translations = test(new_translator, dev_src_sentences[:1000])\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_bs10' + str(5) + '.txt', 'w') as f:\n",
        "    for translation in dev_b_translations:\n",
        "        f.write(translation + '\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3609.0740, device='cuda:0', requires_grad=True)\n",
            "making beam search predictions on the dev set with model 5...\n",
            "...took 1464.46 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVRqgImXg7P4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load checkpoint for inference only\n",
        "###BPE beam search\n",
        "\n",
        "\n",
        "# intialize models so that can load into them\n",
        "new_encoder = Encoder(srcV, input_size=IS, hidden_size=enc_hs, num_layers=NL, bsz=b, dev=device)\n",
        "new_decoder = Decoder(trgV, idx_to_trg_word, input_size=IS, hidden_size=dec_hs, num_layers=NL, attentionMechanism=\"global_att\", realData=usingRealData, bsz=b, dev=device)\n",
        "new_translator = RNNencdec(new_encoder, new_decoder)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cuda()\n",
        "\n",
        "if opt == \"RMS\":\n",
        "    optimizer = torch.optim.RMSprop(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "elif opt == \"ADAM\":\n",
        "    optimizer = torch.optim.Adam(new_translator.parameters(), lr=LR) # initialize optimizer\n",
        "\n",
        "checkpoint = torch.load(fold + 'train_checkpoint' + str(19) + '.tar') # load previous checkpoint\n",
        "\n",
        "new_translator.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "ep_loss = checkpoint['ep_loss']\n",
        "print(ep_loss)\n",
        "\n",
        "# perform inference with the new model\n",
        "if torch.cuda.is_available():\n",
        "    new_translator.cpu()\n",
        "    new_translator.encoder.dev = \"cpu\"\n",
        "    new_translator.decoder.dev = \"cpu\"\n",
        "new_translator.eval()\n",
        "print(\"making greedy predictions on the test set with model \" + str(19) + \"...\")\n",
        "start_time = time.time()\n",
        "dev_g_translations = test(new_translator, dev_src_sentences)\n",
        "print(\"...took %0.2f seconds\" % (time.time()-start_time))\n",
        "print()\n",
        "# Create a local file to upload.\n",
        "with open('/content/gdrive/My Drive/predictions/dev_beam_preds_BPE' + str(19) + '.txt', 'w') as f:\n",
        "    for translation in dev_g_translations:\n",
        "        f.write(translation + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}